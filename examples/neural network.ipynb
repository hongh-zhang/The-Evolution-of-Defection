{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f203e4-92bd-4d6c-afca-7dd35271c6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"examples in using the network module\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unlimited-harmony",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# adding parent directory to import path\n",
    "# otherwise simply place the 'network' folder in the same directory\n",
    "import sys\n",
    "import os\n",
    "parent = os.path.dirname(os.path.abspath(''))\n",
    "sys.path.append(parent)\n",
    "\n",
    "\n",
    "import network\n",
    "\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "geological-remedy",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### MNIST classification\n",
    "# please place the dataset manually inside this folder\n",
    "\n",
    "# load\n",
    "train = pd.read_csv(\"mnist_train.csv\", header=None)\n",
    "test = pd.read_csv(\"mnist_test.csv\", header=None)\n",
    "\n",
    "# preprocess X\n",
    "X   = train.iloc[:, 1:].to_numpy(np.float32) / 255.0 * 0.99 + 0.01\n",
    "X_t = test.iloc[:, 1:].to_numpy(np.float32) / 255.0 * 0.99 + 0.01\n",
    "\n",
    "# one hot encode y\n",
    "y   = np.eye(10)[train.iloc[:,0].to_numpy((int))]\n",
    "y_t = np.eye(10)[test.iloc[:,0].to_numpy((int))]\n",
    "y_true = np.argmax(y_t, axis=1)\n",
    "\n",
    "del test, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb8fb9ac-5511-41d6-949f-19b6aebdb804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network\n",
    "nn = network.NeuralNetwork([\n",
    "                    network.Linear_layer(784, 200, bias=None),\n",
    "                    network.Activation_layer('ReLU'),\n",
    "    \n",
    "                    network.Linear_layer(200, 10, bias=None),\n",
    "                    network.Activation_layer('fast_softmax')\n",
    "                    ])\n",
    "param = {\"lr\": 1e-3, 'batch': 16, \"mode\": \"train\", \"eps\": 1e-9, \"beta\":(0.9, 0.999), \n",
    "         \"epoch\": 0, 'optimizer': 'Adam', 't': 1, 'clip': 1.0, 'decay': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "moving-chester",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss = 3.432325.\n",
      "Epoch 0, Performance = 96.65\n",
      "Average loss = 1.393894.\n",
      "Epoch 1, Performance = 97.21\n",
      "Average loss = 0.921310.\n",
      "Epoch 2, Performance = 97.86\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2808/1624114794.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrand\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'fast_cross_entropy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'classification'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {i}, Performance = {accuracy}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\一个普通的文件夹.exe\\Unimelb\\2021 S2\\ANN\\Project\\The-Evolution-of-Defection\\network\\network.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, y, param, loss_func, rand)\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0mdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalc_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[0merror_ls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;31m# record & report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\一个普通的文件夹.exe\\Unimelb\\2021 S2\\ANN\\Project\\The-Evolution-of-Defection\\network\\network.py\u001b[0m in \u001b[0;36mbackprop\u001b[1;34m(self, dout, param)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmagnitude\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m                 \u001b[0mdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdout\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmagnitude\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[0mdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"mse\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\一个普通的文件夹.exe\\Unimelb\\2021 S2\\ANN\\Project\\The-Evolution-of-Defection\\network\\layers\\basics.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, dout, param)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;31m# update self\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mdw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mdw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdecay\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\一个普通的文件夹.exe\\Unimelb\\2021 S2\\ANN\\Project\\The-Evolution-of-Defection\\network\\layers\\layer.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(delta, param)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;31m# dw = lr * 1st / sqrt(2nd)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'adam'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[0mm1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m             \u001b[0mm2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0mu1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    nn.train(X, y, param, rand=True, loss_func='fast_cross_entropy')\n",
    "    yhat = nn(X_t, mode='classification')\n",
    "    accuracy = accuracy_score(y_true, yhat) * 100\n",
    "    print(f\"Epoch {i}, Performance = {accuracy}\")\n",
    "    param['lr'] *= 0.9\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f2d598a-2de9-457b-b3c1-7b791374acd4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--0--\n",
      "Printing linear layer:\n",
      "{'bias': None,\n",
      " 'input': array([[0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.01],\n",
      "       [0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.01],\n",
      "       [0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.01],\n",
      "       ...,\n",
      "       [0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.01],\n",
      "       [0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.01],\n",
      "       [0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.01]], dtype=float32),\n",
      " 'input_nodes': 784,\n",
      " 'm1': array([[-4.56788446e-08, -3.37637989e-07, -4.15270370e-07, ...,\n",
      "        -4.17579196e-06,  4.45680904e-06, -3.29595983e-07],\n",
      "       [-4.56788446e-08, -3.37637989e-07, -4.15270370e-07, ...,\n",
      "        -4.17579196e-06,  4.45680904e-06, -3.29595983e-07],\n",
      "       [-4.56788446e-08, -3.37637989e-07, -4.15270370e-07, ...,\n",
      "        -4.17579196e-06,  4.45680904e-06, -3.29595983e-07],\n",
      "       ...,\n",
      "       [-4.56788446e-08, -3.37637989e-07, -4.15270370e-07, ...,\n",
      "        -4.17579196e-06,  4.45680904e-06, -3.29595983e-07],\n",
      "       [-4.56788446e-08, -3.37637989e-07, -4.15270370e-07, ...,\n",
      "        -4.17579196e-06,  4.45680904e-06, -3.29595983e-07],\n",
      "       [-4.56788446e-08, -3.37637989e-07, -4.15270370e-07, ...,\n",
      "        -4.17579196e-06,  4.45680904e-06, -3.29595983e-07]]),\n",
      " 'm2': array([[2.53266797e-10, 1.12438402e-09, 9.34491593e-10, ...,\n",
      "        2.35720535e-09, 1.38071196e-09, 5.91489934e-10],\n",
      "       [2.53266797e-10, 1.12438402e-09, 9.34491593e-10, ...,\n",
      "        2.35720535e-09, 1.38071196e-09, 5.91489934e-10],\n",
      "       [2.53266797e-10, 1.12438402e-09, 9.34491593e-10, ...,\n",
      "        2.35720535e-09, 1.38071196e-09, 5.91489934e-10],\n",
      "       ...,\n",
      "       [2.53266797e-10, 1.12438402e-09, 9.34491593e-10, ...,\n",
      "        2.35720535e-09, 1.38071196e-09, 5.91489934e-10],\n",
      "       [2.53266797e-10, 1.12438402e-09, 9.34491593e-10, ...,\n",
      "        2.35720535e-09, 1.38071196e-09, 5.91489934e-10],\n",
      "       [2.53266797e-10, 1.12438402e-09, 9.34491593e-10, ...,\n",
      "        2.35720535e-09, 1.38071196e-09, 5.91489934e-10]]),\n",
      " 'output': array([[-2.93393523, -0.57830491,  0.77745877, ..., -0.81262276,\n",
      "         1.18119937, -3.93710394],\n",
      "       [-1.77023867, -0.60725152, -1.37774045, ..., -2.12423303,\n",
      "        -0.03910906, -1.8781336 ],\n",
      "       [-1.44593335, -1.87657734,  0.23689497, ..., -1.12591952,\n",
      "         0.35639012, -0.86546531],\n",
      "       ...,\n",
      "       [-0.85232103,  4.18668067, -4.54266906, ...,  5.09400897,\n",
      "        -3.21299722, -5.61867533],\n",
      "       [-0.07941369, -3.39645125,  0.15220901, ..., -4.40254371,\n",
      "        -2.60100179,  0.19279124],\n",
      "       [-3.02697114, -0.13834405, -1.34452747, ..., -1.78258421,\n",
      "        -0.08091028, -1.82210368]]),\n",
      " 'output_nodes': 200,\n",
      " 'parameters': [array([[ 0.01954473, -0.02234449,  0.10340682, ...,  0.05960617,\n",
      "         0.0106006 ,  0.06072967],\n",
      "       [ 0.02379317, -0.02493407, -0.01163863, ..., -0.03017686,\n",
      "        -0.023704  , -0.0061777 ],\n",
      "       [ 0.02133551,  0.00937518,  0.00753982, ..., -0.04773455,\n",
      "         0.08130933,  0.01540003],\n",
      "       ...,\n",
      "       [-0.0423178 , -0.04009335,  0.03095247, ..., -0.07241223,\n",
      "        -0.00779153, -0.02037027],\n",
      "       [ 0.0135496 , -0.00428055,  0.04609144, ...,  0.05009973,\n",
      "        -0.00337341,  0.07666485],\n",
      "       [-0.03739105,  0.03009881, -0.02231749, ...,  0.04673351,\n",
      "         0.0813697 , -0.01097683]])],\n",
      " 'type': 'linear',\n",
      " 'weights': array([[ 0.04200215,  0.04064373,  0.16476975, ...,  0.10795032,\n",
      "         0.07603438,  0.12241766],\n",
      "       [ 0.04625058,  0.03805414,  0.0497243 , ...,  0.0181673 ,\n",
      "         0.04172978,  0.05551029],\n",
      "       [ 0.04379292,  0.07236339,  0.06890275, ...,  0.00060961,\n",
      "         0.14674311,  0.07708802],\n",
      "       ...,\n",
      "       [-0.01986039,  0.02289486,  0.0923154 , ..., -0.02406807,\n",
      "         0.05764225,  0.04131772],\n",
      "       [ 0.03600702,  0.05870766,  0.10745437, ...,  0.09844388,\n",
      "         0.06206038,  0.13835285],\n",
      "       [-0.01493364,  0.09308702,  0.03904545, ...,  0.09507767,\n",
      "         0.14680348,  0.05071116]])}\n",
      "--1--\n",
      "Printing activation layer:\n",
      "{'cache': cache(x=array([[-2.93393523, -0.57830491,  0.77745877, ..., -0.81262276,\n",
      "         1.18119937, -3.93710394],\n",
      "       [-1.77023867, -0.60725152, -1.37774045, ..., -2.12423303,\n",
      "        -0.03910906, -1.8781336 ],\n",
      "       [-1.44593335, -1.87657734,  0.23689497, ..., -1.12591952,\n",
      "         0.35639012, -0.86546531],\n",
      "       ...,\n",
      "       [-0.85232103,  4.18668067, -4.54266906, ...,  5.09400897,\n",
      "        -3.21299722, -5.61867533],\n",
      "       [-0.07941369, -3.39645125,  0.15220901, ..., -4.40254371,\n",
      "        -2.60100179,  0.19279124],\n",
      "       [-3.02697114, -0.13834405, -1.34452747, ..., -1.78258421,\n",
      "        -0.08091028, -1.82210368]]), y=array([[-0.        , -0.        ,  0.77745877, ..., -0.        ,\n",
      "         1.18119937, -0.        ],\n",
      "       [-0.        , -0.        , -0.        , ..., -0.        ,\n",
      "        -0.        , -0.        ],\n",
      "       [-0.        , -0.        ,  0.23689497, ..., -0.        ,\n",
      "         0.35639012, -0.        ],\n",
      "       ...,\n",
      "       [-0.        ,  4.18668067, -0.        , ...,  5.09400897,\n",
      "        -0.        , -0.        ],\n",
      "       [-0.        , -0.        ,  0.15220901, ..., -0.        ,\n",
      "        -0.        ,  0.19279124],\n",
      "       [-0.        , -0.        , -0.        , ..., -0.        ,\n",
      "        -0.        , -0.        ]])),\n",
      " 'func_backward': <function <lambda> at 0x00000174E5648310>,\n",
      " 'func_forward': <function <lambda> at 0x00000174E5648280>,\n",
      " 'func_name': 'relu',\n",
      " 'type': 'activation'}\n",
      "--2--\n",
      "Printing linear layer:\n",
      "{'bias': None,\n",
      " 'input': array([[-0.        , -0.        ,  0.77745877, ..., -0.        ,\n",
      "         1.18119937, -0.        ],\n",
      "       [-0.        , -0.        , -0.        , ..., -0.        ,\n",
      "        -0.        , -0.        ],\n",
      "       [-0.        , -0.        ,  0.23689497, ..., -0.        ,\n",
      "         0.35639012, -0.        ],\n",
      "       ...,\n",
      "       [-0.        ,  4.18668067, -0.        , ...,  5.09400897,\n",
      "        -0.        , -0.        ],\n",
      "       [-0.        , -0.        ,  0.15220901, ..., -0.        ,\n",
      "        -0.        ,  0.19279124],\n",
      "       [-0.        , -0.        , -0.        , ..., -0.        ,\n",
      "        -0.        , -0.        ]]),\n",
      " 'input_nodes': 200,\n",
      " 'm1': array([[ 6.03181679e-07,  1.02255782e-06, -9.58665137e-05, ...,\n",
      "         1.57883875e-04, -1.94549890e-04,  5.71437745e-03],\n",
      "       [ 1.77146160e-04, -4.18071619e-05, -4.29861518e-05, ...,\n",
      "        -1.34908053e-03, -1.22597176e-03,  3.64994835e-04],\n",
      "       [-9.27510699e-03,  5.51863534e-05,  8.88773903e-06, ...,\n",
      "         3.16383403e-03,  1.19093640e-04, -2.12107762e-05],\n",
      "       ...,\n",
      "       [ 5.88260807e-04, -2.66919087e-04, -2.00468620e-04, ...,\n",
      "         6.24316909e-04,  3.32830593e-04, -3.82031658e-03],\n",
      "       [ 8.16061240e-06,  6.06255322e-05,  2.74961047e-05, ...,\n",
      "         3.20370367e-05, -6.59125060e-04, -1.35394289e-05],\n",
      "       [ 4.62007544e-08,  6.93135744e-05,  8.40477434e-05, ...,\n",
      "         1.11671206e-06,  1.47728661e-05,  7.38227940e-06]]),\n",
      " 'm2': array([[2.62078631e-06, 5.88114674e-07, 5.68468871e-06, ...,\n",
      "        7.01054559e-05, 2.98265297e-05, 1.80102318e-04],\n",
      "       [5.81328578e-05, 1.29354259e-05, 7.19016582e-05, ...,\n",
      "        1.27357729e-04, 2.09873688e-04, 2.62774842e-04],\n",
      "       [4.61218178e-05, 3.02813183e-05, 2.88689296e-05, ...,\n",
      "        6.95560534e-05, 1.25235424e-05, 5.01217215e-05],\n",
      "       ...,\n",
      "       [5.29343362e-05, 1.28825558e-04, 4.31765156e-04, ...,\n",
      "        1.54872578e-04, 1.36568982e-04, 2.35594007e-04],\n",
      "       [4.86246883e-05, 2.55494248e-05, 2.70287140e-05, ...,\n",
      "        4.81301138e-06, 2.01531230e-04, 9.77095557e-05],\n",
      "       [1.05202078e-06, 7.00980765e-05, 5.15569995e-05, ...,\n",
      "        4.74647116e-06, 2.65062027e-05, 1.43735480e-05]]),\n",
      " 'output': array([[ 12.3948875 , -11.04701036,  -4.75554873,  -8.21867243,\n",
      "         -4.02093383,  -8.55696447,  -2.06043195,  -5.79315292,\n",
      "        -10.65665172,  -2.19397001],\n",
      "       [ -8.27335516,   6.94154414,  -2.67281588,  -5.78034704,\n",
      "         -4.52074691, -10.37143174,  -9.60901192,   0.28137305,\n",
      "         -2.66981073,  -6.58777622],\n",
      "       [ 11.99141903,  -7.7037279 ,  -0.26806749,  -6.58149547,\n",
      "         -6.22816187,  -6.41170647,  -2.3855847 ,  -5.77330303,\n",
      "         -6.26385888,  -1.5227949 ],\n",
      "       [-11.43981808,  -7.64373524,   1.46379158,   9.14093211,\n",
      "         -8.7414843 ,   1.67140843,  -7.62663811,  -8.14245487,\n",
      "          1.0031614 ,  -4.97323299],\n",
      "       [ -9.78535082,   0.74933482,   9.28989227,  -0.53210728,\n",
      "        -12.38366347,  -8.15787948, -10.06807559,  -0.06005842,\n",
      "          0.52442049, -15.46132215],\n",
      "       [ 15.85901777,  -9.93385543,  -9.01131284,  -9.99618673,\n",
      "        -15.17858099,  -2.89738041,   0.7232565 ,  -5.70990769,\n",
      "        -10.93238986,  -5.82316163],\n",
      "       [-19.76223548,  -8.95112846,  -1.87438696,  16.72920808,\n",
      "        -18.08114951,  -4.3974725 , -22.52161546,  -1.45978167,\n",
      "          1.23603699,  -6.62834504],\n",
      "       [-10.48511737, -14.73886983,  -5.8040529 ,   2.62032965,\n",
      "         -8.51207426,   1.58963197,  -3.47461951,  -9.37685288,\n",
      "         12.19109319,   1.03994397],\n",
      "       [ -5.72519991,  -1.7257441 ,  -6.91138521,  -6.16733808,\n",
      "         -3.18589045,  -6.92563558, -11.80845118,   7.10205418,\n",
      "         -5.27777285,  -1.27291618],\n",
      "       [-11.86691507,  -9.79531671,  -4.33451762,  -8.05752339,\n",
      "         10.62155856,  -2.11709545, -15.00266048,  -2.76443345,\n",
      "        -10.8468611 ,   0.77332865],\n",
      "       [-12.06353266,  -2.11862484,  -1.78379026,   1.84266911,\n",
      "        -10.42307547,  -7.47669596, -18.00945543,  10.73145544,\n",
      "          0.41033479,  -4.32600969],\n",
      "       [-12.85656753, -10.62638535,  -6.03390366,  11.3345761 ,\n",
      "        -11.74911898,   0.66219684, -19.00870889,  -7.11293158,\n",
      "         -0.66058962,  -0.72096938],\n",
      "       [ -6.59044598,  -6.89677585,  -2.24254603,  -7.70533485,\n",
      "         -8.88360969,   2.82513993,   8.51602445,  -8.67858684,\n",
      "          1.81933092,  -4.0884432 ],\n",
      "       [ -7.37913388, -10.76247967,  -7.76721454,  -9.58988092,\n",
      "          2.48722197,  -6.21791751,  15.97903337, -13.32625643,\n",
      "         -5.42957051,  -9.04428058],\n",
      "       [-17.54235061,  -9.50970394, -11.68674271,  18.32659459,\n",
      "        -18.02888071,   1.75008124, -22.43307465,  -4.67824738,\n",
      "         -8.48687287,  -2.18895878],\n",
      "       [ -8.03949645,   8.22628136,  -4.60829556,  -7.74564508,\n",
      "         -3.8436713 ,  -9.89631396,  -5.07331354,  -1.34010217,\n",
      "         -2.28334224, -11.75396863]]),\n",
      " 'output_nodes': 10,\n",
      " 'parameters': [array([[-0.11213318,  0.03302853, -0.11917012, ..., -0.02558164,\n",
      "        -0.04409494,  0.05220463],\n",
      "       [-0.04062389, -0.07743123, -0.00621537, ...,  0.05875419,\n",
      "         0.16334716,  0.1261056 ],\n",
      "       [ 0.13995938,  0.06577945,  0.03351662, ...,  0.13700772,\n",
      "         0.0248983 , -0.13528771],\n",
      "       ...,\n",
      "       [-0.14399391,  0.1544737 ,  0.0257911 , ..., -0.1458634 ,\n",
      "        -0.17377297,  0.03930768],\n",
      "       [ 0.06733983,  0.02721373,  0.04748721, ...,  0.03673578,\n",
      "         0.12989875,  0.1142693 ],\n",
      "       [-0.04669311,  0.07775405,  0.04693877, ..., -0.05621228,\n",
      "        -0.09581066, -0.17138843]])],\n",
      " 'type': 'linear',\n",
      " 'weights': array([[-0.22090205, -0.08532221, -0.14302182, ...,  0.04614706,\n",
      "        -0.11388554,  0.10086529],\n",
      "       [ 0.00549905, -0.41732079, -0.12428525, ...,  0.0518236 ,\n",
      "         0.17204699,  0.1101318 ],\n",
      "       [ 0.15996895,  0.20876196,  0.02665914, ...,  0.0077841 ,\n",
      "        -0.06158141, -0.27084028],\n",
      "       ...,\n",
      "       [-0.43656368,  0.24318929,  0.1093087 , ..., -0.42134741,\n",
      "        -0.21460965,  0.01919393],\n",
      "       [ 0.03964892, -0.17302555, -0.24272692, ..., -0.1352078 ,\n",
      "         0.13412323,  0.0872822 ],\n",
      "       [-0.28832042,  0.29857546, -0.0786219 , ..., -0.16424767,\n",
      "        -0.26770751, -0.08275085]])}\n",
      "--3--\n",
      "Printing activation layer:\n",
      "{'cache': cache(x=array([[ 12.3948875 , -11.04701036,  -4.75554873,  -8.21867243,\n",
      "         -4.02093383,  -8.55696447,  -2.06043195,  -5.79315292,\n",
      "        -10.65665172,  -2.19397001],\n",
      "       [ -8.27335516,   6.94154414,  -2.67281588,  -5.78034704,\n",
      "         -4.52074691, -10.37143174,  -9.60901192,   0.28137305,\n",
      "         -2.66981073,  -6.58777622],\n",
      "       [ 11.99141903,  -7.7037279 ,  -0.26806749,  -6.58149547,\n",
      "         -6.22816187,  -6.41170647,  -2.3855847 ,  -5.77330303,\n",
      "         -6.26385888,  -1.5227949 ],\n",
      "       [-11.43981808,  -7.64373524,   1.46379158,   9.14093211,\n",
      "         -8.7414843 ,   1.67140843,  -7.62663811,  -8.14245487,\n",
      "          1.0031614 ,  -4.97323299],\n",
      "       [ -9.78535082,   0.74933482,   9.28989227,  -0.53210728,\n",
      "        -12.38366347,  -8.15787948, -10.06807559,  -0.06005842,\n",
      "          0.52442049, -15.46132215],\n",
      "       [ 15.85901777,  -9.93385543,  -9.01131284,  -9.99618673,\n",
      "        -15.17858099,  -2.89738041,   0.7232565 ,  -5.70990769,\n",
      "        -10.93238986,  -5.82316163],\n",
      "       [-19.76223548,  -8.95112846,  -1.87438696,  16.72920808,\n",
      "        -18.08114951,  -4.3974725 , -22.52161546,  -1.45978167,\n",
      "          1.23603699,  -6.62834504],\n",
      "       [-10.48511737, -14.73886983,  -5.8040529 ,   2.62032965,\n",
      "         -8.51207426,   1.58963197,  -3.47461951,  -9.37685288,\n",
      "         12.19109319,   1.03994397],\n",
      "       [ -5.72519991,  -1.7257441 ,  -6.91138521,  -6.16733808,\n",
      "         -3.18589045,  -6.92563558, -11.80845118,   7.10205418,\n",
      "         -5.27777285,  -1.27291618],\n",
      "       [-11.86691507,  -9.79531671,  -4.33451762,  -8.05752339,\n",
      "         10.62155856,  -2.11709545, -15.00266048,  -2.76443345,\n",
      "        -10.8468611 ,   0.77332865],\n",
      "       [-12.06353266,  -2.11862484,  -1.78379026,   1.84266911,\n",
      "        -10.42307547,  -7.47669596, -18.00945543,  10.73145544,\n",
      "          0.41033479,  -4.32600969],\n",
      "       [-12.85656753, -10.62638535,  -6.03390366,  11.3345761 ,\n",
      "        -11.74911898,   0.66219684, -19.00870889,  -7.11293158,\n",
      "         -0.66058962,  -0.72096938],\n",
      "       [ -6.59044598,  -6.89677585,  -2.24254603,  -7.70533485,\n",
      "         -8.88360969,   2.82513993,   8.51602445,  -8.67858684,\n",
      "          1.81933092,  -4.0884432 ],\n",
      "       [ -7.37913388, -10.76247967,  -7.76721454,  -9.58988092,\n",
      "          2.48722197,  -6.21791751,  15.97903337, -13.32625643,\n",
      "         -5.42957051,  -9.04428058],\n",
      "       [-17.54235061,  -9.50970394, -11.68674271,  18.32659459,\n",
      "        -18.02888071,   1.75008124, -22.43307465,  -4.67824738,\n",
      "         -8.48687287,  -2.18895878],\n",
      "       [ -8.03949645,   8.22628136,  -4.60829556,  -7.74564508,\n",
      "         -3.8436713 ,  -9.89631396,  -5.07331354,  -1.34010217,\n",
      "         -2.28334224, -11.75396863]]), y=array([[9.99998887e-01, 6.59648575e-11, 3.56171935e-08, 1.11594858e-09,\n",
      "        7.42504166e-08, 7.95656874e-10, 5.27392615e-07, 1.26192611e-08,\n",
      "        9.74637762e-11, 4.61465461e-07],\n",
      "       [2.46395439e-07, 9.98572209e-01, 6.66677765e-05, 2.98079658e-06,\n",
      "        1.05043630e-05, 3.02307963e-08, 6.47984036e-08, 1.27909830e-03,\n",
      "        6.68684248e-05, 1.32944477e-06],\n",
      "       [9.99993273e-01, 2.79578292e-09, 4.73990603e-06, 8.58781732e-09,\n",
      "        1.22273863e-08, 1.01770340e-08, 5.70352951e-07, 1.92697604e-08,\n",
      "        1.17986037e-08, 1.35160112e-06],\n",
      "       [1.15164326e-09, 5.12785574e-08, 4.62683960e-04, 9.98675077e-01,\n",
      "        1.71076140e-08, 5.69444356e-04, 5.21628114e-08, 3.11418650e-08,\n",
      "        2.91900808e-04, 7.40832785e-07],\n",
      "       [5.19413552e-09, 1.95285105e-04, 9.99507582e-01, 5.42182990e-05,\n",
      "        3.86438540e-10, 2.64432665e-08, 3.91496127e-09, 8.69268824e-05,\n",
      "        1.55951684e-04, 1.78020595e-11],\n",
      "       [9.99999725e-01, 6.28488559e-12, 1.58107522e-11, 5.90509973e-12,\n",
      "        3.31544637e-14, 7.14824861e-09, 2.67068263e-07, 4.29273987e-10,\n",
      "        2.31547121e-12, 3.83308996e-10],\n",
      "       [1.41895080e-16, 7.03349945e-12, 8.32839362e-09, 9.99999792e-01,\n",
      "        7.62173241e-16, 6.68034840e-10, 8.98636054e-18, 1.26072984e-08,\n",
      "        1.86810465e-07, 7.17700880e-11],\n",
      "       [1.41840247e-10, 2.01566455e-12, 1.53024140e-08, 6.97305041e-05,\n",
      "        1.02019036e-09, 2.48769132e-05, 1.57188227e-07, 4.29647841e-10,\n",
      "        9.99890861e-01, 1.43572091e-05],\n",
      "       [2.68541453e-06, 1.46538899e-04, 8.20082612e-07, 1.72581074e-06,\n",
      "        3.40266677e-05, 8.08479000e-07, 6.12475715e-09, 9.99578718e-01,\n",
      "        4.20074615e-06, 2.30469568e-04],\n",
      "       [1.71141353e-10, 1.35843449e-09, 3.19619752e-07, 7.72240810e-09,\n",
      "        9.99942361e-01, 2.93526843e-06, 7.43905952e-12, 1.53642902e-06,\n",
      "        4.74633921e-10, 5.28375973e-05],\n",
      "       [1.25946294e-10, 2.62545139e-06, 3.66961745e-06, 1.37902483e-04,\n",
      "        6.49571406e-10, 1.23658297e-08, 3.29536817e-13, 9.99822576e-01,\n",
      "        3.29243487e-05, 2.88767904e-07],\n",
      "       [3.11820401e-11, 2.90041631e-10, 2.86384690e-08, 9.99964800e-01,\n",
      "        9.43763806e-11, 2.31755096e-05, 6.63840098e-14, 9.73495441e-09,\n",
      "        6.17376981e-06, 5.81202983e-06],\n",
      "       [2.73737407e-07, 2.01510081e-07, 2.11642609e-05, 8.97726542e-08,\n",
      "        2.76328581e-08, 3.36102045e-03, 9.95384559e-01, 3.39208010e-08,\n",
      "        1.22928859e-03, 3.34148989e-06],\n",
      "       [7.17259406e-11, 2.43392868e-12, 4.86558386e-11, 7.86250640e-12,\n",
      "        1.38222949e-06, 2.29079441e-10, 9.99998617e-01, 1.87444951e-13,\n",
      "        5.03919153e-10, 1.35678873e-11],\n",
      "       [2.64432604e-16, 8.14421301e-13, 9.23364559e-14, 9.99999935e-01,\n",
      "        1.62561486e-16, 6.32283279e-08, 1.98746980e-18, 1.02123113e-10,\n",
      "        2.26495205e-12, 1.23085913e-09],\n",
      "       [8.62612453e-08, 9.99892406e-01, 2.66665811e-06, 1.15726749e-07,\n",
      "        5.72848042e-06, 1.34713192e-08, 1.67499145e-06, 7.00367029e-05,\n",
      "        2.72697863e-05, 2.10203989e-09]])),\n",
      " 'func_backward': <function Activation_layer.__init__.<locals>.<lambda> at 0x00000174C63EEB80>,\n",
      " 'func_forward': <function <lambda> at 0x00000174E5648160>,\n",
      " 'func_name': 'fast_softmax',\n",
      " 'type': 'activation'}\n"
     ]
    }
   ],
   "source": [
    "nn.print_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "present-discovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Regression\n",
    "\n",
    "x1 = np.arange(1,100)\n",
    "x2 = np.arange(1,100)+1\n",
    "x3 = np.stack((x1,x2)).T\n",
    "\n",
    "x4 = x1*x2/99+x1\n",
    "x4 = np.expand_dims(x4,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc2eeebd-0f4a-4924-86f2-11fd6b2a973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maxout layer is a good choice for regression tasks thanks to its high complexity\n",
    "nn2 = network.NeuralNetwork([\n",
    "                    network.Maxout_layer(2, 20),\n",
    "                    network.Maxout_layer(20, 1)\n",
    "                    ])\n",
    "param = {\"lr\": 1e-3, 'batch': 16, \"mode\": \"train\", \"eps\": 1e-9, \"beta\":(0.9, 0.999), \n",
    "         \"epoch\": 0, 'optimizer': 'Adam', 't': 1, 'clip': 1.0, 'decay': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37be96ac-4023-402f-a032-834eed35ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = nn2.layers[0].w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1db55062-29a2-44f8-95a4-636583409f6f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss = 1467.603116.\n",
      "Average loss = 828.350212.\n",
      "Average loss = 381.937697.\n",
      "Average loss = 202.350451.\n",
      "Average loss = 177.803089.\n",
      "Average loss = 201.881748.\n",
      "Average loss = 191.323013.\n",
      "Average loss = 175.533666.\n",
      "Average loss = 176.251075.\n",
      "Average loss = 177.617987.\n",
      "Average loss = 175.656896.\n",
      "Average loss = 174.540761.\n",
      "Average loss = 174.245917.\n",
      "Average loss = 172.835568.\n",
      "Average loss = 172.894721.\n",
      "Average loss = 171.927454.\n",
      "Average loss = 171.485485.\n",
      "Average loss = 171.658132.\n",
      "Average loss = 171.338748.\n",
      "Average loss = 170.391520.\n",
      "Average loss = 170.192945.\n",
      "Average loss = 170.575849.\n",
      "Average loss = 168.697300.\n",
      "Average loss = 169.466577.\n",
      "Average loss = 168.962113.\n",
      "Average loss = 167.559123.\n",
      "Average loss = 166.885667.\n",
      "Average loss = 168.239960.\n",
      "Average loss = 165.712708.\n",
      "Average loss = 165.376352.\n",
      "Average loss = 166.324604.\n",
      "Average loss = 165.028128.\n",
      "Average loss = 164.483603.\n",
      "Average loss = 163.773350.\n",
      "Average loss = 161.770642.\n",
      "Average loss = 164.644926.\n",
      "Average loss = 163.394897.\n",
      "Average loss = 162.135553.\n",
      "Average loss = 160.180896.\n",
      "Average loss = 159.809142.\n",
      "Average loss = 159.726829.\n",
      "Average loss = 158.746169.\n",
      "Average loss = 157.963107.\n",
      "Average loss = 157.883609.\n",
      "Average loss = 156.951314.\n",
      "Average loss = 156.392821.\n",
      "Average loss = 155.195634.\n",
      "Average loss = 154.804315.\n",
      "Average loss = 154.740245.\n",
      "Average loss = 155.632740.\n",
      "Average loss = 153.962242.\n",
      "Average loss = 153.535205.\n",
      "Average loss = 152.948040.\n",
      "Average loss = 154.140740.\n",
      "Average loss = 153.903428.\n",
      "Average loss = 152.713982.\n",
      "Average loss = 153.185891.\n",
      "Average loss = 150.253964.\n",
      "Average loss = 148.884339.\n",
      "Average loss = 150.258011.\n",
      "Average loss = 148.170316.\n",
      "Average loss = 148.343921.\n",
      "Average loss = 147.564985.\n",
      "Average loss = 147.489099.\n",
      "Average loss = 146.778072.\n",
      "Average loss = 147.721500.\n",
      "Average loss = 146.347753.\n",
      "Average loss = 146.790997.\n",
      "Average loss = 144.418334.\n",
      "Average loss = 144.993925.\n",
      "Average loss = 144.974502.\n",
      "Average loss = 143.977435.\n",
      "Average loss = 143.935472.\n",
      "Average loss = 142.801492.\n",
      "Average loss = 142.487107.\n",
      "Average loss = 142.599839.\n",
      "Average loss = 141.331488.\n",
      "Average loss = 140.208873.\n",
      "Average loss = 140.575222.\n",
      "Average loss = 139.931928.\n",
      "Average loss = 140.135301.\n",
      "Average loss = 140.117766.\n",
      "Average loss = 145.513732.\n",
      "Average loss = 139.208526.\n",
      "Average loss = 139.391296.\n",
      "Average loss = 142.723443.\n",
      "Average loss = 138.684203.\n",
      "Average loss = 137.080305.\n",
      "Average loss = 136.042707.\n",
      "Average loss = 136.343059.\n",
      "Average loss = 135.733239.\n",
      "Average loss = 135.351309.\n",
      "Average loss = 135.479284.\n",
      "Average loss = 134.009698.\n",
      "Average loss = 133.249264.\n",
      "Average loss = 133.464942.\n",
      "Average loss = 132.812349.\n",
      "Average loss = 136.136754.\n",
      "Average loss = 132.249790.\n",
      "Average loss = 130.890240.\n",
      "Average loss = 130.930133.\n",
      "Average loss = 129.859849.\n",
      "Average loss = 130.006647.\n",
      "Average loss = 131.691894.\n",
      "Average loss = 129.467881.\n",
      "Average loss = 130.028653.\n",
      "Average loss = 130.678516.\n",
      "Average loss = 130.869777.\n",
      "Average loss = 129.308340.\n",
      "Average loss = 129.929715.\n",
      "Average loss = 126.965017.\n",
      "Average loss = 126.004274.\n",
      "Average loss = 127.863535.\n",
      "Average loss = 126.290688.\n",
      "Average loss = 125.711243.\n",
      "Average loss = 125.186629.\n",
      "Average loss = 124.970771.\n",
      "Average loss = 126.117119.\n",
      "Average loss = 124.229822.\n",
      "Average loss = 123.782466.\n",
      "Average loss = 122.278997.\n",
      "Average loss = 121.888380.\n",
      "Average loss = 122.442979.\n",
      "Average loss = 121.677538.\n",
      "Average loss = 120.139068.\n",
      "Average loss = 120.994712.\n",
      "Average loss = 119.696346.\n",
      "Average loss = 120.792985.\n",
      "Average loss = 122.590554.\n",
      "Average loss = 118.956199.\n",
      "Average loss = 118.733043.\n",
      "Average loss = 118.599822.\n",
      "Average loss = 117.290225.\n",
      "Average loss = 120.430662.\n",
      "Average loss = 119.283445.\n",
      "Average loss = 125.896587.\n",
      "Average loss = 117.460627.\n",
      "Average loss = 118.493320.\n",
      "Average loss = 117.111795.\n",
      "Average loss = 115.992628.\n",
      "Average loss = 118.270395.\n",
      "Average loss = 113.740618.\n",
      "Average loss = 113.155468.\n",
      "Average loss = 112.487363.\n",
      "Average loss = 113.210589.\n",
      "Average loss = 113.989630.\n",
      "Average loss = 111.610200.\n",
      "Average loss = 111.970241.\n",
      "Average loss = 111.459522.\n",
      "Average loss = 111.717215.\n",
      "Average loss = 110.394732.\n",
      "Average loss = 110.595988.\n",
      "Average loss = 110.887887.\n",
      "Average loss = 108.366976.\n",
      "Average loss = 109.671836.\n",
      "Average loss = 109.960360.\n",
      "Average loss = 110.201623.\n",
      "Average loss = 108.322376.\n",
      "Average loss = 108.029697.\n",
      "Average loss = 106.302935.\n",
      "Average loss = 105.943610.\n",
      "Average loss = 106.960067.\n",
      "Average loss = 107.349899.\n",
      "Average loss = 104.688810.\n",
      "Average loss = 105.020805.\n",
      "Average loss = 104.254906.\n",
      "Average loss = 103.303946.\n",
      "Average loss = 105.596133.\n",
      "Average loss = 102.163156.\n",
      "Average loss = 102.253292.\n",
      "Average loss = 101.759549.\n",
      "Average loss = 101.440818.\n",
      "Average loss = 101.537263.\n",
      "Average loss = 104.877675.\n",
      "Average loss = 101.190813.\n",
      "Average loss = 101.105936.\n",
      "Average loss = 99.217676.\n",
      "Average loss = 102.369547.\n",
      "Average loss = 99.012273.\n",
      "Average loss = 98.647380.\n",
      "Average loss = 97.743381.\n",
      "Average loss = 98.195038.\n",
      "Average loss = 100.896143.\n",
      "Average loss = 97.320516.\n",
      "Average loss = 97.847597.\n",
      "Average loss = 95.905370.\n",
      "Average loss = 95.431395.\n",
      "Average loss = 94.640902.\n",
      "Average loss = 95.447957.\n",
      "Average loss = 93.590662.\n",
      "Average loss = 93.731295.\n",
      "Average loss = 95.032462.\n",
      "Average loss = 92.799405.\n",
      "Average loss = 93.859538.\n",
      "Average loss = 94.728780.\n",
      "Average loss = 92.369467.\n",
      "Average loss = 91.695589.\n",
      "Average loss = 91.036985.\n",
      "Average loss = 93.322867.\n",
      "Average loss = 89.608870.\n",
      "Average loss = 89.047565.\n",
      "Average loss = 91.404884.\n",
      "Average loss = 89.403664.\n",
      "Average loss = 88.969259.\n",
      "Average loss = 88.647827.\n",
      "Average loss = 90.043749.\n",
      "Average loss = 87.647780.\n",
      "Average loss = 86.427902.\n",
      "Average loss = 87.190371.\n",
      "Average loss = 86.548227.\n",
      "Average loss = 85.659636.\n",
      "Average loss = 84.706070.\n",
      "Average loss = 85.440688.\n",
      "Average loss = 84.184178.\n",
      "Average loss = 84.339724.\n",
      "Average loss = 83.177424.\n",
      "Average loss = 83.651704.\n",
      "Average loss = 84.024718.\n",
      "Average loss = 81.984531.\n",
      "Average loss = 85.909183.\n",
      "Average loss = 81.524660.\n",
      "Average loss = 82.761736.\n",
      "Average loss = 81.250653.\n",
      "Average loss = 80.051261.\n",
      "Average loss = 80.673503.\n",
      "Average loss = 80.911088.\n",
      "Average loss = 79.366303.\n",
      "Average loss = 78.431327.\n",
      "Average loss = 78.121652.\n",
      "Average loss = 77.632105.\n",
      "Average loss = 79.221589.\n",
      "Average loss = 81.456597.\n",
      "Average loss = 77.027154.\n",
      "Average loss = 77.143783.\n",
      "Average loss = 76.682858.\n",
      "Average loss = 75.929136.\n",
      "Average loss = 78.455099.\n",
      "Average loss = 75.537492.\n",
      "Average loss = 75.702567.\n",
      "Average loss = 73.794504.\n",
      "Average loss = 74.919808.\n",
      "Average loss = 76.160269.\n",
      "Average loss = 73.049782.\n",
      "Average loss = 73.407488.\n",
      "Average loss = 72.639909.\n",
      "Average loss = 72.543705.\n",
      "Average loss = 72.047849.\n",
      "Average loss = 71.563317.\n",
      "Average loss = 71.445078.\n",
      "Average loss = 73.273677.\n",
      "Average loss = 74.457196.\n",
      "Average loss = 72.542347.\n",
      "Average loss = 71.215558.\n",
      "Average loss = 70.071825.\n",
      "Average loss = 69.952577.\n",
      "Average loss = 72.123720.\n",
      "Average loss = 68.764323.\n",
      "Average loss = 69.233535.\n",
      "Average loss = 68.833651.\n",
      "Average loss = 69.007707.\n",
      "Average loss = 66.326442.\n",
      "Average loss = 69.078723.\n",
      "Average loss = 67.881623.\n",
      "Average loss = 65.581684.\n",
      "Average loss = 66.125065.\n",
      "Average loss = 66.745450.\n",
      "Average loss = 65.120198.\n",
      "Average loss = 66.485538.\n",
      "Average loss = 66.910122.\n",
      "Average loss = 63.163453.\n",
      "Average loss = 65.480138.\n",
      "Average loss = 65.065880.\n",
      "Average loss = 63.229492.\n",
      "Average loss = 64.496133.\n",
      "Average loss = 62.671138.\n",
      "Average loss = 62.665589.\n",
      "Average loss = 61.428192.\n",
      "Average loss = 62.117919.\n",
      "Average loss = 61.110292.\n",
      "Average loss = 63.544237.\n",
      "Average loss = 60.718575.\n",
      "Average loss = 61.536518.\n",
      "Average loss = 61.202064.\n",
      "Average loss = 61.518968.\n",
      "Average loss = 62.115898.\n",
      "Average loss = 59.995988.\n",
      "Average loss = 58.829314.\n",
      "Average loss = 58.519416.\n",
      "Average loss = 62.409103.\n",
      "Average loss = 57.162440.\n",
      "Average loss = 59.243969.\n",
      "Average loss = 57.610756.\n",
      "Average loss = 60.415712.\n",
      "Average loss = 63.771370.\n",
      "Average loss = 58.236931.\n",
      "Average loss = 56.920810.\n",
      "Average loss = 59.510134.\n",
      "Average loss = 59.337823.\n",
      "Average loss = 56.684528.\n",
      "Average loss = 57.487013.\n",
      "Average loss = 55.692254.\n",
      "Average loss = 55.902937.\n",
      "Average loss = 52.923947.\n",
      "Average loss = 55.349599.\n",
      "Average loss = 54.572045.\n",
      "Average loss = 53.726660.\n",
      "Average loss = 55.079772.\n",
      "Average loss = 52.499471.\n",
      "Average loss = 54.429121.\n",
      "Average loss = 52.155514.\n",
      "Average loss = 51.675388.\n",
      "Average loss = 53.062683.\n",
      "Average loss = 53.925357.\n",
      "Average loss = 51.975174.\n",
      "Average loss = 51.864256.\n",
      "Average loss = 52.829500.\n",
      "Average loss = 51.010961.\n",
      "Average loss = 51.398929.\n",
      "Average loss = 49.722876.\n",
      "Average loss = 49.768081.\n",
      "Average loss = 50.465407.\n",
      "Average loss = 49.146191.\n",
      "Average loss = 49.252664.\n",
      "Average loss = 50.014423.\n",
      "Average loss = 49.452604.\n",
      "Average loss = 48.143931.\n",
      "Average loss = 48.063092.\n",
      "Average loss = 48.634261.\n",
      "Average loss = 48.933474.\n",
      "Average loss = 48.120617.\n",
      "Average loss = 46.794569.\n",
      "Average loss = 46.728904.\n",
      "Average loss = 47.183899.\n",
      "Average loss = 45.759520.\n",
      "Average loss = 46.152752.\n",
      "Average loss = 46.273180.\n",
      "Average loss = 45.852681.\n",
      "Average loss = 45.311840.\n",
      "Average loss = 46.572571.\n",
      "Average loss = 44.749851.\n",
      "Average loss = 44.368921.\n",
      "Average loss = 50.893017.\n",
      "Average loss = 46.798710.\n",
      "Average loss = 45.077989.\n",
      "Average loss = 43.951654.\n",
      "Average loss = 42.683815.\n",
      "Average loss = 44.500887.\n",
      "Average loss = 43.293269.\n",
      "Average loss = 42.630519.\n",
      "Average loss = 42.670355.\n",
      "Average loss = 44.202044.\n",
      "Average loss = 43.486957.\n",
      "Average loss = 42.344860.\n",
      "Average loss = 43.340539.\n",
      "Average loss = 43.317004.\n",
      "Average loss = 41.586956.\n",
      "Average loss = 40.157522.\n",
      "Average loss = 41.573669.\n",
      "Average loss = 41.275816.\n",
      "Average loss = 40.284582.\n",
      "Average loss = 43.353685.\n",
      "Average loss = 42.735700.\n",
      "Average loss = 39.636127.\n",
      "Average loss = 40.505980.\n",
      "Average loss = 40.806822.\n",
      "Average loss = 40.054647.\n",
      "Average loss = 38.847904.\n",
      "Average loss = 48.816092.\n",
      "Average loss = 38.847895.\n",
      "Average loss = 40.023058.\n",
      "Average loss = 37.932717.\n",
      "Average loss = 37.682313.\n",
      "Average loss = 38.648194.\n",
      "Average loss = 36.702528.\n",
      "Average loss = 38.258178.\n",
      "Average loss = 36.536921.\n",
      "Average loss = 36.616166.\n",
      "Average loss = 36.822519.\n",
      "Average loss = 36.799212.\n",
      "Average loss = 39.018251.\n",
      "Average loss = 36.200195.\n",
      "Average loss = 37.066345.\n",
      "Average loss = 36.784872.\n",
      "Average loss = 36.192769.\n",
      "Average loss = 36.681723.\n",
      "Average loss = 35.533129.\n",
      "Average loss = 34.294364.\n",
      "Average loss = 34.926376.\n",
      "Average loss = 34.280006.\n",
      "Average loss = 33.862439.\n",
      "Average loss = 33.762398.\n",
      "Average loss = 33.868350.\n",
      "Average loss = 35.266837.\n",
      "Average loss = 32.964448.\n",
      "Average loss = 32.967199.\n",
      "Average loss = 32.238291.\n",
      "Average loss = 35.260414.\n",
      "Average loss = 33.378212.\n",
      "Average loss = 32.682005.\n",
      "Average loss = 35.562987.\n",
      "Average loss = 32.213378.\n",
      "Average loss = 33.169434.\n",
      "Average loss = 35.658426.\n",
      "Average loss = 32.440389.\n",
      "Average loss = 31.529226.\n",
      "Average loss = 30.476904.\n",
      "Average loss = 37.542331.\n",
      "Average loss = 31.871109.\n",
      "Average loss = 32.240591.\n",
      "Average loss = 32.848537.\n",
      "Average loss = 30.910255.\n",
      "Average loss = 31.521203.\n",
      "Average loss = 31.083358.\n",
      "Average loss = 31.734308.\n",
      "Average loss = 33.335343.\n",
      "Average loss = 30.019219.\n",
      "Average loss = 29.714283.\n",
      "Average loss = 30.310040.\n",
      "Average loss = 32.275528.\n",
      "Average loss = 27.723979.\n",
      "Average loss = 31.124048.\n",
      "Average loss = 28.898913.\n",
      "Average loss = 29.146651.\n",
      "Average loss = 28.154173.\n",
      "Average loss = 28.234459.\n",
      "Average loss = 28.192131.\n",
      "Average loss = 27.445880.\n",
      "Average loss = 27.595389.\n",
      "Average loss = 27.568668.\n",
      "Average loss = 27.901536.\n",
      "Average loss = 27.523864.\n",
      "Average loss = 26.902402.\n",
      "Average loss = 27.188347.\n",
      "Average loss = 26.652006.\n",
      "Average loss = 26.241801.\n",
      "Average loss = 27.911312.\n",
      "Average loss = 29.057660.\n",
      "Average loss = 28.576297.\n",
      "Average loss = 27.272806.\n",
      "Average loss = 31.589957.\n",
      "Average loss = 25.296255.\n",
      "Average loss = 27.307745.\n",
      "Average loss = 25.049506.\n",
      "Average loss = 26.399596.\n",
      "Average loss = 35.168110.\n",
      "Average loss = 28.558215.\n",
      "Average loss = 28.260744.\n",
      "Average loss = 25.775251.\n",
      "Average loss = 25.291795.\n",
      "Average loss = 24.948442.\n",
      "Average loss = 25.074940.\n",
      "Average loss = 28.492296.\n",
      "Average loss = 23.869388.\n",
      "Average loss = 24.916454.\n",
      "Average loss = 23.327767.\n",
      "Average loss = 26.442033.\n",
      "Average loss = 24.124144.\n",
      "Average loss = 24.069098.\n",
      "Average loss = 25.455915.\n",
      "Average loss = 24.918212.\n",
      "Average loss = 22.507457.\n",
      "Average loss = 23.765427.\n",
      "Average loss = 22.627726.\n",
      "Average loss = 22.478134.\n",
      "Average loss = 23.433783.\n",
      "Average loss = 22.237532.\n",
      "Average loss = 22.497115.\n",
      "Average loss = 22.652341.\n",
      "Average loss = 22.408615.\n",
      "Average loss = 23.358958.\n",
      "Average loss = 21.591705.\n",
      "Average loss = 21.881513.\n",
      "Average loss = 22.862753.\n",
      "Average loss = 24.529800.\n",
      "Average loss = 21.899391.\n",
      "Average loss = 21.254692.\n",
      "Average loss = 22.547157.\n",
      "Average loss = 25.888279.\n",
      "Average loss = 20.268190.\n",
      "Average loss = 21.529662.\n",
      "Average loss = 21.223761.\n",
      "Average loss = 20.988179.\n",
      "Average loss = 20.279647.\n",
      "Average loss = 21.207994.\n",
      "Average loss = 20.345798.\n",
      "Average loss = 20.153723.\n",
      "Average loss = 20.955582.\n",
      "Average loss = 20.613859.\n",
      "Average loss = 21.736460.\n",
      "Average loss = 20.646540.\n",
      "Average loss = 19.511636.\n",
      "Average loss = 20.417840.\n",
      "Average loss = 19.011369.\n",
      "Average loss = 19.573462.\n",
      "Average loss = 18.751458.\n",
      "Average loss = 19.086899.\n",
      "Average loss = 18.991204.\n",
      "Average loss = 18.454873.\n",
      "Average loss = 19.388670.\n",
      "Average loss = 18.016850.\n",
      "Average loss = 18.636067.\n",
      "Average loss = 19.537743.\n",
      "Average loss = 18.496457.\n",
      "Average loss = 19.013393.\n",
      "Average loss = 19.141995.\n",
      "Average loss = 18.860596.\n",
      "Average loss = 18.737287.\n",
      "Average loss = 18.244255.\n",
      "Average loss = 18.831978.\n",
      "Average loss = 17.434059.\n",
      "Average loss = 18.170794.\n",
      "Average loss = 21.474132.\n",
      "Average loss = 18.271113.\n",
      "Average loss = 18.858271.\n",
      "Average loss = 17.094899.\n",
      "Average loss = 19.684520.\n",
      "Average loss = 17.069930.\n",
      "Average loss = 17.877197.\n",
      "Average loss = 18.289138.\n",
      "Average loss = 18.364722.\n",
      "Average loss = 18.061187.\n",
      "Average loss = 17.207406.\n",
      "Average loss = 17.485777.\n",
      "Average loss = 16.046640.\n",
      "Average loss = 16.947500.\n",
      "Average loss = 16.721463.\n",
      "Average loss = 16.344872.\n",
      "Average loss = 16.382051.\n",
      "Average loss = 16.320730.\n",
      "Average loss = 16.024724.\n",
      "Average loss = 16.238874.\n",
      "Average loss = 15.633923.\n",
      "Average loss = 15.892637.\n",
      "Average loss = 15.237414.\n",
      "Average loss = 16.544331.\n",
      "Average loss = 15.789900.\n",
      "Average loss = 15.704132.\n",
      "Average loss = 15.736918.\n",
      "Average loss = 15.516358.\n",
      "Average loss = 14.968799.\n",
      "Average loss = 15.992561.\n",
      "Average loss = 16.717860.\n",
      "Average loss = 14.997372.\n",
      "Average loss = 15.220444.\n",
      "Average loss = 15.153063.\n",
      "Average loss = 14.827786.\n",
      "Average loss = 15.222074.\n",
      "Average loss = 15.512410.\n",
      "Average loss = 14.113561.\n",
      "Average loss = 15.204829.\n",
      "Average loss = 14.421420.\n",
      "Average loss = 14.050704.\n",
      "Average loss = 14.647376.\n",
      "Average loss = 15.046688.\n",
      "Average loss = 14.377137.\n",
      "Average loss = 14.113483.\n",
      "Average loss = 14.182032.\n",
      "Average loss = 13.767470.\n",
      "Average loss = 15.103502.\n",
      "Average loss = 13.486363.\n",
      "Average loss = 13.918153.\n",
      "Average loss = 14.035463.\n",
      "Average loss = 14.218462.\n",
      "Average loss = 13.612184.\n",
      "Average loss = 13.024383.\n",
      "Average loss = 16.059918.\n",
      "Average loss = 14.892428.\n",
      "Average loss = 14.265968.\n",
      "Average loss = 14.969356.\n",
      "Average loss = 12.988944.\n",
      "Average loss = 13.591783.\n",
      "Average loss = 12.924949.\n",
      "Average loss = 13.372476.\n",
      "Average loss = 13.181365.\n",
      "Average loss = 12.792436.\n",
      "Average loss = 12.514530.\n",
      "Average loss = 12.586751.\n",
      "Average loss = 12.720910.\n",
      "Average loss = 12.481232.\n",
      "Average loss = 13.626412.\n",
      "Average loss = 13.104980.\n",
      "Average loss = 12.292158.\n",
      "Average loss = 13.254033.\n",
      "Average loss = 12.007536.\n",
      "Average loss = 12.096826.\n",
      "Average loss = 13.022627.\n",
      "Average loss = 12.302084.\n",
      "Average loss = 12.698972.\n",
      "Average loss = 12.459845.\n",
      "Average loss = 11.941169.\n",
      "Average loss = 13.156951.\n",
      "Average loss = 12.000311.\n",
      "Average loss = 11.732020.\n",
      "Average loss = 11.598000.\n",
      "Average loss = 11.347806.\n",
      "Average loss = 11.798346.\n",
      "Average loss = 12.186721.\n",
      "Average loss = 11.906396.\n",
      "Average loss = 11.772473.\n",
      "Average loss = 11.217713.\n",
      "Average loss = 11.474568.\n",
      "Average loss = 11.091853.\n",
      "Average loss = 11.785698.\n",
      "Average loss = 11.282324.\n",
      "Average loss = 11.046098.\n",
      "Average loss = 10.668922.\n",
      "Average loss = 11.275430.\n",
      "Average loss = 11.329203.\n",
      "Average loss = 11.470649.\n",
      "Average loss = 11.059734.\n",
      "Average loss = 10.932979.\n",
      "Average loss = 11.551135.\n",
      "Average loss = 10.353315.\n",
      "Average loss = 10.278919.\n",
      "Average loss = 11.271299.\n",
      "Average loss = 10.894385.\n",
      "Average loss = 11.412856.\n",
      "Average loss = 10.640713.\n",
      "Average loss = 11.160158.\n",
      "Average loss = 10.752237.\n",
      "Average loss = 10.698395.\n",
      "Average loss = 10.327555.\n",
      "Average loss = 10.498917.\n",
      "Average loss = 11.340710.\n",
      "Average loss = 9.764733.\n",
      "Average loss = 10.810445.\n",
      "Average loss = 9.810326.\n",
      "Average loss = 10.968769.\n",
      "Average loss = 9.802858.\n",
      "Average loss = 9.836776.\n",
      "Average loss = 10.228405.\n",
      "Average loss = 9.749920.\n",
      "Average loss = 10.515875.\n",
      "Average loss = 9.957221.\n",
      "Average loss = 10.334772.\n",
      "Average loss = 9.621481.\n",
      "Average loss = 9.447978.\n",
      "Average loss = 9.496722.\n",
      "Average loss = 9.433848.\n",
      "Average loss = 9.724754.\n",
      "Average loss = 9.202093.\n",
      "Average loss = 9.454078.\n",
      "Average loss = 9.087785.\n",
      "Average loss = 10.986150.\n",
      "Average loss = 9.597406.\n",
      "Average loss = 11.660965.\n",
      "Average loss = 9.669092.\n",
      "Average loss = 9.893203.\n",
      "Average loss = 9.301932.\n",
      "Average loss = 9.672199.\n",
      "Average loss = 8.918078.\n",
      "Average loss = 9.611062.\n",
      "Average loss = 9.456668.\n",
      "Average loss = 9.273650.\n",
      "Average loss = 9.410694.\n",
      "Average loss = 9.223613.\n",
      "Average loss = 9.026893.\n",
      "Average loss = 8.829208.\n",
      "Average loss = 9.010784.\n",
      "Average loss = 9.612516.\n",
      "Average loss = 10.788186.\n",
      "Average loss = 10.236202.\n",
      "Average loss = 9.557305.\n",
      "Average loss = 8.465516.\n",
      "Average loss = 9.438329.\n",
      "Average loss = 9.654951.\n",
      "Average loss = 8.542204.\n",
      "Average loss = 8.613338.\n",
      "Average loss = 8.800647.\n",
      "Average loss = 8.993968.\n",
      "Average loss = 8.563321.\n",
      "Average loss = 8.539285.\n",
      "Average loss = 8.526293.\n",
      "Average loss = 8.329261.\n",
      "Average loss = 8.152666.\n",
      "Average loss = 8.596207.\n",
      "Average loss = 10.342492.\n",
      "Average loss = 9.047189.\n",
      "Average loss = 9.294137.\n",
      "Average loss = 9.549177.\n",
      "Average loss = 8.210375.\n",
      "Average loss = 8.577288.\n",
      "Average loss = 8.148603.\n",
      "Average loss = 9.442978.\n",
      "Average loss = 9.542861.\n",
      "Average loss = 8.617596.\n",
      "Average loss = 7.681729.\n",
      "Average loss = 8.100594.\n",
      "Average loss = 7.997946.\n",
      "Average loss = 7.684029.\n",
      "Average loss = 8.152136.\n",
      "Average loss = 7.888574.\n",
      "Average loss = 8.020982.\n",
      "Average loss = 8.248604.\n",
      "Average loss = 7.680330.\n",
      "Average loss = 7.611565.\n",
      "Average loss = 7.371919.\n",
      "Average loss = 7.688769.\n",
      "Average loss = 8.401395.\n",
      "Average loss = 7.806531.\n",
      "Average loss = 7.397711.\n",
      "Average loss = 7.375888.\n",
      "Average loss = 7.913916.\n",
      "Average loss = 7.301253.\n",
      "Average loss = 8.419924.\n",
      "Average loss = 8.812094.\n",
      "Average loss = 8.215822.\n",
      "Average loss = 7.646063.\n",
      "Average loss = 7.237116.\n",
      "Average loss = 7.373247.\n",
      "Average loss = 7.335182.\n",
      "Average loss = 8.111218.\n",
      "Average loss = 7.681540.\n",
      "Average loss = 10.020404.\n",
      "Average loss = 6.910110.\n",
      "Average loss = 7.274831.\n",
      "Average loss = 7.256952.\n",
      "Average loss = 8.248165.\n",
      "Average loss = 9.192188.\n",
      "Average loss = 9.367513.\n",
      "Average loss = 7.481262.\n",
      "Average loss = 7.469438.\n",
      "Average loss = 8.353894.\n",
      "Average loss = 6.777283.\n",
      "Average loss = 7.282604.\n",
      "Average loss = 6.981643.\n",
      "Average loss = 6.797285.\n",
      "Average loss = 7.062397.\n",
      "Average loss = 6.899947.\n",
      "Average loss = 6.918794.\n",
      "Average loss = 6.731418.\n",
      "Average loss = 6.764719.\n",
      "Average loss = 6.859332.\n",
      "Average loss = 7.181278.\n",
      "Average loss = 7.421984.\n",
      "Average loss = 7.459649.\n",
      "Average loss = 9.591338.\n",
      "Average loss = 9.110570.\n",
      "Average loss = 6.978743.\n",
      "Average loss = 6.640728.\n",
      "Average loss = 6.819517.\n",
      "Average loss = 6.906548.\n",
      "Average loss = 6.550839.\n",
      "Average loss = 6.771545.\n",
      "Average loss = 7.162525.\n",
      "Average loss = 6.329298.\n",
      "Average loss = 6.563600.\n",
      "Average loss = 6.423890.\n",
      "Average loss = 6.496929.\n",
      "Average loss = 6.438319.\n",
      "Average loss = 6.134630.\n",
      "Average loss = 6.552249.\n",
      "Average loss = 6.230295.\n",
      "Average loss = 6.172300.\n",
      "Average loss = 6.357935.\n",
      "Average loss = 6.499131.\n",
      "Average loss = 6.411738.\n",
      "Average loss = 6.954652.\n",
      "Average loss = 7.188008.\n",
      "Average loss = 6.157265.\n",
      "Average loss = 6.715291.\n",
      "Average loss = 6.705796.\n",
      "Average loss = 5.920463.\n",
      "Average loss = 6.270030.\n",
      "Average loss = 6.227821.\n",
      "Average loss = 6.331598.\n",
      "Average loss = 7.440289.\n",
      "Average loss = 7.733795.\n",
      "Average loss = 8.674467.\n",
      "Average loss = 6.114188.\n",
      "Average loss = 6.139255.\n",
      "Average loss = 6.237235.\n",
      "Average loss = 6.673778.\n",
      "Average loss = 6.123746.\n",
      "Average loss = 6.059726.\n",
      "Average loss = 5.829244.\n",
      "Average loss = 6.011739.\n",
      "Average loss = 5.949649.\n",
      "Average loss = 5.986316.\n",
      "Average loss = 5.999131.\n",
      "Average loss = 5.824672.\n",
      "Average loss = 6.192277.\n",
      "Average loss = 6.295134.\n",
      "Average loss = 7.379811.\n",
      "Average loss = 7.037893.\n",
      "Average loss = 5.957937.\n",
      "Average loss = 5.892931.\n",
      "Average loss = 5.694588.\n",
      "Average loss = 5.680397.\n",
      "Average loss = 6.579498.\n",
      "Average loss = 6.614887.\n",
      "Average loss = 6.380620.\n",
      "Average loss = 5.940318.\n",
      "Average loss = 5.819844.\n",
      "Average loss = 6.919521.\n",
      "Average loss = 6.136042.\n",
      "Average loss = 5.310278.\n",
      "Average loss = 6.395386.\n",
      "Average loss = 7.592995.\n",
      "Average loss = 5.911310.\n",
      "Average loss = 7.303575.\n",
      "Average loss = 6.439843.\n",
      "Average loss = 5.647552.\n",
      "Average loss = 5.594591.\n",
      "Average loss = 5.454168.\n",
      "Average loss = 6.514099.\n",
      "Average loss = 5.641504.\n",
      "Average loss = 5.537437.\n",
      "Average loss = 6.728840.\n",
      "Average loss = 6.996726.\n",
      "Average loss = 7.049382.\n",
      "Average loss = 5.767362.\n",
      "Average loss = 5.258863.\n",
      "Average loss = 5.473155.\n",
      "Average loss = 5.519383.\n",
      "Average loss = 5.328725.\n",
      "Average loss = 5.584071.\n",
      "Average loss = 5.498479.\n",
      "Average loss = 5.545934.\n",
      "Average loss = 5.397461.\n",
      "Average loss = 5.592928.\n",
      "Average loss = 5.152787.\n",
      "Average loss = 5.130393.\n",
      "Average loss = 5.541506.\n",
      "Average loss = 5.538562.\n",
      "Average loss = 5.157714.\n",
      "Average loss = 5.630081.\n",
      "Average loss = 5.589919.\n",
      "Average loss = 5.537712.\n",
      "Average loss = 5.216038.\n",
      "Average loss = 5.495028.\n",
      "Average loss = 5.359739.\n",
      "Average loss = 5.295317.\n",
      "Average loss = 5.132148.\n",
      "Average loss = 5.190768.\n",
      "Average loss = 5.354098.\n",
      "Average loss = 5.085539.\n",
      "Average loss = 5.140617.\n",
      "Average loss = 5.756895.\n",
      "Average loss = 5.099675.\n",
      "Average loss = 5.340277.\n",
      "Average loss = 4.919441.\n",
      "Average loss = 6.187327.\n",
      "Average loss = 5.298778.\n",
      "Average loss = 5.248842.\n",
      "Average loss = 4.984119.\n",
      "Average loss = 5.217626.\n",
      "Average loss = 5.266651.\n",
      "Average loss = 4.838273.\n",
      "Average loss = 5.848609.\n",
      "Average loss = 6.370868.\n",
      "Average loss = 4.909612.\n",
      "Average loss = 4.893494.\n",
      "Average loss = 5.175089.\n",
      "Average loss = 5.740199.\n",
      "Average loss = 5.959115.\n",
      "Average loss = 5.530674.\n",
      "Average loss = 6.224374.\n",
      "Average loss = 6.401507.\n",
      "Average loss = 5.813137.\n",
      "Average loss = 8.813591.\n",
      "Average loss = 7.025651.\n",
      "Average loss = 5.627571.\n",
      "Average loss = 4.884397.\n",
      "Average loss = 4.704759.\n",
      "Average loss = 5.030255.\n",
      "Average loss = 4.951447.\n",
      "Average loss = 4.877184.\n",
      "Average loss = 5.033666.\n",
      "Average loss = 5.512338.\n",
      "Average loss = 5.002505.\n",
      "Average loss = 5.064557.\n",
      "Average loss = 4.588800.\n",
      "Average loss = 5.249371.\n",
      "Average loss = 5.266990.\n",
      "Average loss = 4.819487.\n",
      "Average loss = 4.598981.\n",
      "Average loss = 4.524621.\n",
      "Average loss = 4.719617.\n",
      "Average loss = 4.571183.\n",
      "Average loss = 4.544599.\n",
      "Average loss = 4.813385.\n",
      "Average loss = 6.320359.\n",
      "Average loss = 4.610617.\n",
      "Average loss = 4.572998.\n",
      "Average loss = 4.526638.\n",
      "Average loss = 4.756532.\n",
      "Average loss = 4.503100.\n",
      "Average loss = 4.572494.\n",
      "Average loss = 4.618754.\n",
      "Average loss = 5.453227.\n",
      "Average loss = 6.015766.\n",
      "Average loss = 4.912660.\n",
      "Average loss = 4.765262.\n",
      "Average loss = 4.630076.\n",
      "Average loss = 4.226854.\n",
      "Average loss = 4.423258.\n",
      "Average loss = 5.232841.\n",
      "Average loss = 4.433043.\n",
      "Average loss = 4.387307.\n",
      "Average loss = 4.381433.\n",
      "Average loss = 4.516599.\n",
      "Average loss = 4.237059.\n",
      "Average loss = 4.351457.\n",
      "Average loss = 4.187631.\n",
      "Average loss = 4.381286.\n",
      "Average loss = 4.232975.\n",
      "Average loss = 4.324804.\n",
      "Average loss = 6.090640.\n",
      "Average loss = 4.584972.\n",
      "Average loss = 4.212925.\n",
      "Average loss = 4.303061.\n",
      "Average loss = 4.274850.\n",
      "Average loss = 4.410249.\n",
      "Average loss = 4.102467.\n",
      "Average loss = 4.386564.\n",
      "Average loss = 3.917679.\n",
      "Average loss = 4.691883.\n",
      "Average loss = 4.873123.\n",
      "Average loss = 4.188309.\n",
      "Average loss = 4.961974.\n",
      "Average loss = 4.198130.\n",
      "Average loss = 4.277300.\n",
      "Average loss = 4.463885.\n",
      "Average loss = 4.453838.\n",
      "Average loss = 4.331030.\n",
      "Average loss = 4.092414.\n",
      "Average loss = 4.482952.\n",
      "Average loss = 4.000944.\n",
      "Average loss = 4.368756.\n",
      "Average loss = 4.553393.\n",
      "Average loss = 4.283497.\n",
      "Average loss = 4.461201.\n",
      "Average loss = 4.095742.\n",
      "Average loss = 3.984097.\n",
      "Average loss = 4.126475.\n",
      "Average loss = 4.335120.\n",
      "Average loss = 4.652662.\n",
      "Average loss = 4.106871.\n",
      "Average loss = 4.928486.\n",
      "Average loss = 4.320184.\n",
      "Average loss = 4.448375.\n",
      "Average loss = 4.460773.\n",
      "Average loss = 5.279427.\n",
      "Average loss = 4.884510.\n",
      "Average loss = 6.051150.\n",
      "Average loss = 4.655208.\n",
      "Average loss = 4.975222.\n",
      "Average loss = 3.914364.\n",
      "Average loss = 4.408119.\n",
      "Average loss = 4.168721.\n",
      "Average loss = 4.034149.\n",
      "Average loss = 3.967486.\n",
      "Average loss = 4.449460.\n",
      "Average loss = 4.175969.\n",
      "Average loss = 3.921848.\n",
      "Average loss = 4.145166.\n",
      "Average loss = 3.931213.\n",
      "Average loss = 4.728442.\n",
      "Average loss = 4.727499.\n",
      "Average loss = 4.294373.\n",
      "Average loss = 4.941047.\n",
      "Average loss = 4.144388.\n",
      "Average loss = 4.074268.\n",
      "Average loss = 3.837299.\n",
      "Average loss = 4.295770.\n",
      "Average loss = 4.038479.\n",
      "Average loss = 3.994216.\n",
      "Average loss = 3.828126.\n",
      "Average loss = 4.141080.\n",
      "Average loss = 3.638264.\n",
      "Average loss = 5.057412.\n",
      "Average loss = 5.506736.\n",
      "Average loss = 5.755645.\n",
      "Average loss = 4.015224.\n",
      "Average loss = 4.269223.\n",
      "Average loss = 3.919248.\n",
      "Average loss = 4.034508.\n",
      "Average loss = 3.854415.\n",
      "Average loss = 4.148014.\n",
      "Average loss = 4.136610.\n",
      "Average loss = 3.738752.\n",
      "Average loss = 3.628989.\n",
      "Average loss = 3.916272.\n",
      "Average loss = 3.928333.\n",
      "Average loss = 3.681364.\n",
      "Average loss = 3.977201.\n",
      "Average loss = 3.792352.\n",
      "Average loss = 4.320151.\n",
      "Average loss = 4.523604.\n",
      "Average loss = 4.685757.\n",
      "Average loss = 4.253803.\n",
      "Average loss = 4.814404.\n",
      "Average loss = 4.490732.\n",
      "Average loss = 4.738051.\n",
      "Average loss = 3.985296.\n",
      "Average loss = 4.108518.\n",
      "Average loss = 4.020685.\n",
      "Average loss = 3.791291.\n",
      "Average loss = 3.602261.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1000):\n",
    "    nn2.train(x3, x4, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "793fce3e-5104-4ffe-acc1-5b78cfb7e803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 epoch trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.61751627],\n",
       "       [-0.43862728],\n",
       "       [-0.2799403 ],\n",
       "       [-0.14145535],\n",
       "       [-0.02317242],\n",
       "       [ 0.07490849],\n",
       "       [ 0.15278738],\n",
       "       [ 0.21046426],\n",
       "       [ 0.24793911],\n",
       "       [ 0.26521194],\n",
       "       [ 0.26228275],\n",
       "       [ 0.23915154],\n",
       "       [ 0.19581831],\n",
       "       [ 0.13228306],\n",
       "       [ 0.04854579],\n",
       "       [-0.05378666],\n",
       "       [ 0.05704794],\n",
       "       [ 0.14768051],\n",
       "       [ 0.21811107],\n",
       "       [ 0.26833961],\n",
       "       [ 0.29836612],\n",
       "       [ 0.30819062],\n",
       "       [ 0.2978131 ],\n",
       "       [ 0.26723355],\n",
       "       [ 0.21645199],\n",
       "       [ 0.1454684 ],\n",
       "       [ 0.0542828 ],\n",
       "       [-0.05710483],\n",
       "       [ 0.04262425],\n",
       "       [ 0.14070582],\n",
       "       [ 0.21858537],\n",
       "       [ 0.27626289],\n",
       "       [ 0.3137384 ],\n",
       "       [ 0.33101189],\n",
       "       [ 0.32808335],\n",
       "       [ 0.3049528 ],\n",
       "       [ 0.35962731],\n",
       "       [ 0.43197935],\n",
       "       [ 0.48412937],\n",
       "       [ 0.51607736],\n",
       "       [ 0.52782334],\n",
       "       [ 0.5193673 ],\n",
       "       [ 0.49070924],\n",
       "       [ 0.44184915],\n",
       "       [ 0.37278705],\n",
       "       [ 0.28352293],\n",
       "       [ 0.17405679],\n",
       "       [ 0.04438862],\n",
       "       [-0.10548156],\n",
       "       [-0.07655323],\n",
       "       [ 0.17849679],\n",
       "       [ 0.4133448 ],\n",
       "       [ 0.62799078],\n",
       "       [ 0.82755157],\n",
       "       [ 1.1133916 ],\n",
       "       [ 1.3790296 ],\n",
       "       [ 1.62446559],\n",
       "       [ 1.84969956],\n",
       "       [ 2.0547315 ],\n",
       "       [ 2.23956143],\n",
       "       [ 2.40418933],\n",
       "       [ 2.54861522],\n",
       "       [ 2.67283909],\n",
       "       [ 2.77686093],\n",
       "       [ 2.86068076],\n",
       "       [ 2.92429856],\n",
       "       [ 2.96771435],\n",
       "       [ 2.99092811],\n",
       "       [ 2.99393985],\n",
       "       [ 2.97674958],\n",
       "       [ 2.93935728],\n",
       "       [ 2.88176297],\n",
       "       [ 2.80396663],\n",
       "       [ 2.70596827],\n",
       "       [ 2.5877679 ],\n",
       "       [ 2.4493655 ],\n",
       "       [ 2.29076108],\n",
       "       [ 2.11195464],\n",
       "       [ 1.91294619],\n",
       "       [ 1.69373571],\n",
       "       [ 1.45432321],\n",
       "       [ 1.19470869],\n",
       "       [ 0.91489215],\n",
       "       [ 0.61487359],\n",
       "       [ 0.29465302],\n",
       "       [-0.04576958],\n",
       "       [-0.4063942 ],\n",
       "       [-0.78722084],\n",
       "       [-1.1882495 ],\n",
       "       [-1.60948018],\n",
       "       [-2.05091288],\n",
       "       [-2.5125476 ],\n",
       "       [-2.99438434],\n",
       "       [-3.4964231 ],\n",
       "       [-4.01866389],\n",
       "       [-4.56110669],\n",
       "       [-5.12375151],\n",
       "       [-5.70659835],\n",
       "       [-6.30964721]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"{param['epoch']} epoch trained.\")\n",
    "\n",
    "# errors\n",
    "nn2.query(x3, mode='not classification') - x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16d39280-6f24-45a9-81e9-b9b59533fe76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.01765873, -0.30331122,  0.        , -0.04289169,\n",
       "         0.        , -0.28979272, -0.02952687, -1.35488243,  0.0866327 ,\n",
       "        -0.01924133,  0.04080151,  0.05466402, -0.03219964,  0.        ,\n",
       "        -0.01987738,  0.        , -0.02264342,  0.03017396,  0.        ],\n",
       "       [ 0.        , -0.24444836, -0.36591948,  0.        , -0.10112348,\n",
       "         0.        , -0.5155753 , -0.09266805, -1.55446145,  0.1544332 ,\n",
       "        -0.074034  ,  0.09725489,  0.11477714, -0.09229422,  0.        ,\n",
       "        -0.07521719,  0.        , -0.30636752,  0.03016482,  0.        ],\n",
       "       [ 0.        , -2.21345007, -3.23712996,  0.        , -2.67165193,\n",
       "         0.        , -2.04835273, -2.91974715, -2.83562493,  3.46428341,\n",
       "        -2.45689057,  2.59583754,  2.78277699, -2.79676433,  0.        ,\n",
       "        -2.48319581,  0.        , -2.28742419,  0.02935333,  0.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the total weight updates our 1st layer received\n",
    "\n",
    "# Maxout also suffers from the dying ReLU problem,\n",
    "# Zeros below represent dead nodes\n",
    "temp - nn2.layers[0].w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74bb1d64-975c-42fd-bcab-9f2ea255d1e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApgAAAIECAYAAAC9lV7xAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAxOAAAMTgF/d4wjAABeqklEQVR4nO3de3yc5X3n/e810uhgY4QtnySNFdJgDoJgCGDkE0cbGwgG7MgJcVJoN8+Tvp7Ntt2khxyap2mzSdNmm83Tdl/bdLNbaOKQ2sEJELCNgQBGRphDMAclxGmXCFk+H4RAp5Hmfv74ze25ZzSSRtJIM6P5vF8vvWbmnpl7blmR8uW6rt/vcp7nCQAAAMiWUK4vAAAAANMLARMAAABZRcAEAABAVhEwAQAAkFUETAAAAGQVARMAAABZVZrrC0hVXl7uzZs3L9eXAQAAgGEcPHiw3/O88uGez7uAOW/ePLW3t+f6MgAAADAM59yxkZ5nihwAAABZRcAEAABAVhEwAQAAkFV5twYTAAAgn8RiMXmel+vLmHLOOYVC4xuLJGACAACk0d/fr7a2NkWj0VxfSs6Ew2HV19errKxsTO8jYAIAAKTR1tamWbNmqbq6Ws65XF/OlPM8TydOnFBbW5vOO++8Mb2XgAkAAJAiFospGo2qurpapaXFG5eqq6t18uRJxWKxMU2XU+QDAACQwl9zWYwjl0H+9z/WNagETAAAAGQVARMAACAbOlulWEpBUCxqx7PgK1/5inp7e8f8vo6ODq1atSor15ApAiYAAMBEdbZKuxqlvZsTITMWtce7GrMSMv/iL/4ibcAcGBgY8X21tbXas2fPhD9/LAiYAAAAEzVrsVS7TmrbZqFyoMdu27bZ8VmLJ3T63/u935MkrVq1SpdddpluueUW/e7v/q6uueYaXXLJJZKkzZs368orr9Sll16qW2+9VYcPH5YkvfXWWzrnnHPOnMs5p69//etaunSp3v/+9+uf//mfJ3Rt6RAwAQAAJioUlpZvkeqbLFRunWG39U12PBSe0On/8R//UZK0Z88evfLKK5o/f75eeuklPfLII/rlL38pSfr2t7+tF198Ua+++qpWrVqlr3zlK8Oer7y8XPv27dOOHTv0+7//+6OOgo5V8dbdAwAAZFMoLDXeZ8HS13jfhMPlcJqamjRr1qwzj3/wgx/oe9/7nnp7e9Xb26u5c+cO+97NmzdLki688EKVlpbq8OHDikQiWbs2RjABAACyIRaVWu5OPtZy99DCnyw566yzztx/9tln9Xd/93d69NFH9frrr+tb3/rWiAVBFRUVZ+6XlJRkfQSTgAkAADBRfkGPPy2+qTsxXR4s/JmAWbNmqbOzM+1zp06dOrPrUH9/v77zne9M+PMmojgD5iS3EQAAAEWm64DUsTOx5rK0MrEms2OnPT9Bn/vc57RmzRpddtllOnr0aNJz69at0wUXXKALLrjgTCFQLrmxdmafbJFIxGtvb5+8D/DbCNSuSyy69f+ro2OntLZFqmqYvM8HAAB5b3BwUL/61a90/vnnq6SkJLM3dbZatXhwzWUsauGyQLPFcP8OzrmDnucNu2iz+EYwJ7mNAAAAKFJVDUMLekLhgg2XE1F8VeR+GwHJQqVf6ZWlNgIAAADFrvhGMKVEG4GgSWwjAAAACotzTpKUb0sJp5r//fv/HpkqvhFMafg2AoxgAgAASaFQSOFwWCdOnFB1dfWYA9Z04HmeTpw4oXA4rFBobGOSxRcwU9sINN5n4dKfKidkAgAASfX19Wpra9PJkydzfSk5Ew6HVV9fP+b3FV/ATG0jEFyT6bcRKMLFuAAAIFlZWZnOO+88xWKxopwqd86NeeTyzHvz7R9s0tsUSdOyjQAAAMBUGa1NUfGNYErpQ2SRthEAAADItuKsIgcAAMCkIWACAAAgqwiYAAAAyCoCJgAAALKKgAkAAICsImACAAAgqwiYAAAAyCoCJgAAALKKgAkAAICsImACAAAgqwiYAAAAyCoCJgAAALKKgAkAAICsImACAAAgqwiYAAAAyCoCJgAAALKKgAkAAICsImACAAAgqyY1YDrnljrnmp1ze51z/2UyPwsAAAD5oXSSz/9zz/NWSJJz7gnn3Nme570zyZ8JAACAHJrUEUzP86KS5JwrkdQhqXsyPw8AAAC5l3HAdM79nXPuLeec55y7LOW5xfFp8F85515wzl0ceO7jkn4h6bTneQNZu3IAAADkpbGMYP5I0kpJv0nz3Hck/ZPneedL+mtJ9/pPeJ73A0kXSqp1zn0w9Y3Ouc8659r9r3fffXcs1w8AAIA8k3HA9DzvGc/z2lOPO+fmS7pS0vfjhx6QtMg5d55zrjz+3pikLkm9ac77Lc/zIv7XWWedNZ7vAwAAAHkiG0U+iyQd8qe/Pc/znHNtkuolXe6c+4+yIPuM53kHsvB5AAAAyGOTWkXued42Sdsm8zMAAACQX7JRRf62pBrnXKkkOeecbPSyLQvnBgAAQIGZcMD0PO+opJclfSJ+aKOkds/zfj3RcwMAAKDwjKVN0Xecc+2SIpJ2OeeCAfLTkj7tnPuVpM9L+p3sXiYAAAAKRcZrMD3P+/QIz70paVlWrggAAAAFbVJ38hkL51yTc25rT09Pri8FAAAAE5A3AdPzvG2e522qrKzM9aUAAABgAvImYE65zlYpFk0+FovacQAAAIxbcQbMzlZpV6O0d3MiZMai9nhXIyETAABgAoozYM5aLNWuk9q2Wagc6LHbtm12fNbiXF8hAABAwZrUnXzyVigsLd9i99u22Zck1TfZ8VA4d9cGAABQ4IpzBFOyENl4X/KxxvsIlwAAABNUvAEzFpVa7k4+1nL30MIfAAAAjEneBMwp7YPpF/S0bbNp8U3dduuvySRkAgAAjFverMH0PG+bpG2RSMSb9A/rOiB17Exec+mvyezYac9XNUz6ZQAAAExHzvMmP8+NRSQS8drb2yf/gzpbrVo8uOYyFiVcAgAAjMI5d9DzvMhwz+fNCOaUSxciQ2HCJQAAwATlzRpMAAAATA8ETAAAAGQVARMAAABZRcAEAABAVhEwAQAAkFUETAAAAGRV3gTMKd3JBwAAAJMmbwKm53nbPM/bVFlZmetLAQAAwATkTcAEAADA9EDABAAAQFYRMAEAAJBVBEwAAABkFQETAAAAWUXABAAAQFYRMAEAAJBVBEwAAABkFQETAAAAWZU3AZOtIgEAAKaHvAmYbBUJAAAwPeRNwAQAAMD0QMAEAABAVhEwAQAAkFUETAAAAGQVARMAAABZRcAEAABAVhEwAQAAkFUETAAAAGQVARMAAABZRcAEAABAVhEwAQAAkFUETAAAAGRV3gRM51yTc25rT09Pri8FAAAAE5A3AdPzvG2e522qrKzM9aUAAABgAvImYAIAAGB6IGACAAAgqwiYAAAAyCoCJgAAALKKgAkAAICsImACAAAgqwiYAAAAyCoCJgAAALKKgAkAAICsImACAAAgqwiYAAAAyCoCJgAAALKKgAkAAICsKu6A2dkqxaLJx2JROw4AAIBxyZuA6Zxrcs5t7enpmZoP7GyVdjVKezcnQmYsao93NRIyAQAAxilvAqbneds8z9tUWVk5NR84a7FUu05q22ahcqDHbtu22fFZi6fmOgAAAKaZ0lxfQM6EwtLyLXa/bZt9SVJ9kx0PhXN3bQAAAAUsb0YwcyIUlhrvSz7WeB/hEgAAYAKKO2DGolLL3cnHWu4eWvgDAACAjBVvwPQLetq22bT4pm679ddkEjIBAADGpXjXYHYdkDp2Jq+59Ndkduy056sacnuNAAAABch5npfra0gSiUS89vb2qfmwzlarFg+uuYxFCZcAAAAjcM4d9DwvMtzzxTuCKaUPkaEw4RIAAGACincNJgAAACYFARMAAABZRcAEAABAVhEwAQAAkFUETAAAAGQVARMAAABZRcAEAABAVhEwAQAAkFUETAAAAGQVARMAAABZRcAEAABAVhEwAQAAkFV5EzCdc03Oua09PT25vhQAAABMQN4ETM/ztnmet6mysjLXlwIAAIAJyJuACQAAgOmBgAkAAICsImACAAAgqwiYAAAAyCoCJgAAALKKgAkAAICsImACAAAgqwiYAAAAyCoCJgAAALKKgAkAAICsImACAAAgqwiYAAAAyCoCJgAAALKKgAkAAICsImACAAAgqwiYAAAAyCoCJgAAALKKgAkAAICsImACAAAgqwiYAAAAyCoCJgAAALKKgAkAAICsImACAAAgq4o7YHa2Sqf2S7Fo4lgsasc6W3N3XQAAAAWsNNcXkDOdrdLOq6RYvxS5XVpxvx1vvktqf1AqKZfW7pOqGnJ7nQAAAAWmeAPmrMVS7c3S2w/Y17Mfs+Pt2+225nZ7DQAAAMakeKfIQ2EbtVy00R63b0+Ey0Ub7blQOHfXBwAAUKCKN2BKFiCXfW/ocf8Y6zABAADGLG+myJ1zTZKa5syZM3UfGotKz9w+9PjezZILSYcek9a2sA4TAABgDJznebm+hiSRSMRrb2+f/A+KRa2g5+0HEscq66Seg4nHTJUDAAAM4Zw76HleZLjn82YEc8p1HZA6dkiuVKq7zUYsg2HTlUiXfJlwCQAAMEbFGzCrGqR1L9hIZlWDFBtIDphr90mzl+Tu+gAAAApUcRf5VDXYCGUsKrXcnfzc/i/a8ViUYh8AAIAxKN4RTCnebP1qKTxT6j0iqUSqu1U6+ZJ0aJf02CopfJZ0/Dkb7aTYBwAAYFTFHTBnLbZp8OPN8QMx6eBD8fth6eTz8ftOOviI1PVrKbI+BxcKAABQOIq3itw30C09+H6p7+gILyqRNGh3L/pTqf4jUte/S6UVBE4AAFB0RqsiJ2BKUv870o+qxvfei/5U+q3fttHQrgNMowMAgGlvtIBZ3EU+khXx7PvU+N//i7+WHrlc2rlU2nGl1P7Q6O8BAACYxop7DWYsarv2tG2TKmql3o5xnqhfOv2K3X32o1LjfVKoVJr1AatSZ1QTAAAUkeIOmF0HpIM7hu7gMxGxXmnvR+MPnFRSIa17kZAJAACKRnFPkVc1SCu2SP2nZIU8TnLlWfwATxrslTp20ksTAAAUDYp8JAt/sah05Cnp5T+MH3SSsvBvE6qwUU1XLl32dWnWeVSeAwCAgsZe5Jnwp69nL5HOer90tFn65d9k59yxXrv1YtLPP2f3/VZHPR2ETQAAMO0wgjmc9odsbeZ7v5GOPx8InKWSBrL3ORf+kTR3mTSznsAJAAAKAn0ws6X9Ianr36Sff3aSPiA+JX/Ng4RMAACQ15gizxY/9M36gDTYJz3321KsX1IsSx/gSeU1UvQ9WxNK1TkAAChQjGCOV7Aw6JXPS7E+KVRmt+NReo7dDpyWFJY+9E1p3gq2pAQAAHmHKfKp0NkqHdqdqEB35ZI3zqA5HH9LSsm2pQyFE8/FomxTCQAApgwBcyoNWacZUnam0P2WSWEpFJLOvlC6+rtS9ZWJ3YgO7rCenox0AgCASUbAzIX2h7Lb6uiMMkn9iYfLfiAd/LFtdTkjIvV3SmtbGMkEAACTiiKfXIist6/5K6zVUduPpF98I/k14WopemKMJ+5Pfvjcx+22rFrqbpcWrrERTQAAgBxiBHOqtD9k1edn/ZbU+ab0yh9LvR3Z/QxXIoXKpRX3M1UOAAAmDSOY+SIY+HoOZj9cSpI3KJXMlJ7ZJF0e2JaSIiAAADCFGMHMlTf/QXr5j7JbbR46S4q9m3zssr+R2h+UTr/K+kwAAJAVjGDmqws+Iy28Qer8pTTzfdnZkjI1XKpMeuVP7O45lyXWZzKiCQAAJhEjmPlmUrekDEuXfU06vFs6ske6/BvSguuspyZhEwAAZIg2RYXKD5qvfFHyeifxg0JSaaW0ppmgCQAAMjJawAxN5cVgDCLrpYv+s7TqX6VQhZJ/VCVZ/KCYNPtKad+npZ1XSaf2285EqfytMZPeGk3/WgAAUNQYwSwEfrjr+jdbrxkqlU63Ss9tlu3wMxFh2U5B8R6b5yyx9ZnBVkedrdKuRql2nbR8i410+jsIdeykeAgAgCJDkc904Ie32UsSx0JhKVQmxeJV6OPe/zw4KhmSTu+X5KQ9H7WtJ+s32N7ntetsx6DoO9KqH0std9vj+iZ7HgAAII4RzELW2WpV6O/9Rtr/RSnWK7kyyesf/b2jcvZ1+TelhTfaiOWeO23HIF99U2JEEwAAFA1GMKezqobE6GbtWunQbumVL0x81vxMiyRP+vnn4sechpx4URPhEgAADEHAnC78sHnW+6U9mwLT5WmC4ajS9d9Mc47mTZL3r1LVBVSgAwCAM5gin478qXO/IOjIUzaFfvZFVsAz8E72Pis8V4qekkrKpHUvEjIBACgCTJEXo+DUuWTFQTVrLHQ+u9GOubDkRdO/P2NhKXrc7sb6h7YxAgAARYk+mMWiqsEqwq95ULr8W4FwWSJV1IzzpIFAWfthC5gnXrQm8QAAoGgxglls/N6Wsz4gVdZZb829d03snHOWW/uiXVfpzJrPi/5UOvcu1mYCAFCEWINZ7DpbpR0fSvTTlCRXInmDWTh5yKbiP/RfpQ/8rnT48UTABQAABYutIjGyqgbp5pelD31bKplpO/l4g9a4fcJiVs3+0n+Sti+UnrldevMfsnBeAACQz5giR6IoqGaN7cpz4DvSz//YOhOVVEqDPVK4WoqeCLwpJCmW+WcMdNnOQ698QaqstSl6ps8BAJiWmCJHep2t1rj91S9L85ZLR/dIg732XGmVNNCpMQVMX9k8qf+45EJSSYW0dp+F2q4DhE0AAAoEbYowPqmjml0HrEo8FpVe/5p08CfjOGmp1H/M7nqD0szfknpPSi/fJh3bK61tIWwCADANEDAxMj/o+bedrdKhXeM8WXCHICd1viY9scoe1qyVZp4r7d0sdey0sEnIBACgIFHkg7GpapBW/tAKgmZdMIETpSzNqL1DenK11LZNmtsoDXRbmAUAAAVnUtdgOudWSvqvssV6D3ie97ejvYc1mAWis9Wmszt2SIN90t5PBvY/zwJXwhpNAADyVK7bFP27pGs8z1su6cPOuRmT/HmYKlUNVgUeWS+9r0m65WXpos/bc+EqG+EsH+8OQbI1mmc3SKdet2nzXY2MaAIAUCAmdQ2m53kdgYeDGlfZMQpCVYN0+V9J85ZJC1fbOspnmxLPj2fv85MvSHs/avcX3Gh7qTOKCQBA3st4BNM593fOubecc55z7rKU5xY75/Y6537lnHvBOXdxyvNrJP2b53m92bls5K3Ieql0hlR1oVRSLilkrYnGGi5THXlSenYjjdoBACgAY5ki/5GklZJ+k+a570j6J8/zzpf015Lu9Z9wzkUkfUHS59Kd1Dn3Wedcu//17rvvjuGSkLeqGqR1L0ort0mD78n+p+bsuXD1OE7oSa5MevmPpbbt0qn9NmUeizJ1DgBAnhlzkY9z7i1Jd3ie90r88XxJv5Y0x/O8Aeeck3RIFkbflvSIpP/oed6bmZyfIp9pqLPVprf3ftLWZ/YdysJJnQXO6iul069Ka/awMxAAAFNkKop8Fkk65HnegCR5lljbJNVL+rikBknfcc495Zyry8LnodBUNUj1G6SV90vRU4EnJvI/P8+q1o83S1WXSK9/lUIgAADyxGQX+fyzpH+ezM9AAYmsl25+yUYzZ75Peu83NqpZMkOKHh//eU/sk04MJpq1d7YykgkAQA5lI2C+LanGOVcamCKvl41iAsn8LSglqfoKKVQqPRuvFA9VSLHx1IEN2s3hp6Unb2LKHACAHJvwFLnneUclvSzpE/FDGyW1e57364meG0XAH9X80LclF9KZ/0mGKsZxMs+mzM9ukF7+I2nXUqbMAQDIgYyLfJxz35F0q6SFkk5I6vI877z4cxfIKserJb0j6Xc8z3ttPBdEkU8R86vCDz8h/fxPdGZkctxC0vL7pdmXMJIJAEAWjVbkM6lbRY4HARPqbJV2XmlbUE6kN395jdR3xKbK1z4vzV6SeC4WZftJAADGKddbRWbMOdfknNva09OT60tBrgV7aIYqdaZ/5liE58fbIcWkWJ/0/Ketf6Zk4ZLtJwEAmDSMYCK/nemhudmCYcUCqdffgTSkjEY4g8VDF3xW6nxNOrxbqm+Slm+xEU4AAJAxpsgxPfhB87lP2tR52Ryp/9j4z+dKpJta4ttaMk0OAMBYFMwUOTAiv1n7uhek5T+Q+k9O7HyzL5fe+Ctb63lqf/JzbD8JAMCEEDBRWKoarCq8pExSSfwrJJUvHNt5Tr4ktW+XBntYnwkAQJZN6k4+wKTwi4BiUXvc+aa0967E8+FqKXpilJPEl4a4cunk89KzG6XL/1Y6vld6+wFbnzlr8aRcPgAA0x0BE4UpuG4yFJZKym1tZvmCePV4hry+xP2ff85uXal08ZfsvLQzAgBgzJgiR+ELtjWKnoofnMD/tGddZMHy1H6mywEAGAdGMDE9+HucV70k/fv3pF98I/HcWPc4f+c1addVOtMGqb5JmnmuhUxGMgEAGFXejGDSaB1ZUdUgXf5X0hV/b03aKxbEw+U4mrUrJikk1d4hPXMHe5sDAJChvAmYnudt8zxvU2VlZa4vBdPBBZ+RVv5QGuiWatbaKGbZGCvNJal8vtTyCWvMPtBj5wuipREAAEPkTcAEsi6yXlrbIl37sLT8+1L/UTvuSuIvyGBUs++wEhXnIemVLyT6Zg50S0/fxhpNAABSEDAxvVU1WDV41YVSaaV0zhLJVUihcp0Jjhlz0pHHpR2XS298Q/rpBdKhXdI5lybWaAIAALaKRBHpbLXelge+I730nyRXJnn9EztnSaXkhaQ5l0mnX7URUwqBAADTHFtFAj5/NPOCz8SLgEo19FdgjL8Sgz2Si0nHm20kk+bsAAAQMFGkLviM7Wu+cptVm6tEqqiVVY6P0WC888GJl6WOHYkemkyZAwCKFH0wUbzO9M58Uer8pfTcJ3Wm9+V4lM+W9nzMpstPvmC7C63dx5Q5AKDoMIIJVDVI9Rukm/ZKC2+0YxW1Yz9Pb4fk9UgnnpO8AWnucqbMAQBFKW9GMJ1zTZKa5syZk+tLQbEKhaXjLdY388gzEztX2Tw7V3CaPBRmNBMAUBSoIgeC/Erzjh3Snk2S12d9M73B+AtKJQ1kdq65q6xK/cSL1nIzVC6tuN/6cwIAUMCoIgfGwq80j6yXbnlZuujzFi5nRKRlP9CYemce3yOdeF7SoJ0jfI707Mek9ocm6eIBAMgPBExgOP6+5tc8KH34TWnOEqmkTFKJVDnONZqxHqn544mp8+BWk52t9jiIrSgBAAWIgAmMJrJeKp1hgXPdi9KKH0g9R8Z/vuqlts3kqf3S3s221WT7Q3a7d3MiZMaiiecJmQCAApI3RT5AQfCLdEJhyXPSvFVS5xtS39HMz3H0KWnXUtnCzJi0aKO0cLVUc5PUtk3yYtLFX5Rav2GP65uoRgcAFBQCJjBWVQ3SzS/ZKOT+L1q4LK+RoielWF8GJ/CSb/s7rUl7d4fkSqW3H7AvycLl8i0WaAEAKBBMkQPjUdVg0+bHW6QFN0oazDBcpnHkcemJVdY/s/Ts5OcaPk+4BAAUHAImMF5VDdLaFmnx/xMfxZwvLbhhYueMnkx+vOtqa3PE1pMAgAJCH0wgG9ofkirrpCeul+Y2Soef0Li3nJRsJ6HejsT9vqNsPQkAyBv0wQSmQmS9VH2FtGaPFD5bUsx28xkvP1yWzYtvQTkg1ayj2AcAUBAImEA2hcLSocdsu8nB9ySVyH7Nxvmr1n/MbheukZb9i9R1IEsXCgDA5MmbgOmca3LObe3p6cn1pQDj56/LvPZhad0Ldr/6Kp2ZLndl4zvvJX8uPffb1t6ItZgAgDyXNwHT87xtnudtqqyszPWlABPjbzfpV5qffk1SyIqAvP74i8ZYGf74KmtdNNAjHdqd7SsGACCr8iZgAtNSVYONZK7cJg322NT5sh9IkVvjL8j0VzBejFdSLr38h9Kb/8A2kgCAvEUVOTBVOlutSKfrgLRjqeRiFjpD5WProVlSaTPucy6z0dEVW6zIKBa1c1NlDgCYZFSRA/kiOHV++TcsXFYskEJl1oooI87e5/VYY/ZQmbT3E8n7mjOqCQDIMQImkAsXfEa65kHpw29a38wzPS9rpPDsEd6YMuMQPSlVXSy99pe2b3nNTTaSCQBADhEwgVyJrJd6DkrHmm0P8gU3SnOXSdFTYzvPiRapfbukkNR9UNq9glFMAEBOleb6AoCi5hcB+aOOu1dZIdDhpyXFAlXnGQiFLWwqJHX+krWYAICcYQQTyLWqBmn2Evvye2h+6JsWLkvG0LbLLxSqXGg9M0fav7yzdehUOlXpAIAsIWAC+cQvBLrgM9IVfy+pxPpnZiws9XRIg93Sc/dY0U9q0OxsteN7NydCZixKkRAAIGsImEC+uuAz0rL7pL4TY3iTPyrppNOvSLMulPZ9Wtp5VSI4zlos1a6zoqC9m615+97N9riW/c4BABPHGkwgX8WiUtsPJQ3KRjKrpb6jmb5ZUql0er9NtbsSaaDbRjNDYWn5FntZ2zb7kqT6JjseGmWXIb+fZ/B19OAEAAQwggnkq64DUsdOC34rt1pALJs3hhMMJIqEFq6VXvyMtOMKaeeVdu7G+5Jf3nhfZuGS6XUAwCjyZgTTOdckqWnOnDm5vhQgP1Q1WNFPcLSw+aPxJ+P/bRgqk2K9I58nXC0dfkLy4kVAsajU/4708p3Jr2u5e/QRzOD0umShtOVue1zfxPQ6AEASW0UChaOzVdq1VKpZJ13yZemdN23k0BsY23kW3iyd2CtFO60l0oqtUvMm6dAue3ztwyOHTH/E0g+ZUubT6wCAaWG0rSIJmEAhCa5/7GyVHr1Ccl68RVGZ5FxipDIdVyJ5MUme5MLSFf8oHdwqHXlWKjtL6j1iOwxF1o98HQM90tYZicebuqXSMbRUAgAUNPYiB6YTv42Rf/9D37RwWTZPKjs7EC5L0r/fG9SZ7Sa9qPTif7CRy9IKC5c1a6Xam0e+hljUpsWDWu5mi0oAwBkETKCQ+XuaX/Xfpf7jkpxUc4sUGuPy6v4T0twV0qrtVgA0nOD0eH2TjVzWNyVaHhEyAQDKoyIfAOPkT2eXPGgFPc9uSOzqI8n+OzI2+nmOvyA9c4d07Dnrv1l1oU3HB9sPBSvb/TWXfsujjp20KgIASGINJjC9tD8kPXO7VFYtDfZIC26UjuyWBkepNA8qPUcaeEcqqZDmr5KO7JGW/k9boxlZb2s/Z54rvfdWIkzSBxMAigpFPkCxaX9IWrjaguBTt8abszudWXuZqbJ5Uv+x5GPXPGhrNJvvkjp2SOteIFQCQBEaLWAyRQ5MN/6UeU9HPFyGldhCcgz6j0luhuR12+NQha3TbL5LevsByZWy5hIAkBZFPsB0FVkvXfH3UihQUV4+P34nw/+29MOlZA3dt8+1cClJkdsZvQQApMUIJjCdXfAZaeENUucvrUVRy+9KlXVSz8GxnadiodR7OPF44Wppxf00VgcApEXABKa7qobESKMrCWw3WWIjmn2HRj9HMFxK1qw9FrV1nqEwI5kAgCRMkQPFpOpCqaRcWrRRWrlVGujUsE3ZR3LkSenB86QdV0g7r7Sg6QdOAEDRYwQTKCZVDdLafYntJmfWS09/2HbxyVi8aMgf+YxFpYFua7TesVNa28KIJgAUOUYwgWLjbzcZi0qv/pmFS+ePYmbyJyGlcnzeSumVL9huPjU3WXj1MaoJAEWJgAkUq64D0rG90jmXWwFQxQKpdKYVAcllfp6jT0lHHrf7/Z2JQOlvK7mrkZAJAEUmb6bInXNNkprmzJmT60sBikNVg01nz1psTdMXrrbdeQZ6pN0r49tNjqE5e0WNBc2dV0rLfyC9vS2xZ3lwVBMAMO2xkw+Aodq2S82bbGQzU6EK65UZFNyzfDSdrYm1oT62oASAvDTaTj5MkQNIFotKbT+0cFlZN4b3pdnv/MLPWXA8tX/kafLOVptK37s5sTsQU+wAULDyZoocQJ7oOmDV4DVrbY3moo1S90HpRIs9H54tRU9ldq7HGiWFJOekULk1Z/e3sgyatViqXWdT6pLUeJ/UcjdT7ABQoJgiBzCUP13ddcBGEnevkqqXSp4nHX3SXlNaLQ2cyPyclXVS/2lp3b70U97+iKUfMqWxTbEDAKYMU+QAxs5vZVTVIM1eYsVAH/pbG8V0pVLd7VJp+djO2XNQGuy1bSulxNS5PyUeCktLv5v8nsb7CJcAUIAImABG5wfNdS9INz0nDXZLvR0a85+QigXS3k9Kv/z/pJ1XWcX5U7dao/YTL0qPpIxsttydCKDj4e8wFERvTgCYdARMAJmrapBKZ0jHnpPKqiXFxlAI5CyUxnqklz8nVTdK3oB0eLf04LnSrqU2yilJdXfY2s+2bcmFP2NB4RAA5AwBE8DYVDVIK7ZIsX5p4Rpp9ocyfKMXuB2UTr+WeKrvWOL5yJ3Sqq1WEFTfZAVHXQfGfp3BwqG9m62/p7/Gs3YdhUMAMIko8gEwPqf2S6/9hdT+4+yed1O3VFpp9yfaB5PCIQCYFBT5AJgcobB0aJekEvuaf7204IaJn3fvJ5L7ZgbD5VjXT4bCVigUROEQAEw6RjABjF9qEc3uVdLcRunIHik8S+o/NoaThSTF4vdLpJIKqe4WqfFe28Jy1mIbjezYaVXtmYxqMoIJAJNitBFMGq0DGL/UkBfc27x5sxUA+YU7oypRImAOSrMvtWB45Gkp2iUtuMZGTOubpJnnWrgdKWQGw2V9U3LzdomQCQCTiClyANnj98+MrJeW3Sf1HB7DmwMjoeEq6cR+KTxH6jtqleeHdtnuQo33Si33jF4J7u9I5I9Yllba7UQKhwAAGWEEE0D2xaLSb34gaXB87492xm+7k48feVbasyExkpmuEtzfhaiqITGi6h+varCQOZHCIQDAqBjBBJB9XQdsmlwlsj8zLv5E2cTOG3svES4b7x06Cpna+9IPkcHel/4ORQCASUPABJB9VQ2268/NL0krt0mhCqmiVlJ/ds5/5f9IP01O70sAyAtMkQOYHP4o4ewlUuiHUvNdSq4Un4CfXiD1nxg6TR4K2xS4ZKHSL+ihchwAphQjmAAmX2S9dNNeaeGN8QMlY9hi0hf4c9V/QpKTLvqToaGR3pcAkHMETABTIxSWjrfYaOLKrVL0HdtqUiUZniB15NPZOVObr8ei1o4oqOXu8e1nDgAYF6bIAUyNYFV3KCxVXWj9LJ+5Qzq8W3KlkjeQ+fkW3GDvf/o26dheafn3pYWrbW2m3/ty6Xel5k30vgSAKZY3I5jOuSbn3Naenp5cXwqAyeL3yfTvv/eWjWou2igtuH7o6+ddK5XNHXq8slY68rj04xqrKj/nUtti8smbEr0vG++V9n1KOvyMVL3MjnfssJHM4KjnWLefBACMKm8Cpud52zzP21RZWZnrSwEwVaoapDV77P7h3dZIPVShM22NyudIVZcMfV9Ph90O9kjl86UPfk066zzpeLNUOiNRZd62TfJ6pBP7pEu/aiG0+S7p2Y9KO6+yPc+DLYwAAFnBFDmA3AqFpUOPJSq9uw5Ye6E3via1P6hRq87nXCn97HpJnhQ+R+o9Im2Pj3qWzYvvhz4oHfmZrfl8+4H4G0ukn/+xBdvhmrYDAMbFeZ6X62tIEolEvPb29lxfBoCp5O++E1wfeeJFadfVsoDppLrbpGPPS/1HRjiRk5T6Ny3dsQBaGAHAmDnnDnqeFxnueUYwAeReup11SmfY/uHVjbY3efv2+PT5SNIEyZpbpdMvJ6bVU9HCCACyjhFMAPnLH9mMRaWHfsumv8fKlUte3/DPM4IJAGPGCCaAwuWPbHYdkAa6x3eO4cJl7XqpJJxoYdR4r1W1s085AExY3lSRA8CwZi2W5jamHBzjn6/ZV1qvTV/Hw1LdBhvBPLhD2rNh5GryztahzdppcQQAaREwAeS3WNRaCR3ercSfrHHsad75ujVyr7vd+mjKk1o+KZ3/B9L8FdZPs3adNW9PDY2drRY+925OhEz/umhxBABDEDAB5LeuA4nm6Te/LF32tzrTJ3M0JYGioFiv5Mqkq/+XVH21f1B6fKWFS785e8s9Q0PjrMUWPtu2Wagc6LHbtm12nBZHAJCEIh8A+S/YxqizVdpxRXwkMWYhcrBHadsR3fy6tCNNo3ZJKq+R+g4lHq9plt78toXGmrXStQ8nF/74I5b+mk2JAiEARWu0Ih8CJoDC46+HPPKU9PIfWvuiWG+aF45jKn1GROrvtH3TpeT+nAM90tYZiddu6rZWSgBQZEYLmEyRAyg8VQ3S7CXShX8gfejbtrZSsj3NN3VLNbfEXzjGcClJ3e027R2LJq+7jEWl5z6Z/NqWu4cW/gAAaFMEoMDVrJFeLZdqbpdW3G+jjZd9XTr8WCJ4jlXkIzZyOW95fD/zmOR51uxdkiIbJOcS0+VMkwNAEgImgMJW1SCt3Zc8lV3VIC24Pl55Pg6/uV96e6t05Flp7orA/uWyUdIV99v9vZutAKnrAP0zASCANZgAppdgMc6MiE15j0V5jdR3VNKgvb/vtDT4buL54LrLWJRwCaAosQYTQHHpOmCN0/1wWbNWCgUKc0ZrcdR3SNKgVFJp7y+rSn7+mdulU/vtfihMuASANAiYAKaXqgZpxRarBK9vsnZDK++XSmZI4Tka0spoOIM9dttzMP4+SZV1Nu2+80rpxIs0WAeAYTBFDmB6CvbO9B9X1klP3Sodbx7fOedcJZ18QVKJtPAG6XiLtTNiFBNAkWGKHEBxqmpIruyetVja939ZuJy7YnznPPmCNGdpooCIXXwAIC0CJoDiENxy8obHJhAy90lHHpdcqXTxl+y8qb0wY1GmzwEUNQImgOJQ1WDT2f5+48ebLWx+pFM659Kxn69uvd3uapSevi0RMv0q9tT9zIP8nYiCCKUAphECJoDiUdUgvfdWYiRz+RZrOXTWB8Z+rv7T0sxzpfDZ0qFd0uPXSidfTrRIGm76vLM1eYcgKbNQCgAFhCIfAMXHLwCSpOa7khupj0V4thQ9NfR4zVqrXk+3u0+wT2d9k9R4n2056T9mVyAABWC0Ih928gFQfPyq785WqWOHpBJZ+yJ/7/KQMtrHPF24lKTDT0vtD0tVFw6tMA+FLURKFir97SYJlwCmEUYwARQ3fz1k179Jz31Smr1EOvWaVDojvqPPOLkSqaTCtrFM18ZooEfaGmgAH9whCADyHG2KAGAkVQ0WKus3SOtekFY/LS27T+o/ac+7Uql66djP6w1KC9emX4cZi9q0eFDL3UMLfwCgQBEwAcDn986sulAKlUmLNko3PWcN2sfKlUof/H+HTnkH12DWrLUq9vome7x3szTQTaEPgILHGkwASFXVYKOZM8+1lkbtP5YW3CgdeSLzc9z4lPTeb2wE87237NbvmdnxqIXLY3ulfZ+y1kmS7aG+Z4MdZ4cgAAWMgAkA6VQ1xIuA4i2NLv6StGuZFOvJ7P1P3ijF+qSKBVJvp1R9uXT6NWn+CsnzpEu+Ir3+lUSRz9LvSs2brOVRfRM7BAEoaHlT5OOca5LUNGfOnKYTJ07k+nIAwARbGj19mwXA8QrPkaInpcidUk+HdHK/tGCVbTvpG6nFEQDkiYIp8vE8b5vneZsqK6miBJBH/GnqvZstXM7w/56O489n9KRUWSsdeUo68bzk9VrD9qCjzTaVDgAFLG8CJgDkLX8f85q1Un9nfB/zmBQql1QuVV+tjP+c9nQk9888+ULy8+Xn2NpPto4EUMBYgwkAo/H3MfcLdWYttgbt86+Veg5aIHx6/diKgNIpmyd1t0vP/bat0zy0U1pxvxRZn5VvAwCmSt6swfTRaB1AQQm2HaqolXo7Jna+BTdIR56WNCjJSSUzpHXDNGsHgBwpmDWYAFCQ/OnzhWuk3iMTP9+RJ2XhUpI8qXYdFeUACg4BEwAmoqpBWrNHCp+tRDDMksgGadm/UPQDoOAQMAFgokJhqWOX9byUbKq85taJn/fK/26N13c1UvADoKAQMAFgoqoapBVbbJvHhWukuY3SoUcklSS/rrRaksv8vA9GrDXS3Eb2KQdQUAiYAJANkfVWaX75N61x+qKN0tylya8ZOCFpDIWV3qDkyqTSmdJjK6X2hxLPdbYODZ20NgKQJwiYAJAtVQ3S7CW2JlOSjj9nvTM/0mkjmz43hg5x3qDU/hMpPFNq3mwBsrPVps2b75JO7bfX+dXsTKcDyAMETADItlBYOvSY7Sl+7cNS2dnSdY9Y2HSVUvVVYzjZoFQ+3yrUS2dIJ16xMFlzk/T2A9LOK6WTLydaJVF1DiAP0AcTACaDv4d5cE/xgW4r2jm0y6bQPU9q357Z+ULlUqwv/iAsLbhmaGP30fYxT3dNsahVqdNnE8AY0AcTAHKhqmFo0HvvLenYXguXkoXLyJ1WdT6aM+FSkqIWLudfl/yakfYx96fV925OrN1kWh3AJGGrSACYKv6Wk7GotHtVImj2dkiuRCqdLUWPZ36+o08lP/b3MU9n1mKbPm/bZo8b75Na7rbH9U1MqwPIKqbIASAX/Crw3ass+F38JSk2IO2+RvL6rLhnLMJzpOhJC4vLt9ix1Knv4LaWPv/1w02rA0Aao02REzABIJdS10W2PyQ1f1wafG+MJwpJC66XjrVIl31NOrZHOviIhcf6DYmX9b8j/agq8XhTt1RaOeFvA0BxYQ0mAOSz1LWaC1dLJTPGcaKYtSxyJdLLf2gV5rFeqXmTdOJFC7ID3dKjFye/reVuG9mkhyaALGINJgDki1jUqsz7j43v/f1p1m96g9Izd0h9p6Wys6zdkZwUud3CaNs2m5ofeFc63mJrRKkoBzBBjGACQL7oOmBV5jVrpcu/ZSOZtbdJrnx855t/nVRZJ/UclGLvxcOlpMpaa94uSXV32P3Du6V5yyn2AZAVrMEEgHwSXJN5ar/08z+28JctkTssUM6ISN2Bv7UzItKH37Rm7gAwCtZgAkAhCa7JDIVt2nrhGqm6MTvn7/yl9d7sTvkP+ZU/IlwCyBoCJgDkq6oG29c8fLZ0okWau0IqnZVZY/bhdP1SOvLU0OO7Vyb2NQeACSJgAkA+C+5rvvpnFjirlwZeMI7+ldFTQ495A9LrX03s8gMAE0DABIB85u/+4zdDD4Wlw49JrlSqulSK3GqvK62e2OdU1kkHd0jtDycfp30RgHEgYAJAvguuy6xqkNa9IK17UVr+L9LhJ2x0c0ObdN3j4zv/wtVS70nbQaj5o1LbdjvOXuUAxok+mABQaIJ9Kte2WNV5LCq1bB7f+Vy5NH+5dOQJe/zsJmnZ96SDP07sVT7zXAuZ9MgEkAECJgAUsqqGRIP23iOSC0veGNdRHnrEbitqpd4OSYPScx+3Y4s2Shd+zpq1H2+xNaChsIXa1L3OASCOKXIAKHRdB6SjzVLFAguX51wef8KN/L7Q7OTHvR1DX9N3QnpsmfXirF5qhUA7r5aevo2pcwDDImACQKGrapBWbLG9xuubpHXPSysfsJ18RhJLU02e6uhTkuIbcrzzS9vjvKxKOrRLql3Hzj8A0mKKHACmg8j6xHrMUFiqulA6+aK04EYp+o508oUxnCwkKTb0cM/BxK0rlS7+UqL4CAACGMEEgOkitdp8zR6p7BwLl65UimyQ6tZncKI04TLVgutYfwlgWARMAJiugk3a170oLf9e9kYcDz+ZvP6SfpkAAgiYADBdBZu0VzVILffYGspFG6WbWqTwnAmcPCa9+ue2vST9MgGkYA0mAExn/jR2Z6vUsdNGMxvvlZ77bSl6Mv4ipzOFPEOEJQ3T9ujgQ1ZdPn+VFf3UrKXoB4AkRjABoDgERzPfe0vq2CGpxLaIlCe5kmHeOFJPTU8a7LZwWbHAWiV17MjudHln69D90ZmOB/IeARMAioVfBORvN7lyq9R3WgqVS97gCCEzA71HpPBMqXmz9Pj11itzoms0O1tt2n3v5kTIZDoeKAgETAAoRlUNUv0G6fJvSLE+qaxaKplh6zMzqjRPo/eIFCqTjjdL5ecktpfs70w0Zm9/yPp1ZhI+Zy22Xptt2yxUDvTYbds2enACec553nDrbnIjEol47e3tub4MACge7Q9JC1fbjkCvfzXeTH2e1H9sYuc9Z4md0xu0EDt3hXT6Val0hhR9zxrCz1psobFjp03hp7Y+8kcs27YljtU32VQ/PTiBnHHOHfQ8LzLc84xgAkCxi6y30BcKSx27bF2mHy7nXy+VLRz7OV1YOr3f1mjG+uzxivslbyAxnR7tGn1EMhSWGu9LPtZ4H+ESyHMETACA8bec7D9ljdkXbZSu+6k0+5Kxn8uLDn38YL002GNrPnuPSI81Wrj0RySl5KnyzlabTm+5O/lce+4cWvgDIK8wRQ4ASOZXblc12BT3zqttxLH3iI1EpobHMUtpi7Sp2x7v2SAd22tT5ZJ9bvk5Une7hdCl35Uevdge16yVrn3YRjI7WxNbZPpiUbt2dhsCJgVT5ACAsalqkGYvSVScr9hiayYrFli4dBOdnk4Z2Hj6w9LD51u7o3nLLSzOWizNX2FhckbEwuW+TyUeH9trAZJKcyAv0WgdADCyyHoLmc2bLWT2HpEW3GjhrffQxM9/5Em7rayTVm23YBuLSpf9tR0/tEv6UZXd9xvFv/eWhd9YNFFpLtn6zJa7E1PvVJoDOcEIJgBgdH7IHOi2tZkuZOGyvEZZG6tYtsWKjfwRyMdWSr/1H5Jf03ifvcaf+g6Fbf1mfZOFyq0zktd1+tPmNGwHphQBEwCQmch6Wx95yZel4y3SwjVS1UWSBrJz/qdvlnpPSM13WUgsq5L2fjz5NS13Dw2KXQdsCj1o6XftuMQ0OpADTJEDADLnjxyubbGQ9thy2wFo/vXSuZ+Qnr9nnCcOWYX59rmJQz0H7XbRRmnZ9xJT31JidNIPj2VVyad79GJr8L62Jblhu5S9aXSKi4BhTWrAdM7NkbRb0gWe5501mZ8FAJhCfoBa94KFqlmLpZZ7xn6eUIUU65UUG/qcK5UWXG/9M7sO2NpLyZqydx2wzxzotnDpF/+s2y/tXJJ4PPPcxDS6ZKHSD5oTadjuB9vadYlz+KOiwzWNB4rIZE+Rd0laI6llkj8HAJALVQ321XJPYkRw7UtS5M70ry+bm/w41ivNviL9a+ddIx17TurYYWGu5R4Lmf6o5N7N0uPXSn0nLUx2t0vbqxPhsu+0FQNJ2W/YzjaWwIgmdQTT87yopJPOucn8GABALnUdsFG74Ijg8u9b6yF/mluy0cr+40Pff/rV9Oc9+qRUd7s00GvrPdu2SV7Mpsub77ItLRdtlN73cdvq8keBafJb3pB62hOV5p2t0htfSz5/y93jH8GcjFFRYBrJuNG6c+7vJK2X9D5Jl3ue90rgucWS7pM0V1KnpHs8z3sj8PzjnuetzuRzaLQOAAUouB4xFk0EwHQqaqXejszOG66Sop1SaZU00JnyZIk0d6l0Yr9UdYF0+ueJpxauka75SWJ/9fYHbZvKmrXSiq1S8yZrfxRsezRr8djXTw70WOW6b1O3VFo5/OtZt4lpIpuN1n8kaaWk36R57juS/snzvPMl/bWke8dwgZ91zrX7X+++++4YLgkAkBeqGpJbArU/aPcXbbTQtXBN4rWr4iOPowpbuJQsXKY2eC+rko4/J3m9Fi5Lz5Zqb7PnDu+WHny/tPPKRNBduMYatDdvko4+a3032x+1HYR2Xi09dau0a2nmVeWx6NBtLINV7qltkKhmRxHJOGB6nveM53lDhhadc/MlXSnp+/FDD0ha5Jw7L8PzfsvzvIj/ddZZ1AIBQEELhaWScguRK+63Eb3rHrGAVzJDKjvbprlHlWY/86D+k/HQGS8QGnjHRiV9fUdt1NKVSiv+VVrydanmJnvNYLdN34dn2uOyKgulg32Z7XPuB8O2bTYqWjrL1n2eWZPZPTQ4sm4TRSQbRT6LJB3yPG9Akjybc2+TVC/Z9Liky51zjzvnLsnC5wEA8llVg7R2n4VLf1QzFLaQue4FC1LPfTI7n5UaOr3+oa+pWy/N+oD0xA32OHKnzmxX2XfUbv21opHbM5uqDq47XbXdtrj0i4sO7rBR0bZtia0vJfs3uPhLFryDTeEXbbTjrNvENDLpjdY9z1vteV51/Pb1yf48AEAeCE6Z+0JhC1sjrc+cDM7ZiOPCNfHPHabwNLIhORSPpKrBqtmXb7H1m0ebE5Xsg+/aqOiMiB0PNnzfvUpKrX3wPDvOFDmmkWwEzLcl1TjnSiXJWcl4vWwUEwCAhK4D1nbozP/9OClyhzVqnwx16y1UPrbMdh9SidS+PTvn9kP0rMVS3c0WLoO62+24P4I5a7FN0ad+fvt2O84UOaaRCQdMz/OOSnpZ0ifihzZKavc879cTPTcAYJqparBp8ptfllY+IK19QVq5Vbriv0muUlKWp4kP77bCH2/QKtdLyoZ/bft2G11NtwZzpL3MQ2GrRJ+RUlA7I2LHmfoeHXvFTzsZB0zn3Hecc+2SIpJ2OeeCAfLTkj7tnPuVpM9L+p3sXiYAYNqoapBmL5HqN0jVV1gAq2qQ5lymM4U9lbUT/JCQFCq37Sc7HtaZNZeDPckvq6iV6u6QVGKP2x8cGmpGq/4+td+awKcbwWy5J/GergPSocdsKj4ossGO+1PpmZhOgYzq+mlpLFXkn45Xepd6nrfA87zzAs+96XneMs/zzvc870rP814b64U455qcc1t7enpGfzEAYHrpOiCd3m/7mi9YLUW7rI3QeJTNkxSTqpclHy+fl/w4PNtGNUMl0ppnbY2mC0vvpXTjG636W7KCn3QjmP62lpKF6DV7bE1okHN2PNM+mNMtkFFdPy1l3Gh9qtBoHQCKlD8qN2uxVWEf2mUhM7gbUOnZ1o4oWyoWSP3vSgtWSkeetVHU06/aTkS1Nyemt0/tl177y+T1k4s2Spd82YLh07cFGrffZ/0w/RZG1z6cvFe5v6Vm8HVj2QEoW+fJJ8HvyVeo30uRGK3R+qRuFQkAQMb8EbzO1uSq7Pom6cI/kp65M/MdgDLVe8RCpl/1fbzZQuHeT9jo2fItNgK5e5UUPjv5vX719/LvWwP3+qZ4u6HSxDaSHTsT6zT9x0lbagZel+luPtNxm0p/r/hgwJzIXvHIuUlvUwQAwJhUNUgrtkj9nYnQNOdyaW6jPe9KpQ99SxP7v7DANHXvEbvtbrdRyVXbE1O2zXfZdH347OSRVMlGMxeulmIDNsV98ZcscPpT1xd/yY6/8TWbupYSrY2C/UGXb7HjY9kq0g9kQYUcyEbbFQkFh4AJAMg/kfXJYazrgFWEL9oorXtROv8z0sIb4y8ukda9lOH2k3EL16U/7i8bu/hLVnzz9gPSY41Dw6XvWLP07EbplT+VZp5rjdXbtkk/vUB6bKUdD64lHK4/6Fj3IZ9OgSx1yn9Tt936azIL8XsCazABAAWis9VCWiicKHSpucnWQc5eYkHkyXXS0SczOJnTmcryVJV1Uu8pqbRCip4MHK+VegJT9CWVVpVescBGQWdE7H1lZyVGRaXsT11PtzWY/s/SX5IQXK/asXPso7uYEqOtwSRgAgAKUzBwSrb/90OLs79O01dRO/TcFQukW96Qdl42tE2Rb1O37ceeLdMxkKX+LCX7njJdl4opR8AEAEx/sWiikluS9bUczNLJQ1JJvKdmupA5mmyPKvrV9sHp9mDTdwIZpsBoAZM1mACAwtd1wCq5a9ZKa1+Ubn5JWtNiI4ySNOfKcZ7YSfOusXDpyjIPlzMi0kc6s7+W0B+9fONriWP+6OXuVRM/P5AledOmyDnXJKlpzpw5ub4UAEChqWqwqeHgNGtnq02bL1xjVeAnXxzHiT3pREv8bn/isCu1dkSDvYljoXIp1mehtrtd2vcp2ypSGlsbopEEm5JLQ9dfjqUpOdPSmER5M4Lped42z/M2VVZmcZ0KAKB4pFZo+zvnhM+2avCatVLJWVacI9l0d0XN6OeN9Q49VjE/fbgsmyfd8rqFPb8HZv3HLPx2/doCb9BAt9T+UObfYyhsFe6LNlqo3DrDbhdtjPfgzHAafrrtBoS8kzcBEwCArAuFbZ/v+ibbUWfFFht9nBGR+k9br01XNvbz9qRMlcf67Lb/uPTeW7bm8sYnpT13Whuj/7NFeuZ2a1/kh8yBbnv8zO2Zh8zOVpsKT62f8Ju++8FwtL3K2Z4Rk4wiHwDA9JY6FdzZaj0r33vLRhWbN0uD72bhg+Ktj8rnS0v+Rnr9z2yqfEZEummftPPyRDujdfulnUsSz3/4Tal0xugfEYta8/e3Hxj63KKN0or7bYo7kyrzIYVRirc9utf+bZgmxwgo8gEAFLd0U+elM+y29mZp9gez8zllc6XyeVLfUWnfPYnweMvr0vO/I/V1JdZnbq+227Jqez4YLoMjjelGIkcbGMp0dNIvjApa+l2p5R6myTFhBEwAQHHyRwOPP2ePa9dbk/VxcVL/MamvM/nwjXuk5o/aKGHlHOmm55Kf7z9hz6dbB9n+0NB1kp2t0sGHbJo/KLLBlgJ0HUhsP1mzNnmdpj862XXA3jPzXKmsKvk8j15sr625KXmaPBh6J8NoU/ooOARMAEBx6jogdeywsLZoo7TkL21NZqh8HCfzRxX7kw8//H4LlzMiNmL50HnJz7sSe775Lunky8kjjQtXJ7ae9Eci3/ia5A1IZSkdV5yzgiZ/Wnu00clT++2+P8rq626X5CQv0EPUD+K7lk5O4KPgaFoiYAIAilNVg7TuBdvbfMX9tt3kii2SC9vU9XiF50glM5OPrdguW6MZC77QglxJpa2p3HlFfPRwrY1AvveWdLTZAmBwJLKk0qbha9Ym9u1++wELn35AG210UrL1mPVN0rpXUr4BT2r/iYXKgZ7Ems/BvsnZF5yCo2kpb/pgAgAw5VILWWpvluaviE9p10k9B8d+zuhJqTQl3O1emu6FFmYHe5IPH3nWRlZrb5bqbk70vPQN9ljoXLXdtqBcvsWO+702Zy1OHp30t7A8MzrpJfqGVtZJj16SfP7y+RZg334guZgocvvkFP74U/qSfa/+91uI+6rjDEYwAQDwBXcE6j8lqSSwLjPl/zLDc4c/z0Dn0Nen46UbERywyvauA7Zm0t+NyFexQFr3c3veb2+0fEuiOryzVep41ALaLW8kv9eVSO3bbYSwMmLh0g+iH+m077vv6NC1qAtX2yjvZIW9UNiaxgc13ke4LGB5M4LJTj4AgJwL7gjUdUDq/KWNBi7aKL3v49Jzn5SqPiiFz7Lp61T+6J+k5OnwDLkS66lZOVfq75Je3mCtjYJ6j0g7LpN6DksatFB47cOJ1kNvfM1GKS/6E9tNKGjB9dZ4PjhS6LdJeu8tC9eVdfFzBxx5yoLr7CVj/54yEYvajkRBLXczglnA6IMJAMBIgn00/fuSBdDTrVJzU+K1H+mU9myUjjw+yknjPTPTKZsn9Z+04iPn7FhwN6FQReKxP41f35S8beSijfb82w9Ic1dIx5tt7eZgj1Wct29PnG/ZD6T332WFP6/9hdT+4/TXFdkgrfxh9gOfX9BzptI9ZftLQmZeog8mAAATEeyj6d8PhS1ovr01+bX7PiWt+GEGJ/VkITON/mOSBu01sb6hW1X6jyMbpNt+ZSEstR3RJV9O7GB03SM2SjnYI7lyqf3B5PP923ctXO5eFd+haJjrOvjQ5FR0d+xIFBwt32LrShvvtZFZf10pCg4BEwCAsUoddfOrudu2WbV2RkaaQXSS1594Tdm8oS+55EvWoH3pd5OP+2sX1+yxwFZWJa36Sfwj+2ThVfEtMp109Enppf9sU+cnnh96XXV3SJE7pVDZxEcSU/tddrZKez8hnXOphUp/16GWe2wJwvLvpy8som9m3iNgAgAwVl0H0o+6zYjYGslw1ainGFlKyOs/NvQlr3xe6j4k/fT85ONPrrH+kW98LXGs6iLbZSjpIwIB9uhTNtUeqhj6OaESC3rrXphYFXm6fpcD3RZsjzdLezYk2iK1bbMK+tqbMzsPfTPzTt4U+QAAUDCCxUD+qN57b1mj9rkrpNOvSgvXSNF34qOCkhbcIB19xhqlZ8Ph3dJPahOPV++Rmj9mYa18fqKIp/E+qfnjUl+akHpGPGimTsdHNtg6ThdKtBIai+D61WC/y+g70qVflx5bZv8elXXWGmprfMtMVypd/KX0I6bB8/jfX3DNJn0z8wJFPgAAZIsfqLoO2Kja7lXS3EZpydetAvvUfmnPndYaaM7VUulMm6KeqPKF0tyltk7yjJCGVLKH51ifzkz5xUKHHku0QcqUP9JYuy5RqDPQLf30gkRvTl/dHdLBnyQej1ZQFFyi4Mu0ICgYeoPn6zow/hHayThnnqPIBwCAqeIXAVU1WKBc22JFNtVX2vHSGTbKuXCNNDMSD5cl6aem03HDbGPZdzgRLl1Z/GBKuAxV2NaTo6mss2p4f4cgKXkbykyl26HHbwAflBoupUT1/HDS9c286PNDX5e6LnMypteZsk+LgAkAwGQJVqD7j9e2SJd/M1Hlvez7Q6emh+P1ZfCa/vTHSyqljp9KCg3dy9zff72k0tZi7vuUrSmtb7LrHE9xj79DT2qV+4yUQa+T+5Ifu1ILtsHAlipd38zHrrb1myOFvMnYlpKtLtNiihwAgFzobLU9w/dssPWHctL86ywYHX92gidP6bNZcpY0+K7dD8+VoseTX77kr6QjT0qHn7DRw5IKae2+xHT/RKZ5B3oSayt9izZaM/hgP05fZZ005yrp8GPpC4vS9c187pOJ0dZFG6Vl3xu+l+ZEpteHMxnnzHNMkQMAkI+qGhK759Sslda+IF3zY+mdN0d/73BT5WekDB754VKycFmWss3l/j+zwKeYFLlDavyXxJpCP+Bl2gYo2EIo3UhjZZ3U8AVp6XcSI6e+igU2gnryBQ3bjzNdBf+K+y1Y+qOfwZ6gqSFvMralnMg5p2nLpbwJmM65Jufc1p6enlxfCgAAU8OfMr/2YWn2pdKzTYmWRPOvt/WJ6WQyVT6S/vgIZkll/MCg7T4UudOatLfcIz19mxXl+AEokzWFwfWIA92JUb3KOgt/kgXIxxqlRy62RvJBfV2JkDl/ZfrpZf/fLBgcQ2ELmWufT35tupA33LaUw03HZ2K855zG6zfzJmB6nrfN87xNlZWVo78YAIDpwl+n2XVAOtZs+5FHNkg37JIu/Uo8mDlJaUbDSmfb68drMGVQZ6DLpu3Lqmza/if10o6rpKdutaBYc5OFvlhUevMfrGApGIIq66xpets2m/rv2GlrLnsOSpHb7fuSrDVR39GhI5het/UR9fdXl9KHrNS1rb7WbyQ/Tg15IzXIH2nN50gmcs5pvH6TNZgAAOQLf7QwGKBO7bfg8fpX4ms148qqrUL9iRuk8DlSb0d2rqGi1s7lSiRvMHHclVpIXPQR6Y2vS52vSS4seSXSmp9J77VJr3zOqsTPuUw6/Urivf5UdWxg6HrMdD7SKb33f6TXv5rcImm41j+Z7meernWS/96OnWNvxSRN/Jyn9tv36a8hlWy6/5IvWyeCPDXaGkwCJgAA+SxdeNpzp4XN+iap/mPS3k9Kse7xnd+VS2Vnj9KIXTbamDqlfea5ikQlfKgiPmoXCKebuqVQ6dBCmOFUXSq902ojnYs22vS3NHxo62yVdi2VatbZa/2Q13yXdGinFSz5r8+nPph+OF24JrngKbLBGumPJ/BOEYp8AAAoZOmKWq592B537JRmfUCac7m9tmyeVHd74M3D9ZMskUpm2BpMr0/qz6D5+nDhUkqES1cWvz+Y/Pwzt0vPfiwRkjccHzo9HuwF2vlq8o5HsYHRp46HGzBLPZ5uej1YzDQe4z3nrMW27CC1mr59e2I5QoEiYAIAkM+GK2pZvsWOh8K2NWXNWun6HdZuaNHGeFV1hTRn6dBzVl8prX5GuvOQtOBGyYsNfY3vTOP2DCT14HTSB//K1mUe3i21/9im2S/4rPTcZgusLjji12uvDVq4eviq8GCl9azFUt2tif6Z/lrGtx+w45MZ1KZpFfhEMUUOAEChC07R+vdjUemZOyzcSRbWPFm1uGRh7tY3pOf/Q3z9X0rvzMkw/3qbKj+829aQRt+TvECTeVeaPHIZ2ZA8uveRTpvOH+i2IqJje5PXZ6ZOwfvT68EemNncvnGi6y+ZIgcAAHkrOEXr33/vrXhVeqkFresela5/1FoRyVll91O3Sh274tPVUzDg1PUrC04uLPWfGNpuyV9zuanbblOnjh+9WOo9YfuZH9plW1/6o5PpelEGB9Emo/3PRKvAqxpsG87UrTGdG9/2nHmkNNcXAAAAJkFVg+2Ek1qVvvJfrXL51T9Lrkp3ZSlT3GFJE+gNmU7PQbv1/PN6FiQv+hPrjSlPir6j5LAbH1ktqbQK9e3xJvEzItKq7cmjk6m9KNu3W6FP6s4+2Zoy95cqSHZuf/Q00118ghXkqbsSuVBB7wTECCYAANNVVYO1ukndyab6SmnVj5Nf6/XHWxFtiI9yZjlcplNZJ139v6Vf/ldJnj0+1iz9+n9KBx+x61lwg11Tas/OlT+SSuMtj9L1oly00Z4bbWefiRrvLj6drdJjy6X2B+1al2+x5QOSfd8dj9p0foEiYAIAUGzSjfbNiEg3tUgrfyh98M/jDd4nMSaUzbERzR9VJcLfNQ9a782XPyfF+m1XnyNPSMeeG/r+X3wzUVwz3PaRfmN332jBL9OCndG2w/QbvAffm3ruWYul2psTa079Svm3H7B+o2uaC3qKnIAJAEAxSTfaV7PWpp9/8df2mtlLpNXPShXzkt9bksXd9lJbI1Uvi1/fgKzNUSw+pR6W+g4lv7ayLlEx7i8BWNsiXfyl5Nelrm0cbvvGzlabrg5u2xiL2rHUdZup22E+fVt8atxZKF+4xh4/datNz+9qlNofGrol5JlrTLN/+or787rJeiZYgwkAQDFJHe0Lha2vpl/53HXARtde/3PbtnFGRLrlDal5k63ZTN3hxxdstj4eP/+sVH21tOD6RKW7pCFT9QtutFHNilqbRverwmNRafcKqfYWqfFe20/97Qfs+vtOS/NXJNZIBqfJ/cBYc5N9tW2ztk2eJx18yEYYg+s2g4U93R3S8ebE9+4NSKUzE62ZJHvvwtWJ90iJnYbefmBopfxIo6yT0SR+kuTNCKZzrsk5t7Wnp2f0FwMAgPEZra9mVUN8X/S9NrL54TetNdCKf403Zh+UQqm9MV2g2foEosWJ5+MBrXbk17lSqfeQjXb6o42vf1Ua6LUQ9+RNiX3Qu9stXK7anmhOH1zb6AdGf6vGyAa73749UdWe7t+qvsnCpZTo4Rm5Q2r/SaKYadFGG1XtOmCh19+j3B+tXLhmbKOsuxptxHQgvmuTPxq982obJc0j9MEEAABDpY6WtT9kO/KUVdv6yNp10oWfk55eL/UdTX7vSNtKShrSc7OiVqpeKh18WEN2AUp33pLKeNGPk+rWS4Pd8fZHpXa9wYKgGRGpv9PC86zF6Uf70u0H7tvUbWs6Uw30jL6v+tqXpMdX2b9X5HYraPpRVeCfIT4aPNL+6cFr/Pkf2/fpjyo//7uJUVr/e5yikUz2IgcAANnR/pBN9773ViJ8DnRL//a/rSCn5Xek2ZfZzkILV0vRTttZaDQbTkg9b0u7lme+p3p4jhQNrOOcf52FR3/0MGjRRumSL6df1zhcs3NfzVpbQhCcUp95rk3Bj7av+sI1UulZtouRZKOcSdfnrGJ/5Q9HbtJ+5hpXSyf2DT2HvMmpkB8BjdYBAEB2RNZba6BgX83SGdIFn5He1ySt2yet/lm8eXjIwqW//WPqNpBBO5fY8+GzMr+W1PWeR59KHy7L59t6yt2rLKilq+ZeuDp9uKyss3WnzXclKsJ3Xi09fH5ipHF189D31d2RWIc58G5if/jg9VXWSfKSp8hTlyoEr7F2nQXVOVelfJg3dBo/DxAwAQBAdvjBMxSWDj1mo3/RdyyI3fqGTYUHLbjeglZ3u/Rg/dCpdskKaMoWDD0+mOFIZ99RC481N1lI3NVogfHUfns+FrVRwXRmnW/T7u0PJkYuZ38wHhSddP4fWPHTGc76dh78ib2msk463iK9/xNDz31rq/27HHoseU1oKDx0mttfw7loo5071VXfybuemUyRAwCA7PPXcHYdGDqlXFkrnX2RVYNH7rTm6n1HpbkrpPM/I+29K/lcqdPhY+akNXutkfn+LybWa664X/r1P9nj1OlrV2ojn35V+/wbpFBIOvwzDVknWlknnX1BynKAEhuJDIXTr+9ctFFa9i+23CCT/cprbpL6T6VfclBSaZ+37vm8WYNJmyIAAJB9ftCparCQ5LdGqv+YTfeGwon1hjc+aSN9H/hdC6JBZdW2b/mEeFaMFDyPNyA922T3518vlZ2TWCsp2RS/1y8NdEly0tFAsJt3nXTsqcTjm/fb9xMs4Km+yr73vZ9InNeVSguukw4/ngicK+4f+dKDbZFSlddIA6etqKligQX5PMEIJgAAmHyj9XBMbQAfrKqebKEyq/Qun588Te/CgX3TA1y55AWq5CtqpMG+oaOsC1ZLRwIjnos2Sg1fkN74mhVMlZRJ614cvrrdd+JF6Zk70q8xlSxcDnTnVRU5azABAMDkCxYG+YLrDdNt99h4r7XgmWyxfrvtOyqVzEwcTxcupUS4LJtrt72HksNl+Xy7PfK4zoTLyjoLl09cL1ureZ2Nkp7pZXlVYl3omeuKWhB9/Fprej+cy//blIbLTDCCCQAA8kNwlDM4ojl3hXTylXgLowxzS3nN0C0mhwhJiqU5PFofzxHeO6QVUUDkTptyPxzfqSiywarI337Aps8jt9uUefD7P7hDmrcssTNQOq7URkKncHtJRjABAEBhCI5yBkc0b3hMmnOZEuEypKERxlkxkG/UcCmlDYhSBuFyhPfO/lD64+XzbS3m4cA2mO3bLVwu2mjh8u0HrA1S74lEuK5dO/we8H5VvjdghUTpdgDKEUYwAQBAfgpWou9qlOYtlz74F1JJhdT1b9KzmyQNSnOWSlf/kxW5PHx++nZHueJKbG/zkUZeV2yV6m6VflKfXIhU32S7JT22bPj3L1wjhc+WOnZJK7ZYr9IpwE4+AACg8KUrEjq134Jm5LbE8RMvSbuWKnmEMWVryuHMXZHYX3yi5l1j5/JG2PryDGdbZZ54PvnwDU9Kz33SptxDFUOby0u2laU8ac8G6WjzlIVMAiYAACguJ16Wdl0ROFCiEfc49w0X4lQqaWCYN2UYXtN/oIadak9n0UYpNpjcbL12vVQStun1KawmZw0mAAAoHrGo9ItvpBwclGZfKQuDI703XbiUhg+XUtK60DlXaWzRaphwWV6TcsDZ9pOeN3Qnn46HLFyWVFql+bzlNtKbYwRMAAAwPaT20lz7klVYS1J3myTPwlt4duBNaUKnG8ee3iWV1lJp/rXjufJkQwqUPCnaKR18aPj3DPbY56/anhd7khMwAQDA9JDaS7P6Q9a+Z+GaxFaU1z1sVddzl0m1t6U5Sby5euTOoXunjyRcZVXiR38mzb1m6POhipHfP1qoPfozu25JKluY6LUZtO4VqXRGRpc72fImYDrnmpxzW3t6enJ9KQAAoBBVNdj6w+VbEqN4s5dI1z0iXfOgtPpnUvUV9pobHreddOSlNHOPWi9LOam3Q1pww8ifWVlno6S9HTozGnr8maGvG3b6PW7OVZl9j5JUEkpfKb/zMqm/0wqicowiHwAAUHw6W631Ue06ael3k/cRD82QQk6qWWeP/X3DU1XWSX2npeX/IrX9UHr7p5KX4UBZRs3cRzyBktZwllQm9iSfgkIfinwAAABS+aOdjfdK+z6V/NyCVdKNT9l9vxH6gtWB51fbFHrPQWnBSmuT1HivTckHjTQtPqFwKUmxRAP2GRELl6HyvCn0Kc3ppwMAAOTKrMXJRUGN90ktd9tjSTq2145f/CVp9yoLmpJ06DFpzR4pVGprPjtbpTe+Zn0v566w/pzeQGJavO426fjzmTWAL5sn9R9LPubCafZFD0lL/5fU/kBihDXWJ1Uvky7765wX+hAwAQBAcUotCgqF7Vay48u/L9XebMfXtiRGBbsO2Ajo8i1233+9f56OHVLzXZKc7eRz6VftvU+vl448qWH7ZobK4+EypbfmkHAp6dK/kl74tBUwBVXWWBiegl6YI2ENJgAAKF7pdgiKRRMhcrzn6WyVBnpslHP2Ejtn81022rhwjVQyM9HTsrJOqoxIJ+M7+dTdLg10xcNoCn/t5qKN1hezffvQ1yzaKK24f1JHMVmDCQAAMJyqhqFBLBQe++hf6nmqGqxiffYSe9x1wKbW65uky78pHXnC1nFW1tlaTj9cqkS65M+kstlDPkKSdMdBO0fHLguheYqACQAAMNmCLZRmL7H7K/9VujWlpdBNzdIv/sZGOuvuGHqeFz5tBUUrtkjHW6TIhuTnIxssyPpT9zlCwAQAAJgKwVFOf4Q0tYL9tT+39ZyRO6WTL6ScwFnwbLnH1oau2SO5lJ2InLPjOVx/KREwAQAApl7qtpabuu320C5pbqMU7bKp8xkR6SOd8Qr2eFP4YOX62w8kv//tB+x4LE1h0BSiihwAAGCqjVTBfvARG4msWWt7i5fOsKKdvZulgztsejwUHrkCfqxFSllGFTkAAEAujFTBLo1e3Z6tCvhxGK2KnIAJAACAMaFNEQAAAKYUARMAAABZRcAEAABAVhEwAQAAkFUETAAAAGQVARMAAABZRcAEAABAVhEwAQAAkFV5EzCdc03Oua09PT25vhQAAABMQN4ETM/ztnmet6mysjLXlwIAAIAJyJuACQAAgOmBgAkAAICsImACAAAgqwiYAAAAyCoCJgAAALKKgAkAAICscp7n5foakjjn+iQdm8KPPEvSu1P4ecg+foaFj59hYePnV/j4GRa+qf4ZzvM8r3y4J/MuYE4151y753mRXF8Hxo+fYeHjZ1jY+PkVPn6GhS/ffoZMkQMAACCrCJgAAADIKgKm9K1cXwAmjJ9h4eNnWNj4+RU+foaFL69+hkW/BhMAAADZxQgmAAAAsoqACQAAgKwiYAIAACCrijZgOucWO+f2Oud+5Zx7wTl3ca6vCcmccxXOuZ/Ef0b7nXO7nXPnxZ+b75zb6Zw74Jx73Tl3TeB9wz6H3HHO/Y5zznPO3RF/zM+wQDjnyp1z/xD/ebzmnPt+/Piwf0f5G5s/nHO3OOdeds69Ev99ujt+nN/BPOWc+zvn3Fvxv5mXBY6P63cuJ7+PnucV5ZekJyXdE7//EUkv5Pqa+BryM6qQdIsSxWifkfRU/P7/lvSV+P2rJLVLCo/2HF85+1meK2mvpOck3cHPsLC+JP03SX8f+F1cGL8d9u8of2Pz40uSk3RS0qXxx+dK6pU0i9/B/P2SdI2kiKS3JF0WOD6u37lc/D7m/B8xRz+4+ZLekVQaf+wkHZZ0Xq6vja8Rf25XSnorfv9d///k4o/3SVo92nN85eTnFpL0uKQrJD0VCJj8DAvgS9LM+N/Ls1OOD/t3lL+x+fMV/7c/Iema+ONLJR2UVMbvYP5/BQPmeH/ncvX7WKxT5IskHfI8b0CSPPsXb5NUn9Orwmj+QNKDzrlq2X9JHw4895ak+pGem7KrRKrPSmr2PO8l/wA/w4LyAdkI2Bedcy865/Y4527UyH9H+RubJ+L/9h+VtN059xtJz0q6WzaCye9gYRnv71xOfh+LNWCiwDjnvij7L7Ev5PpakDnn3CWSNkr6L7m+FoxbqaT3SWr1PO9KSb8v6V/jx5HnnHOlkv5M0gbP894n6UZJ3xM/P0yyYg2Yb0uqif/iyTnnZEm+LadXhbScc38kaYOkmz3P6/Y874SkAefcwsDLzpXUNtJzU3W9SLJK9u9/wDn3lqRGSf8kaZP4GRaKNkkxSVskyfO8n0v6P7LQOdzfUf7G5o/LJNV6nveMJHme94JsPeWl4new0Iz0ezXe5yZNUQZMz/OOSnpZ0ifihzZKavc879e5uyqk45z7rKS7JK3xPO904Kltkn4v/pqrJNVJejqD5zCFPM/7H57n1Xied67needKapH0f3ue9z/Ez7AgeJ53XNITktZKknPu/ZLeL6lZw/wd5W9sXvHDxUWSFO/E8QFJb4rfwYIy0u/VeJ+bzOst2q0inXMXSLpXUrVs8evveJ73Wk4vCkmccxHZH8d/l9QVP9zned7VzrkFsmme90vql/QZz/N+Fn/fsM8ht5xzT0n6tud5P+FnWDicc78l6X9JmisbzfxLz/MeGOnvKH9j84dz7i5JX5T97EKS/srzvB/wO5i/nHPfkXSrpIWyIq0uz/POG+/vXC5+H4s2YAIAAGByFOUUOQAAACYPARMAAABZRcAEAABAVhEwAQAAkFUETAAAAGQVARMAAABZRcAEAABAVhEwAQAAkFX/P7/RfSDne5WtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 800x640 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn2.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e2ceeda-d65f-4fd8-a7e9-90f1ccbea332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.61751627],\n",
       "       [-0.43862728],\n",
       "       [-0.2799403 ],\n",
       "       [-0.14145535],\n",
       "       [-0.02317242],\n",
       "       [ 0.07490849],\n",
       "       [ 0.15278738],\n",
       "       [ 0.21046426],\n",
       "       [ 0.24793911],\n",
       "       [ 0.26521194],\n",
       "       [ 0.26228275],\n",
       "       [ 0.23915154],\n",
       "       [ 0.19581831],\n",
       "       [ 0.13228306],\n",
       "       [ 0.04854579],\n",
       "       [-0.05378666],\n",
       "       [ 0.05704794],\n",
       "       [ 0.14768051],\n",
       "       [ 0.21811107],\n",
       "       [ 0.26833961],\n",
       "       [ 0.29836612],\n",
       "       [ 0.30819062],\n",
       "       [ 0.2978131 ],\n",
       "       [ 0.26723355],\n",
       "       [ 0.21645199],\n",
       "       [ 0.1454684 ],\n",
       "       [ 0.0542828 ],\n",
       "       [-0.05710483],\n",
       "       [ 0.04262425],\n",
       "       [ 0.14070582],\n",
       "       [ 0.21858537],\n",
       "       [ 0.27626289],\n",
       "       [ 0.3137384 ],\n",
       "       [ 0.33101189],\n",
       "       [ 0.32808335],\n",
       "       [ 0.3049528 ],\n",
       "       [ 0.35962731],\n",
       "       [ 0.43197935],\n",
       "       [ 0.48412937],\n",
       "       [ 0.51607736],\n",
       "       [ 0.52782334],\n",
       "       [ 0.5193673 ],\n",
       "       [ 0.49070924],\n",
       "       [ 0.44184915],\n",
       "       [ 0.37278705],\n",
       "       [ 0.28352293],\n",
       "       [ 0.17405679],\n",
       "       [ 0.04438862],\n",
       "       [-0.10548156],\n",
       "       [-0.07655323],\n",
       "       [ 0.17849679],\n",
       "       [ 0.4133448 ],\n",
       "       [ 0.62799078],\n",
       "       [ 0.82755157],\n",
       "       [ 1.1133916 ],\n",
       "       [ 1.3790296 ],\n",
       "       [ 1.62446559],\n",
       "       [ 1.84969956],\n",
       "       [ 2.0547315 ],\n",
       "       [ 2.23956143],\n",
       "       [ 2.40418933],\n",
       "       [ 2.54861522],\n",
       "       [ 2.67283909],\n",
       "       [ 2.77686093],\n",
       "       [ 2.86068076],\n",
       "       [ 2.92429856],\n",
       "       [ 2.96771435],\n",
       "       [ 2.99092811],\n",
       "       [ 2.99393985],\n",
       "       [ 2.97674958],\n",
       "       [ 2.93935728],\n",
       "       [ 2.88176297],\n",
       "       [ 2.80396663],\n",
       "       [ 2.70596827],\n",
       "       [ 2.5877679 ],\n",
       "       [ 2.4493655 ],\n",
       "       [ 2.29076108],\n",
       "       [ 2.11195464],\n",
       "       [ 1.91294619],\n",
       "       [ 1.69373571],\n",
       "       [ 1.45432321],\n",
       "       [ 1.19470869],\n",
       "       [ 0.91489215],\n",
       "       [ 0.61487359],\n",
       "       [ 0.29465302],\n",
       "       [-0.04576958],\n",
       "       [-0.4063942 ],\n",
       "       [-0.78722084],\n",
       "       [-1.1882495 ],\n",
       "       [-1.60948018],\n",
       "       [-2.05091288],\n",
       "       [-2.5125476 ],\n",
       "       [-2.99438434],\n",
       "       [-3.4964231 ],\n",
       "       [-4.01866389],\n",
       "       [-4.56110669],\n",
       "       [-5.12375151],\n",
       "       [-5.70659835],\n",
       "       [-6.30964721]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn2(x3, mode='asd')-x4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

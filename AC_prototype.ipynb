{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ba32a7-6ceb-4ae0-8f0d-90f5d64f1727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an attempt on Actor-Critic A2C\n",
    "# not part of the project as I haven't figure out the correct feedback\n",
    "\n",
    "# code modified from\n",
    "# https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41b6da6-103a-4599-95ff-390e91668ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# - implement it\n",
    "# - test whether sharing first layers help improving performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "276b8cce-a207-4f3f-be53-467bb63bc090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gc\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import axelrod as axl\n",
    "from time import time\n",
    "from pprint import pprint\n",
    "from itertools import permutations\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "\n",
    "import network\n",
    "from axl_utils import State, set_match, set_play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c262f71-1065-4d58-8287-a945f09b37ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 3), (3, 3), (5, 0), (0, 5), (5, 0), (1, 1), (1, 1), (0, 5), (3, 3), (5, 0), (0, 5), (5, 0), (0, 5), (3, 3), (5, 0), (0, 5), (5, 0), (1, 1), (0, 5), (5, 0)]\n",
      "Player 1 score = 50\n",
      "Player 2 score = 45\n"
     ]
    }
   ],
   "source": [
    "C = axl.Action.C\n",
    "D = axl.Action.D\n",
    "\n",
    "# config game rules\n",
    "GAME_LEN = 20 + 1\n",
    "GAME = axl.Game(r=3, s=0, t=5, p=1)\n",
    "Match = set_match(game=GAME, turns=GAME_LEN)\n",
    "play = set_play(Match)\n",
    "\n",
    "game = play(axl.Prober4(), axl.TitForTat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f0a384f-a3a4-473a-a37d-c33865f76707",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNplayer(axl.Player):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    name = 'NNplayer'\n",
    "    classifier = {\n",
    "        'memory_depth': -1,\n",
    "        'stochastic': False,\n",
    "        'inspects_source': False,\n",
    "        'manipulates_source': False,\n",
    "        'manipulates_state': False\n",
    "    }\n",
    "    \n",
    "    decision = (axl.Action.C, axl.Action.D)\n",
    "    \n",
    "    def __init__(self, network, state, gamma=0.999, mode=\"dense\", N=-1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = network\n",
    "        self.state   = state\n",
    "        \n",
    "        self.gamma   = gamma\n",
    "        \n",
    "        self.mode = 1 if mode==\"dense\" else 0\n",
    "        self.N = -1\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state.reset()\n",
    "        self.reward = 0\n",
    "        self.network.reset_state()\n",
    "        \n",
    "    def strategy(self, opponent):\n",
    "        \"\"\"Query the network to make decision\"\"\"\n",
    "        idx = self.network.query(self.state.values())\n",
    "        return self.decision[idx]\n",
    "    \n",
    "    # overwrite update_history to update self state\n",
    "    # this function is automatically called by axelrod library\n",
    "    def update_history(self, *args):\n",
    "        self.history.append(*args)\n",
    "        self.update_state(*args)\n",
    "        \n",
    "    def update_state(self, play, coplay):\n",
    "        \"\"\"update current game state & record transition into replay memory\"\"\"\n",
    "        s  = self.state.values()\n",
    "        s_ = self.state.push(play, coplay)\n",
    "        \n",
    "        # reward\n",
    "        r  = axl.interaction_utils.compute_scores([(play, coplay)])[0][0]\n",
    "        \n",
    "        self.network.rewards.append(r)\n",
    "#         # dense reward\n",
    "#         if self.mode:\n",
    "#             r  = r if s[0,0,1]==-1 else np.NaN  # set last turn reward to NaN\n",
    "#             self.memory.push(s, play, s_, r)\n",
    "        \n",
    "#         # sparse reward\n",
    "#         else:\n",
    "#             if s[0,0,1]==self.N:\n",
    "#                 self.memory.push(s, play, s_, 0)\n",
    "#                 self.reward += r\n",
    "#             else:\n",
    "#                 self.memory.push(s, play, s_, r+self.reward)\n",
    "#                 self.reward = 0\n",
    "        \n",
    "    def on_policy_train(self, epoch, param):\n",
    "        param['t'] = 1\n",
    "        length = len(self.memory)\n",
    "        for _ in range(epoch):\n",
    "            # organize data\n",
    "            ts = Transition(*zip(*self.memory.sample(length)))\n",
    "            ss  = np.vstack(ts.state)\n",
    "            ss_ = np.vstack(ts.next_state)\n",
    "            ats = np.array([[True, False] if a==axl.Action.C else [False, True] for a in ts.action])\n",
    "            rs  = np.array(ts.reward, ndmin=2).T\n",
    "            \n",
    "            # pass to network\n",
    "            self.network.learn((ss, ss_, ats, rs), param, self.gamma)\n",
    "        \n",
    "        self.network.update_target()\n",
    "        self.loss = self.network.loss\n",
    "              \n",
    "    def off_policy_train(self, param):\n",
    "        self.network.learn(param)\n",
    "    \n",
    "    def plot(self, **kwargs):\n",
    "        self.network.plot(**kwargs)\n",
    "        \n",
    "    # test mode using \"with\" statement\n",
    "    def __enter__(self, *args):\n",
    "        self.network.test_mode(True)\n",
    "    \n",
    "    def __exit__(self, *args):\n",
    "        self.network.test_mode(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3bb07941-44da-4fef-93fc-e4b133314395",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C():\n",
    "    \n",
    "    def __init__(self, actor, critic, gamma=0.9):\n",
    "        \n",
    "        self.actor  = actor\n",
    "        self.critic = critic\n",
    "        self.gamma = gamma\n",
    "        self.test = False\n",
    "        \n",
    "        self.reset_state()\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.actions = []  # [(log_prob(chosen action), state_value)]\n",
    "        self.rewards = []  # [reward from env]\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \n",
    "        # probability of actions :: 1x[n actions] array\n",
    "        probs = self.actor(state)[0]\n",
    "        \n",
    "        # state value :: 1x1 array\n",
    "        value = self.critic(state)\n",
    "        \n",
    "        return probs, value\n",
    "    \n",
    "    def __call__(self, *args):\n",
    "        return self.forward(*args)\n",
    "    \n",
    "    def query(self, state):\n",
    "        \n",
    "        probs, value = self.forward(state)\n",
    "        \n",
    "        if self.test:\n",
    "            print(probs)\n",
    "            return probs.argmax()\n",
    "        \n",
    "        # sample action\n",
    "        cum_probs = np.cumsum(probs)\n",
    "        action = (cum_probs > np.random.uniform()).argmax()  # Int index of action\n",
    "        \n",
    "        # save\n",
    "        self.actions.append((action, np.log(probs[action]),value))  # (Int, 1x1 array, 1x1 array)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def learn(self, param):\n",
    "        \n",
    "        self.actor.set_optimizer(param)\n",
    "        self.critic.set_optimizer(param)\n",
    "        self.actor.set_loss_func('mse')\n",
    "        self.critic.set_loss_func('mse')\n",
    "        \n",
    "        # cumulative discounted reward a.k.a \"true\" value\n",
    "        returns = []\n",
    "        cum_r = 0\n",
    "        for R in self.rewards[::-1]:\n",
    "            cum_r = R + self.gamma * cum_r\n",
    "            returns.insert(0, cum_r)\n",
    "            \n",
    "        # standardize for better convergence\n",
    "        returns = (returns - np.mean(returns)) / np.std(returns)\n",
    "        \n",
    "        # calculate losses\n",
    "        policy_losses = []\n",
    "        value_losses = []\n",
    "        for (action, log_prob, value), R in zip(self.actions, returns):\n",
    "            \n",
    "            advantage = R - value\n",
    "            policy = np.array([1, 0]) if action==0 else np.array([0,1])  # HARDCODED FOR NOW\n",
    "\n",
    "            # record losses\n",
    "            policy_losses.append(log_prob * advantage * policy)  # DOUBLE CHECK THIS !!\n",
    "            value_losses.append(self.critic.loss_fn(R, value)[0])\n",
    "        \n",
    "        # sum all losses then feedback to networks\n",
    "        policy_loss = np.array(np.sum(policy_losses))\n",
    "        value_loss  = np.array(np.sum(value_losses))\n",
    "        print(policy_loss, value_loss)\n",
    "        self.actor.backprop(policy_loss, param)\n",
    "        self.critic.backprop(value_loss, param)\n",
    "        \n",
    "    def test_mode(self, on):\n",
    "        if on:\n",
    "            self.test = True\n",
    "        else:\n",
    "            self.test = False\n",
    "            \n",
    "    def plot(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a62a5c2e-8c46-4502-9fa5-5d10c2d47803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1 = network.Linear_layer(GAME_LEN*2, 100)\n",
    "\n",
    "actor = network.NeuralNetwork([\n",
    "                    network.Flatten_layer(),\n",
    "                    layer1,\n",
    "                    network.Activation_layer('ReLU'),\n",
    "                    network.Linear_layer(100, 40),\n",
    "                    network.Activation_layer('ReLU'),\n",
    "                    network.Linear_layer(40, 2),\n",
    "                    network.Activation_layer('Softmax')\n",
    "                    ])\n",
    "critic = network.NeuralNetwork([\n",
    "                    network.Flatten_layer(),\n",
    "                    layer1,\n",
    "                    network.Activation_layer('ReLU'),\n",
    "                    network.Linear_layer(100, 200),\n",
    "                    network.Activation_layer('ReLU'),\n",
    "                    network.Linear_layer(200, 1),\n",
    "                    ])\n",
    "\n",
    "p2 = NNplayer(A2C(actor, critic), State(GAME_LEN, C=1, D=-1, N=0.1), gamma=0.9)\n",
    "param = {\"lr\": 3e-4, 'batch': 16, \"momentum\": 0.9, \"mode\": \"train\", \"eps\": 1e-16, \"beta\":(0.9, 0.999), \n",
    "         \"epoch\": 0, 'optimizer': 'adam', 't': 1, 'clip': 1.0, 'decay': 0.0}\n",
    "\n",
    "del actor, critic\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b96e987f-3f5a-4df9-9435-7ac82bc7525d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.477 0.523]\n",
      "[0.526 0.474]\n",
      "[0.297 0.703]\n",
      "[0.498 0.502]\n",
      "[0.356 0.644]\n",
      "[0.77 0.23]\n",
      "[0.642 0.358]\n",
      "[0.51 0.49]\n",
      "[0.265 0.735]\n",
      "[0.863 0.137]\n",
      "[0.537 0.463]\n",
      "[0.632 0.368]\n",
      "[0.249 0.751]\n",
      "[0.178 0.822]\n",
      "[0.594 0.406]\n",
      "[0.334 0.666]\n",
      "[0.375 0.625]\n",
      "[0.127 0.873]\n",
      "[0.725 0.275]\n",
      "[0.368 0.632]\n",
      "[0.798 0.202]\n",
      "[(5, 0), (0, 5), (5, 0), (1, 1), (1, 1), (0, 5), (3, 3), (3, 3), (5, 0), (0, 5), (3, 3), (3, 3), (5, 0), (1, 1), (0, 5), (5, 0), (1, 1), (1, 1), (0, 5), (5, 0)]\n",
      "Player 1 score = 47\n",
      "Player 2 score = 42\n"
     ]
    }
   ],
   "source": [
    "with p2:\n",
    "    play(p2, axl.TitForTat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8320f29d-f825-4b47-8b01-a45f47705877",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run inifinitely many episodes\n",
    "for i_episode in range(1000):\n",
    "\n",
    "    play(p2, axl.TitForTat())\n",
    "\n",
    "    # perform backprop\n",
    "    p2.off_policy_train(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c8484f04-6c3d-4c59-b7e8-14701b9fc5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.38 0.62]\n",
      "[0.254 0.746]\n",
      "[0.32 0.68]\n",
      "[0.619 0.381]\n",
      "[0.628 0.372]\n",
      "[0.327 0.673]\n",
      "[0.106 0.894]\n",
      "[0.618 0.382]\n",
      "[0.753 0.247]\n",
      "[0.241 0.759]\n",
      "[0.022 0.978]\n",
      "[0.336 0.664]\n",
      "[0.439 0.561]\n",
      "[0.174 0.826]\n",
      "[0.841 0.159]\n",
      "[0.872 0.128]\n",
      "[0.404 0.596]\n",
      "[0.037 0.963]\n",
      "[0.777 0.223]\n",
      "[0.821 0.179]\n",
      "[0.03 0.97]\n",
      "[(5, 0), (1, 1), (1, 1), (0, 5), (3, 3), (5, 0), (1, 1), (0, 5), (3, 3), (5, 0), (1, 1), (1, 1), (1, 1), (1, 1), (0, 5), (3, 3), (5, 0), (1, 1), (0, 5), (3, 3)]\n",
      "Player 1 score = 40\n",
      "Player 2 score = 40\n"
     ]
    }
   ],
   "source": [
    "with p2:\n",
    "    play(p2, axl.TitForTat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "95d93ac8-7a6b-4c37-8da2-5714df6f09a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--0--\n",
      "Printing flatten layer:\n",
      "{'optimizer': <function Layer.set_optimizer.<locals>.optimizer at 0x000001A2FF68F310>,\n",
      " 'shape': (1, 2, 21),\n",
      " 'type': 'flatten'}\n",
      "--1--\n",
      "Printing linear layer:\n",
      "{'bias': 0,\n",
      " 'input': array([[ 0,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1,  0,  1,  1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]),\n",
      " 'input_nodes': 42,\n",
      " 'm1': array([[ 0.000e+00,  0.000e+00,  0.000e+00, ...,  0.000e+00,  0.000e+00,\n",
      "         0.000e+00],\n",
      "       [ 7.783e-04,  3.708e-70,  5.804e-04, ..., -7.346e-05, -5.291e-04,\n",
      "        -2.403e-05],\n",
      "       [-6.379e-03,  1.233e-69, -1.322e-03, ...,  6.213e-05, -1.711e-04,\n",
      "        -6.586e-04],\n",
      "       ...,\n",
      "       [-1.251e-03,  6.910e-70, -1.493e-03, ..., -6.213e-05,  1.711e-04,\n",
      "        -6.586e-04],\n",
      "       [-1.251e-03, -1.153e-69, -1.493e-03, ..., -6.213e-05,  1.711e-04,\n",
      "        -6.586e-04],\n",
      "       [-1.251e-03, -1.290e-69, -1.493e-03, ..., -6.213e-05,  1.711e-04,\n",
      "        -6.586e-04]]),\n",
      " 'm2': array([[0.000e+00, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
      "        0.000e+00],\n",
      "       [2.594e-03, 5.719e-05, 5.680e-04, ..., 1.639e-04, 4.234e-04,\n",
      "        6.850e-04],\n",
      "       [2.594e-03, 5.719e-05, 5.680e-04, ..., 1.639e-04, 4.234e-04,\n",
      "        6.850e-04],\n",
      "       ...,\n",
      "       [2.594e-03, 5.719e-05, 5.680e-04, ..., 1.639e-04, 4.234e-04,\n",
      "        6.850e-04],\n",
      "       [2.594e-03, 5.719e-05, 5.680e-04, ..., 1.639e-04, 4.234e-04,\n",
      "        6.850e-04],\n",
      "       [2.594e-03, 5.719e-05, 5.680e-04, ..., 1.639e-04, 4.234e-04,\n",
      "        6.850e-04]]),\n",
      " 'optimizer': <function Layer.set_optimizer.<locals>.optimizer at 0x000001A29E54AF70>,\n",
      " 'output': array([[ 0.639, -4.41 ,  1.837,  0.134, -2.175,  0.363,  1.066, -0.737,\n",
      "        -0.3  , -1.22 , -0.312, -0.466,  0.236, -2.289, -1.577, -0.855,\n",
      "         0.694, -0.428,  0.239, -0.723, -0.843, -0.322, -0.108,  1.342,\n",
      "        -0.394,  1.149, -0.955,  1.333, -1.315,  1.427,  1.245,  0.352,\n",
      "        -2.219,  1.161, -0.617, -2.319,  2.012,  1.494, -0.681, -2.611,\n",
      "         1.138, -0.486, -1.042,  1.252,  0.031,  0.51 ,  1.092, -0.463,\n",
      "        -0.948,  0.12 , -0.907,  2.183,  0.645, -1.793,  1.041, -2.866,\n",
      "        -0.539, -1.636,  0.43 , -0.437, -2.829, -0.288,  0.872,  0.919,\n",
      "        -2.491, -1.148,  1.606, -0.531,  0.43 ,  1.652, -0.034,  1.696,\n",
      "         0.48 ,  0.29 ,  0.618, -2.604, -0.851, -0.887, -2.825, -0.411,\n",
      "        -3.071,  0.971,  1.123, -2.5  , -2.283, -1.026,  2.404,  2.475,\n",
      "        -0.535, -1.468, -1.588,  0.844,  0.235, -0.158,  1.216,  0.794,\n",
      "        -0.626, -0.26 , -1.099, -0.844]]),\n",
      " 'output_nodes': 100,\n",
      " 'type': 'linear',\n",
      " 'weights': array([[ 0.163,  0.037, -0.028, ..., -0.32 , -0.284, -0.054],\n",
      "       [ 0.215, -0.087,  0.035, ...,  0.135,  0.004,  0.093],\n",
      "       [ 0.059,  0.307,  0.193, ..., -0.463,  0.325,  0.187],\n",
      "       ...,\n",
      "       [-0.097, -0.043, -0.288, ..., -0.333,  0.047, -0.011],\n",
      "       [ 0.048,  0.051, -0.219, ..., -0.208, -0.333,  0.31 ],\n",
      "       [ 0.059,  0.303, -0.429, ...,  0.121,  0.146,  0.033]]),\n",
      " 'with_bias': True}\n",
      "--2--\n",
      "Printing activation layer:\n",
      "{'cache': cache(x=array([[ 0.639, -4.41 ,  1.837,  0.134, -2.175,  0.363,  1.066, -0.737,\n",
      "        -0.3  , -1.22 , -0.312, -0.466,  0.236, -2.289, -1.577, -0.855,\n",
      "         0.694, -0.428,  0.239, -0.723, -0.843, -0.322, -0.108,  1.342,\n",
      "        -0.394,  1.149, -0.955,  1.333, -1.315,  1.427,  1.245,  0.352,\n",
      "        -2.219,  1.161, -0.617, -2.319,  2.012,  1.494, -0.681, -2.611,\n",
      "         1.138, -0.486, -1.042,  1.252,  0.031,  0.51 ,  1.092, -0.463,\n",
      "        -0.948,  0.12 , -0.907,  2.183,  0.645, -1.793,  1.041, -2.866,\n",
      "        -0.539, -1.636,  0.43 , -0.437, -2.829, -0.288,  0.872,  0.919,\n",
      "        -2.491, -1.148,  1.606, -0.531,  0.43 ,  1.652, -0.034,  1.696,\n",
      "         0.48 ,  0.29 ,  0.618, -2.604, -0.851, -0.887, -2.825, -0.411,\n",
      "        -3.071,  0.971,  1.123, -2.5  , -2.283, -1.026,  2.404,  2.475,\n",
      "        -0.535, -1.468, -1.588,  0.844,  0.235, -0.158,  1.216,  0.794,\n",
      "        -0.626, -0.26 , -1.099, -0.844]]), y=array([[ 0.639, -0.   ,  1.837,  0.134, -0.   ,  0.363,  1.066, -0.   ,\n",
      "        -0.   , -0.   , -0.   , -0.   ,  0.236, -0.   , -0.   , -0.   ,\n",
      "         0.694, -0.   ,  0.239, -0.   , -0.   , -0.   , -0.   ,  1.342,\n",
      "        -0.   ,  1.149, -0.   ,  1.333, -0.   ,  1.427,  1.245,  0.352,\n",
      "        -0.   ,  1.161, -0.   , -0.   ,  2.012,  1.494, -0.   , -0.   ,\n",
      "         1.138, -0.   , -0.   ,  1.252,  0.031,  0.51 ,  1.092, -0.   ,\n",
      "        -0.   ,  0.12 , -0.   ,  2.183,  0.645, -0.   ,  1.041, -0.   ,\n",
      "        -0.   , -0.   ,  0.43 , -0.   , -0.   , -0.   ,  0.872,  0.919,\n",
      "        -0.   , -0.   ,  1.606, -0.   ,  0.43 ,  1.652, -0.   ,  1.696,\n",
      "         0.48 ,  0.29 ,  0.618, -0.   , -0.   , -0.   , -0.   , -0.   ,\n",
      "        -0.   ,  0.971,  1.123, -0.   , -0.   , -0.   ,  2.404,  2.475,\n",
      "        -0.   , -0.   , -0.   ,  0.844,  0.235, -0.   ,  1.216,  0.794,\n",
      "        -0.   , -0.   , -0.   , -0.   ]])),\n",
      " 'func_backward': <function <lambda> at 0x000001A29C3298B0>,\n",
      " 'func_forward': <function <lambda> at 0x000001A29C329820>,\n",
      " 'func_name': 'relu',\n",
      " 'optimizer': <function Layer.set_optimizer.<locals>.optimizer at 0x000001A29E54AC10>,\n",
      " 'type': 'activation'}\n",
      "--3--\n",
      "Printing linear layer:\n",
      "{'bias': 0,\n",
      " 'input': array([[ 0.639, -0.   ,  1.837,  0.134, -0.   ,  0.363,  1.066, -0.   ,\n",
      "        -0.   , -0.   , -0.   , -0.   ,  0.236, -0.   , -0.   , -0.   ,\n",
      "         0.694, -0.   ,  0.239, -0.   , -0.   , -0.   , -0.   ,  1.342,\n",
      "        -0.   ,  1.149, -0.   ,  1.333, -0.   ,  1.427,  1.245,  0.352,\n",
      "        -0.   ,  1.161, -0.   , -0.   ,  2.012,  1.494, -0.   , -0.   ,\n",
      "         1.138, -0.   , -0.   ,  1.252,  0.031,  0.51 ,  1.092, -0.   ,\n",
      "        -0.   ,  0.12 , -0.   ,  2.183,  0.645, -0.   ,  1.041, -0.   ,\n",
      "        -0.   , -0.   ,  0.43 , -0.   , -0.   , -0.   ,  0.872,  0.919,\n",
      "        -0.   , -0.   ,  1.606, -0.   ,  0.43 ,  1.652, -0.   ,  1.696,\n",
      "         0.48 ,  0.29 ,  0.618, -0.   , -0.   , -0.   , -0.   , -0.   ,\n",
      "        -0.   ,  0.971,  1.123, -0.   , -0.   , -0.   ,  2.404,  2.475,\n",
      "        -0.   , -0.   , -0.   ,  0.844,  0.235, -0.   ,  1.216,  0.794,\n",
      "        -0.   , -0.   , -0.   , -0.   ]]),\n",
      " 'input_nodes': 100,\n",
      " 'm1': array([[-7.619e-05, -1.336e-04,  1.958e-40, ...,  9.546e-19,  4.076e-38,\n",
      "        -8.170e-05],\n",
      "       [ 6.233e-37,  3.424e-36,  5.822e-44, ..., -9.982e-37,  1.444e-45,\n",
      "         1.671e-36],\n",
      "       [-1.697e-04, -2.977e-04,  4.835e-41, ...,  7.678e-14,  2.594e-35,\n",
      "        -1.820e-04],\n",
      "       ...,\n",
      "       [-6.436e-07, -1.129e-06,  2.260e-46, ...,  3.795e-27,  9.730e-36,\n",
      "        -6.906e-07],\n",
      "       [-1.514e-06, -2.656e-06,  6.113e-43, ...,  4.675e-15,  4.111e-36,\n",
      "        -1.624e-06],\n",
      "       [-3.018e-06, -5.293e-06,  3.977e-43, ...,  6.169e-21,  1.416e-44,\n",
      "        -3.237e-06]]),\n",
      " 'm2': array([[2.346e-06, 2.578e-04, 2.966e-07, ..., 2.200e-05, 2.863e-05,\n",
      "        3.229e-05],\n",
      "       [3.886e-07, 6.213e-05, 1.463e-10, ..., 8.220e-06, 7.576e-07,\n",
      "        1.588e-05],\n",
      "       [1.360e-05, 9.627e-04, 2.789e-06, ..., 4.744e-05, 4.671e-05,\n",
      "        2.248e-04],\n",
      "       ...,\n",
      "       [5.521e-07, 1.226e-04, 2.307e-07, ..., 3.275e-06, 7.389e-06,\n",
      "        2.379e-05],\n",
      "       [2.293e-06, 2.541e-04, 6.868e-07, ..., 1.501e-05, 1.024e-05,\n",
      "        7.026e-05],\n",
      "       [7.691e-07, 1.452e-04, 3.040e-08, ..., 5.849e-06, 1.366e-06,\n",
      "        4.485e-05]]),\n",
      " 'optimizer': <function Layer.set_optimizer.<locals>.optimizer at 0x000001A29C394790>,\n",
      " 'output': array([[ 5.828,  4.012, -2.014, -0.145, -0.433,  2.207, -3.703,  2.869,\n",
      "        -1.857, -0.081,  2.416, -1.345,  3.04 ,  1.636, -0.457, -0.684,\n",
      "         4.285,  3.749,  2.563, -0.968, -2.837,  2.517, -3.179, -0.277,\n",
      "         2.932,  4.121,  6.279,  0.798,  4.43 , -0.463, -0.167,  2.057,\n",
      "         1.406,  0.07 ,  2.789,  5.068, -2.076, -1.379, -1.069,  2.62 ]]),\n",
      " 'output_nodes': 40,\n",
      " 'type': 'linear',\n",
      " 'weights': array([[-0.115,  0.066, -0.105, ..., -0.309, -0.007,  0.191],\n",
      "       [ 0.097,  0.288,  0.133, ...,  0.199,  0.051, -0.355],\n",
      "       [ 0.297,  0.208, -0.015, ..., -0.141, -0.14 ,  0.102],\n",
      "       ...,\n",
      "       [ 0.215,  0.146,  0.18 , ..., -0.073, -0.083,  0.059],\n",
      "       [ 0.115,  0.042,  0.048, ..., -0.188, -0.112, -0.038],\n",
      "       [ 0.164,  0.139,  0.214, ..., -0.11 ,  0.193,  0.045]]),\n",
      " 'with_bias': True}\n",
      "--4--\n",
      "Printing activation layer:\n",
      "{'cache': cache(x=array([[ 5.828,  4.012, -2.014, -0.145, -0.433,  2.207, -3.703,  2.869,\n",
      "        -1.857, -0.081,  2.416, -1.345,  3.04 ,  1.636, -0.457, -0.684,\n",
      "         4.285,  3.749,  2.563, -0.968, -2.837,  2.517, -3.179, -0.277,\n",
      "         2.932,  4.121,  6.279,  0.798,  4.43 , -0.463, -0.167,  2.057,\n",
      "         1.406,  0.07 ,  2.789,  5.068, -2.076, -1.379, -1.069,  2.62 ]]), y=array([[ 5.828,  4.012, -0.   , -0.   , -0.   ,  2.207, -0.   ,  2.869,\n",
      "        -0.   , -0.   ,  2.416, -0.   ,  3.04 ,  1.636, -0.   , -0.   ,\n",
      "         4.285,  3.749,  2.563, -0.   , -0.   ,  2.517, -0.   , -0.   ,\n",
      "         2.932,  4.121,  6.279,  0.798,  4.43 , -0.   , -0.   ,  2.057,\n",
      "         1.406,  0.07 ,  2.789,  5.068, -0.   , -0.   , -0.   ,  2.62 ]])),\n",
      " 'func_backward': <function <lambda> at 0x000001A29C3298B0>,\n",
      " 'func_forward': <function <lambda> at 0x000001A29C329820>,\n",
      " 'func_name': 'relu',\n",
      " 'optimizer': <function Layer.set_optimizer.<locals>.optimizer at 0x000001A29C394160>,\n",
      " 'type': 'activation'}\n",
      "--5--\n",
      "Printing linear layer:\n",
      "{'bias': 0,\n",
      " 'input': array([[ 5.828,  4.012, -0.   , -0.   , -0.   ,  2.207, -0.   ,  2.869,\n",
      "        -0.   , -0.   ,  2.416, -0.   ,  3.04 ,  1.636, -0.   , -0.   ,\n",
      "         4.285,  3.749,  2.563, -0.   , -0.   ,  2.517, -0.   , -0.   ,\n",
      "         2.932,  4.121,  6.279,  0.798,  4.43 , -0.   , -0.   ,  2.057,\n",
      "         1.406,  0.07 ,  2.789,  5.068, -0.   , -0.   , -0.   ,  2.62 ]]),\n",
      " 'input_nodes': 40,\n",
      " 'm1': array([[1.706e-03, 1.706e-03],\n",
      "       [1.171e-03, 1.171e-03],\n",
      "       [2.427e-40, 2.427e-40],\n",
      "       [1.769e-10, 1.769e-10],\n",
      "       [2.633e-05, 2.633e-05],\n",
      "       [7.762e-04, 7.762e-04],\n",
      "       [7.790e-48, 7.790e-48],\n",
      "       [8.357e-04, 8.357e-04],\n",
      "       [1.165e-13, 1.165e-13],\n",
      "       [5.970e-05, 5.970e-05],\n",
      "       [6.720e-04, 6.720e-04],\n",
      "       [1.055e-09, 1.055e-09],\n",
      "       [1.016e-03, 1.016e-03],\n",
      "       [6.475e-04, 6.475e-04],\n",
      "       [6.950e-06, 6.950e-06],\n",
      "       [2.747e-07, 2.747e-07],\n",
      "       [1.344e-03, 1.344e-03],\n",
      "       [1.118e-03, 1.118e-03],\n",
      "       [5.278e-04, 5.278e-04],\n",
      "       [6.367e-06, 6.367e-06],\n",
      "       [5.413e-42, 5.413e-42],\n",
      "       [8.771e-04, 8.771e-04],\n",
      "       [3.363e-40, 3.363e-40],\n",
      "       [1.664e-12, 1.664e-12],\n",
      "       [9.947e-04, 9.947e-04],\n",
      "       [1.285e-03, 1.285e-03],\n",
      "       [1.966e-03, 1.966e-03],\n",
      "       [2.010e-04, 2.010e-04],\n",
      "       [1.338e-03, 1.338e-03],\n",
      "       [1.173e-10, 1.173e-10],\n",
      "       [2.251e-05, 2.251e-05],\n",
      "       [5.655e-04, 5.655e-04],\n",
      "       [6.605e-04, 6.605e-04],\n",
      "       [2.817e-06, 2.817e-06],\n",
      "       [7.547e-04, 7.547e-04],\n",
      "       [1.457e-03, 1.457e-03],\n",
      "       [9.807e-36, 9.807e-36],\n",
      "       [3.553e-14, 3.553e-14],\n",
      "       [2.162e-36, 2.162e-36],\n",
      "       [7.524e-04, 7.524e-04]]),\n",
      " 'm2': array([[3.464e-03, 3.464e-03],\n",
      "       [1.017e-02, 1.017e-02],\n",
      "       [2.228e-05, 2.228e-05],\n",
      "       [7.664e-04, 7.664e-04],\n",
      "       [5.616e-04, 5.616e-04],\n",
      "       [5.173e-03, 5.173e-03],\n",
      "       [1.656e-06, 1.656e-06],\n",
      "       [4.249e-03, 4.249e-03],\n",
      "       [8.776e-05, 8.776e-05],\n",
      "       [8.374e-03, 8.374e-03],\n",
      "       [4.902e-03, 4.902e-03],\n",
      "       [3.707e-04, 3.707e-04],\n",
      "       [6.500e-03, 6.500e-03],\n",
      "       [5.428e-03, 5.428e-03],\n",
      "       [3.767e-03, 3.767e-03],\n",
      "       [1.767e-04, 1.767e-04],\n",
      "       [1.744e-02, 1.744e-02],\n",
      "       [1.079e-02, 1.079e-02],\n",
      "       [2.252e-03, 2.252e-03],\n",
      "       [1.867e-03, 1.867e-03],\n",
      "       [8.852e-06, 8.852e-06],\n",
      "       [2.858e-03, 2.858e-03],\n",
      "       [1.516e-05, 1.516e-05],\n",
      "       [2.365e-03, 2.365e-03],\n",
      "       [1.688e-04, 1.688e-04],\n",
      "       [1.807e-02, 1.807e-02],\n",
      "       [6.093e-04, 6.093e-04],\n",
      "       [1.313e-02, 1.313e-02],\n",
      "       [2.786e-04, 2.786e-04],\n",
      "       [6.574e-04, 6.574e-04],\n",
      "       [9.444e-04, 9.444e-04],\n",
      "       [2.873e-03, 2.873e-03],\n",
      "       [2.192e-03, 2.192e-03],\n",
      "       [3.676e-03, 3.676e-03],\n",
      "       [4.847e-03, 4.847e-03],\n",
      "       [8.100e-03, 8.100e-03],\n",
      "       [5.567e-04, 5.567e-04],\n",
      "       [1.430e-03, 1.430e-03],\n",
      "       [6.768e-05, 6.768e-05],\n",
      "       [7.396e-03, 7.396e-03]]),\n",
      " 'optimizer': <function Layer.set_optimizer.<locals>.optimizer at 0x000001A29C394700>,\n",
      " 'output': array([[-16.477,  -8.242]]),\n",
      " 'output_nodes': 2,\n",
      " 'type': 'linear',\n",
      " 'weights': array([[-0.076, -0.188],\n",
      "       [-0.34 , -0.123],\n",
      "       [ 0.032,  0.061],\n",
      "       [ 0.203,  0.159],\n",
      "       [ 0.012, -0.092],\n",
      "       [-0.166, -0.237],\n",
      "       [ 0.052,  0.438],\n",
      "       [-0.39 , -0.206],\n",
      "       [-0.234,  0.524],\n",
      "       [ 0.195, -0.027],\n",
      "       [-0.273, -0.138],\n",
      "       [-0.176,  0.058],\n",
      "       [-0.265, -0.203],\n",
      "       [-0.462,  0.017],\n",
      "       [ 0.09 ,  0.277],\n",
      "       [ 0.156,  0.159],\n",
      "       [-0.145, -0.026],\n",
      "       [ 0.037, -0.175],\n",
      "       [-0.148, -0.194],\n",
      "       [ 0.027,  0.27 ],\n",
      "       [ 0.182,  0.081],\n",
      "       [-0.033, -0.21 ],\n",
      "       [-0.038,  0.173],\n",
      "       [-0.183, -0.667],\n",
      "       [-0.426, -0.166],\n",
      "       [-0.352, -0.493],\n",
      "       [-0.187,  0.049],\n",
      "       [ 0.026,  0.151],\n",
      "       [-0.671,  0.15 ],\n",
      "       [ 0.13 ,  0.265],\n",
      "       [ 0.148, -0.166],\n",
      "       [-0.146, -0.175],\n",
      "       [-0.269, -0.254],\n",
      "       [ 0.499,  0.123],\n",
      "       [-0.507,  0.346],\n",
      "       [-0.309, -0.096],\n",
      "       [ 0.312, -0.305],\n",
      "       [ 0.077,  0.059],\n",
      "       [ 0.347, -0.088],\n",
      "       [ 0.165, -0.448]]),\n",
      " 'with_bias': True}\n",
      "--6--\n",
      "Printing activation layer:\n",
      "{'cache': cache(x=array([[-16.477,  -8.242]]), y=array([[2.651e-04, 9.997e-01]])),\n",
      " 'func_backward': <function <lambda> at 0x000001A29C329790>,\n",
      " 'func_forward': <function <lambda> at 0x000001A29C329700>,\n",
      " 'func_name': 'softmax',\n",
      " 'optimizer': <function Layer.set_optimizer.<locals>.optimizer at 0x000001A29C3348B0>,\n",
      " 'type': 'activation'}\n"
     ]
    }
   ],
   "source": [
    "p2.network.actor.print_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b6fccf-3566-4e39-b1b7-437d4b107c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

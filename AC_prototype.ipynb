{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ba32a7-6ceb-4ae0-8f0d-90f5d64f1727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an attempt on Actor-Critic A2C\n",
    "# not part of the project as I haven't figured out the correct backpropagation\n",
    "\n",
    "# code modified from\n",
    "# https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41b6da6-103a-4599-95ff-390e91668ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# - implement it\n",
    "# - test whether sharing first layers help improving performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "276b8cce-a207-4f3f-be53-467bb63bc090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import axelrod as axl\n",
    "from time import time\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import permutations\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "\n",
    "import network\n",
    "from axl_utils import NNplayer, State, set_match, set_play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c262f71-1065-4d58-8287-a945f09b37ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 3), (3, 3), (5, 0), (0, 5), (5, 0), (1, 1), (1, 1), (0, 5), (3, 3), (5, 0), (0, 5), (5, 0), (0, 5), (3, 3), (5, 0), (0, 5), (5, 0), (1, 1), (0, 5), (5, 0)]\n",
      "Player 1 score = 50\n",
      "Player 2 score = 45\n"
     ]
    }
   ],
   "source": [
    "C = axl.Action.C\n",
    "D = axl.Action.D\n",
    "\n",
    "# config game rules\n",
    "GAME_LEN = 20 + 1\n",
    "GAME = axl.Game(r=3, s=0, t=5, p=1)\n",
    "Match = set_match(game=GAME, turns=GAME_LEN)\n",
    "play = set_play(Match)\n",
    "\n",
    "game = play(axl.Prober4(), axl.TitForTat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "58482cef-b3aa-4be7-8194-f8d96d0abe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', \n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class NNplayer(axl.Player):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    name = 'NNplayer'\n",
    "    classifier = {\n",
    "        'memory_depth': -1,\n",
    "        'stochastic': False,\n",
    "        'inspects_source': False,\n",
    "        'manipulates_source': False,\n",
    "        'manipulates_state': False\n",
    "    }\n",
    "    \n",
    "    decision = (axl.Action.C, axl.Action.D)\n",
    "    \n",
    "    def __init__(self, network, state, reward='dense', policy='off', name='DQN'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.name    = name\n",
    "        self.state   = state\n",
    "        self.network = network\n",
    "        \n",
    "        self.policy_mode = True if policy==\"off\" else False      # off-policy = 1, on-policy = 0\n",
    "        self.reward_mode = True if reward==\"dense\" else False    # dense reward = 1, sparse reward = 0\n",
    "        self.N = self.state.N                                    # how not-yet-happened turn is encoded\n",
    "        self.reset()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "    \n",
    "    # the following 3 functions override the orginal implementation in axelrod library\n",
    "    # they are automatically called by axl during each game\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the attributes to start a new game\"\"\"\n",
    "        self.reward = 0\n",
    "        self.state.reset()\n",
    "        self.transitions = []\n",
    "        self.network.reset_state()\n",
    "        self._history = axl.history.History()\n",
    "        \n",
    "    def strategy(self, opponent):\n",
    "        \"\"\"Query the network (each turn) to make decision\"\"\"\n",
    "        idx = self.network.query(self.state.values())\n",
    "        return self.decision[idx]\n",
    "    \n",
    "    # overwrite update_history to update our state\n",
    "    def update_history(self, *args):\n",
    "        self.history.append(*args)\n",
    "        self.update_state(*args)\n",
    "    # --------------------------------------------------------------------------------\n",
    "        \n",
    "    def update_state(self, play, coplay):\n",
    "        \"\"\"Update current game state & record transition into replay memory\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        play : axl.Action\n",
    "            action from last turn, (C or D)\n",
    "        coplay: axl.Action\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # update game state\n",
    "        s  = self.state.values()\n",
    "        s_ = self.state.push(play, coplay)\n",
    "        last_turn = s[0,0,1]!=self.N\n",
    "        \n",
    "        # compute reward\n",
    "        r  = axl.interaction_utils.compute_scores([(play, coplay)])[0][0]\n",
    "        \n",
    "        # rewrite action\n",
    "        action = [True, False] if play==axl.Action.C else [False, True]\n",
    "        \n",
    "        # dense reward\n",
    "        if self.reward_mode:\n",
    "            r  = r if (not last_turn or not self.policy_mode) else np.NaN  # set last turn reward to NaN (off-policy only)\n",
    "            transition = Transition(s, action, s_, r)\n",
    "        \n",
    "        # sparse reward\n",
    "        else:\n",
    "            if not last_turn:\n",
    "                transition = Transition(s, action, s_, 0)\n",
    "                self.reward += r\n",
    "            else:\n",
    "                transition = Transition(s, action, s_, r+self.reward)\n",
    "                self.reward = 0\n",
    "        \n",
    "        # record transitions for training\n",
    "        self.transitions.append(transition)\n",
    "        \n",
    "        # last turn operations\n",
    "        if last_turn:\n",
    "            self.end_episode()\n",
    "    \n",
    "    def end_episode(self):\n",
    "        # for off-policy learner,\n",
    "        # push all transitions into replay memory\n",
    "        if self.policy_mode:\n",
    "            for t in self.transitions:\n",
    "                self.network.push(t)\n",
    "            self.transitions = []\n",
    "\n",
    "        # for on-policy learner,\n",
    "        # push all rewards,\n",
    "        # then call train function\n",
    "        else:\n",
    "            for t in self.transitions:\n",
    "                self.network.push(t.reward)\n",
    "            self.transitions = []\n",
    "            self.network.train()\n",
    "    \n",
    "    \n",
    "    def train(self, *args, **kwargs):\n",
    "        self.network.train(*args, **kwargs)\n",
    "    \n",
    "    def plot(self, **kwargs):\n",
    "        \"\"\"Let the network plot its training loss\"\"\"\n",
    "        self.network.plot(**kwargs)\n",
    "\n",
    "    # test mode using \"with\" statement\n",
    "    def __enter__(self, *args):\n",
    "        self.network.test_mode(True)\n",
    "    \n",
    "    def __exit__(self, *args):\n",
    "        self.network.test_mode(False)\n",
    "    \n",
    "    def set_greedy(self, value):\n",
    "        self.network.greedy = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3bb07941-44da-4fef-93fc-e4b133314395",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C():\n",
    "    \n",
    "    def __init__(self, actor, critic, param, gamma=0.9):\n",
    "        \n",
    "        self.actor  = actor\n",
    "        self.critic = critic\n",
    "        self.param  = param\n",
    "        self.gamma  = gamma\n",
    "        self._test_mode   = False\n",
    "        \n",
    "        self.reset_state()\n",
    "    \n",
    "    def set_param(self, param):\n",
    "        self.param = param\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.saved_actions = []  # [(log_prob(chosen action), state_value)]\n",
    "        self.saved_rewards = []  # [reward from environment]\n",
    "        \n",
    "    def push(self, reward):\n",
    "        \"\"\"Push one reward value into memory\"\"\"\n",
    "        self.saved_rewards.append(reward)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \n",
    "        # probability of actions :: 1x[n actions] array\n",
    "        probs = self.actor(state)[0]\n",
    "        \n",
    "        # state value :: 1x1 array\n",
    "        value = self.critic(state)\n",
    "        \n",
    "        return probs, value\n",
    "    \n",
    "    def query(self, state):\n",
    "        \"\"\"Query for action, chosen action & its probability is saved for training\"\"\"\n",
    "        probs, value = self.forward(state)\n",
    "        \n",
    "        # for test mode, output probabilities then choose action deterministically\n",
    "        if self._test_mode:\n",
    "            print(probs)\n",
    "            return probs.argmax()\n",
    "        \n",
    "        # otherwise choose action with given probability\n",
    "        else:\n",
    "            # sample action\n",
    "            cum_probs = np.cumsum(probs)\n",
    "            action = (cum_probs > np.random.uniform()).argmax()  # Int index of action\n",
    "\n",
    "            # save\n",
    "            self.saved_actions.append((action, np.log(probs[action]),value))  # (Int, 1x1 array, 1x1 array)\n",
    "\n",
    "            return action\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the network, should be called after each game,\n",
    "        hyperparameters should be given before calling this function (via set_param)\n",
    "        \"\"\"\n",
    "        \n",
    "        # avoid training under test mode\n",
    "        if self._test_mode:\n",
    "            return\n",
    "        \n",
    "        # set up optimizers etc.\n",
    "        assert self.param, \"\"\"No hyperparameters given\"\"\"\n",
    "        self.actor.set_up(self.param)\n",
    "        self.critic.set_up(self.param)\n",
    "        \n",
    "        # cumulative discounted reward i.e. \"true\" value\n",
    "        returns = []\n",
    "        cum_r = 0\n",
    "        for R in self.saved_rewards[::-1]:\n",
    "            cum_r = R + self.gamma * cum_r\n",
    "            returns.insert(0, cum_r)\n",
    "        returns = (returns - np.mean(returns)) / np.std(returns) # standardize for better convergence (?)\n",
    "        \n",
    "        # calculate losses\n",
    "        policy_losses = []\n",
    "        value_losses = []\n",
    "        for (action, log_prob, value), R in zip(self.saved_actions, returns):\n",
    "            \n",
    "            advantage = R - value\n",
    "            policy = np.array([1, 0]) if action==0 else np.array([0,1])  # HARDCODED FOR NOW\n",
    "\n",
    "            # record losses\n",
    "            policy_losses.append(-log_prob * advantage * policy)  # DOUBLE CHECK THIS !!\n",
    "            value_losses.append(self.critic.loss_fn(R, value)[0])\n",
    "        \n",
    "        # sum all losses then feedback to networks\n",
    "        policy_loss = np.array(np.sum(policy_losses))\n",
    "        value_loss  = np.array(np.sum(value_losses))\n",
    "        print(policy_loss, value_loss)\n",
    "        self.actor.backprop(policy_loss, param)\n",
    "        self.critic.backprop(value_loss, param)\n",
    "        \n",
    "    def test_mode(self, on):\n",
    "        if on:\n",
    "            self._test_mode = True\n",
    "        else:\n",
    "            self._test_mode = False\n",
    "            \n",
    "    def plot(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, *args):\n",
    "        return self.forward(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a62a5c2e-8c46-4502-9fa5-5d10c2d47803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1 = [network.Flatten_layer(), network.Linear_layer(GAME_LEN*2, 100)]\n",
    "\n",
    "actor = network.NeuralNetwork([\n",
    "                    *layer1,\n",
    "                    network.Activation_layer('ReLU'),\n",
    "                    network.Linear_layer(100, 40),\n",
    "                    network.Activation_layer('ReLU'),\n",
    "                    network.Linear_layer(40, 2),\n",
    "                    network.Activation_layer('Softmax')\n",
    "                    ])\n",
    "critic = network.NeuralNetwork([\n",
    "                    *layer1,\n",
    "                    network.Activation_layer('ReLU'),\n",
    "                    network.Linear_layer(100, 200),\n",
    "                    network.Activation_layer('ReLU'),\n",
    "                    network.Linear_layer(200, 1),\n",
    "                    ])\n",
    "\n",
    "param = {\"lr\": 3e-4, 'batch': 16, \"mode\": \"train\", \"eps\": 1e-16, \"epoch\": 0, 't': 1, 'clip': 1.0,\n",
    "         'optimizer': ('Adam', 0.9, 0.999), 'regularizer': ('l2', 1e-3), \"loss_fn\":\"mse\"}\n",
    "nn = A2C(actor, critic, param)\n",
    "p1 = NNplayer(nn, State(GAME_LEN, C=1, D=0.1, N=-1), policy='on')\n",
    "\n",
    "del actor, critic\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b96e987f-3f5a-4df9-9435-7ac82bc7525d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.413 0.587]\n",
      "[0.694 0.306]\n",
      "[0.691 0.309]\n",
      "[0.828 0.172]\n",
      "[0.539 0.461]\n",
      "[0.734 0.266]\n",
      "[0.673 0.327]\n",
      "[0.717 0.283]\n",
      "[0.844 0.156]\n",
      "[0.66 0.34]\n",
      "[0.435 0.565]\n",
      "[0.533 0.467]\n",
      "[0.36 0.64]\n",
      "[0.525 0.475]\n",
      "[0.696 0.304]\n",
      "[0.741 0.259]\n",
      "[0.597 0.403]\n",
      "[0.605 0.395]\n",
      "[0.563 0.437]\n",
      "[0.349 0.651]\n",
      "[0.385 0.615]\n",
      "[(5, 0), (0, 5), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (5, 0), (0, 5), (5, 0), (0, 5), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (5, 0)]\n",
      "Player 1 score = 59\n",
      "Player 2 score = 54\n"
     ]
    }
   ],
   "source": [
    "with p1:\n",
    "    play(p1, axl.TitForTat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "68cab6bb-6875-4861-93c8-6e3b9f74f206",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.804322081133353 -36.47068513584241\n",
      "18.935814427500613 -36.839830718004414\n",
      "21.352525542048227 -35.63169402155203\n",
      "20.26440537014677 -30.887934358727843\n",
      "18.149912908503925 -31.43293475059233\n",
      "18.31648091649007 -24.23260301801937\n",
      "15.342729581553114 -20.529193402639986\n",
      "18.26090666020042 -20.693005108347247\n",
      "12.231516073153804 -21.77742418454487\n",
      "16.00303338297415 -20.451026369039823\n",
      "12.410729819385272 -22.246446323744866\n",
      "11.432368839854883 -19.450764471951736\n",
      "6.340955305858999 -15.050833139775794\n",
      "5.071459277518772 -10.802054152694344\n",
      "12.364893810675376 -11.85774469068356\n",
      "9.667424341730877 -10.03950918740788\n",
      "7.147654587092153 -6.8487236908196\n",
      "4.253945452028107 -6.2020223516701725\n",
      "4.318201324109573 -4.792881476390072\n",
      "-1.0407349559420291 -3.6184486771103206\n",
      "3.5486527795522 -1.6919818401037698\n",
      "-7.357898658034882 1.8852506561321283\n",
      "-4.994549608146037 2.6478093226319097\n",
      "-0.7971395153697464 4.063475689019488\n",
      "0.05534673327173223 3.9445196249885086\n",
      "-4.684959978153041 5.769817763417757\n",
      "1.6233089110003938 4.222584307496516\n",
      "-1.1302864009239348 4.663595388291261\n",
      "0.34392187247644734 5.075833570411743\n",
      "2.376675360958559 1.844386740144318\n",
      "0.4218686648659724 1.447174446826835\n",
      "-4.291739851740465 3.8592326804026578\n",
      "0.3380881624863745 -0.5442176781498458\n",
      "0.7572540312357215 1.640373061799056\n",
      "-0.30452023820783314 -0.9437330673510083\n",
      "1.8311446017119426 -0.4981799690865705\n",
      "0.537175536354487 -1.86919372662544\n",
      "4.693172411220187 -0.5812413833183654\n",
      "-1.3469045356203582 -0.316847906981105\n",
      "-0.13763937357157507 1.2219802172451883\n",
      "1.1255472047475163 0.3007365652672993\n",
      "1.5544462325692534 -1.1765874118771311\n",
      "3.279936603351536 -2.670770929836758\n",
      "-0.450740988452792 2.1071512329121385\n",
      "3.289204984690164 -0.6364083135347274\n",
      "3.5956044944318224 -1.6950696465048196\n",
      "1.7789650777820751 -1.2210578766171323\n",
      "-0.32969911698615795 0.8039906159081025\n",
      "1.8662766867007843 1.0545438831501937\n",
      "3.0486102479922157 2.2192309322732964\n",
      "-3.3263649954312293 5.780013427932489\n",
      "3.9696704913582357 2.9045014264616613\n",
      "3.0921914711951453 1.2055337443806713\n",
      "-8.217798391609666 1.8250566718975914\n",
      "-6.897000535677854 2.7441444947524065\n",
      "1.2236995349697657 1.0861692124231896\n",
      "1.1308377087197043 0.47018731267874214\n",
      "1.5225868674866936 -1.792681276257004\n",
      "-4.45338620619417 -2.225266660706312\n",
      "2.0146519514798116 -2.2923322778340722\n",
      "6.2932223596715255 -5.887157350896276\n",
      "2.3672365619895404 -2.88849737892198\n",
      "6.364188231766619 -4.680565535259497\n",
      "3.3578002527481527 -1.5372234042385093\n",
      "-0.6891617400409717 0.016104489398353294\n",
      "-0.6788773329918703 -0.7964874873590637\n",
      "1.6245066768157974 -0.23722975343310626\n",
      "6.44514546217613 -1.913104605493245\n",
      "-2.1157889337995104 1.912462908436591\n",
      "3.4676388152645545 0.6934649823070336\n",
      "1.7549137652112041 2.695392420639422\n",
      "-6.49942228193882 3.8947971134895947\n",
      "-3.28786375318236 1.002539898582281\n",
      "-1.3338267438072453 1.6025038960450226\n",
      "-2.512804435231194 3.670977126101086\n",
      "-1.2790692607683352 1.5547822441101462\n",
      "4.915032474403256 -2.060699121018958\n",
      "1.4484937999359622 -1.0044978666525433\n",
      "-2.721412246071573 -1.0222563874645658\n",
      "3.5978646686705864 -2.961499317222242\n",
      "-1.6958320433855685 1.8160747805242003\n",
      "2.1537888027908543 2.3959206471600347\n",
      "1.4423796948481613 0.7055173956104097\n",
      "-1.5926916037819288 2.2919644154737746\n",
      "1.5058484662810825 0.024835481575492313\n",
      "1.0954309460189964 0.6533469501504312\n",
      "1.5614539980456117 -1.6143564535685995\n",
      "2.704056646839226 -1.4671529768774452\n",
      "0.2712648096850656 0.1082248569036377\n",
      "2.7100940215210203 -1.0604593674543308\n",
      "0.7103998356842438 -2.0617624013747085\n",
      "2.176625232480366 -1.2374888542947362\n",
      "-0.20173743983836334 -0.6871500929775185\n",
      "0.8163183895487693 -1.4415950112643139\n",
      "1.8179359319473443 2.2991661400104073\n",
      "-1.6466937929374137 1.827032864270545\n",
      "2.976057757873024 1.4140634375242622\n",
      "4.8563712519070155 1.3343853036847722\n",
      "0.39666102900650335 1.7040383487139787\n",
      "-1.5113931059592596 1.0591078163223817\n",
      "-0.9752248885597732 1.7355000039103623\n",
      "2.106984154009237 0.9189354575663005\n",
      "-3.7065140274638204 -0.4510298556639629\n",
      "0.5597713350548921 -0.7761540340279423\n",
      "2.676098299680085 -2.0030715758943627\n",
      "4.955441883849727 -1.2016841873070137\n",
      "-1.9042398237865732 0.45659451355954217\n",
      "-0.7125385619394243 -0.5431874119255387\n",
      "-1.37199441025428 0.44096809260842473\n",
      "-0.11412655847046377 1.2823816855628543\n",
      "2.712525270967482 0.9041547319156655\n",
      "-1.344313318480287 -0.4223292324394885\n",
      "-4.138702421486514 -1.0598260429289308\n",
      "0.9828835013280329 -1.43594550250473\n",
      "0.3562916881635645 -0.23385145198943902\n",
      "3.8310303766581786 0.1329037374006803\n",
      "1.1005186335557975 -0.6536748685886922\n",
      "2.5841720068482936 0.5228834938164129\n",
      "1.6194795262332478 1.8363857339642151\n",
      "-2.4668411528900682 1.7962506714647928\n",
      "3.340010113358047 1.2898883553779683\n",
      "4.2111920256832045 2.016757475039363\n",
      "1.0231666996038844 -0.45106248816640804\n",
      "1.8206388755337002 -1.8682907620474625\n",
      "0.07299853797368538 -1.2009496765312755\n",
      "2.8065051717491745 0.563949180015757\n",
      "1.347617796905675 0.6178976164263648\n",
      "0.7356276900971095 0.47108437463064057\n",
      "1.199790706531977 -2.5678014055487046\n",
      "2.431977136935025 -0.9357074497265945\n",
      "-2.752339621523054 0.4999155820726213\n",
      "0.5459044835335982 -0.17577775127298256\n",
      "-1.0570977890374387 -0.5938266544794308\n",
      "3.9171644146742746 -0.43599299583016804\n",
      "-2.6706515344868493 1.4911343504524228\n",
      "-3.2126306894945382 1.318507360439474\n",
      "-0.3251809082164685 0.020099187930786133\n",
      "2.46372301200222 1.4779795570572016\n",
      "0.681804903881891 1.0845463626168637\n",
      "0.46135157850890124 -0.1935769892397905\n",
      "-0.8726524836843041 0.705802712408556\n",
      "1.8504975427847858 -0.9517352034113402\n",
      "-5.744952993180991 -1.3104879495996271\n",
      "0.8940125139898408 -1.0798082586232194\n",
      "-0.5788405305490655 0.15051371086400422\n",
      "0.34561839397929245 1.1311373164077967\n",
      "1.5085051457206504 -1.3446188891854551\n",
      "3.989917734868228 0.1372337557759824\n",
      "2.123075043769825 -0.8411401818278219\n",
      "0.26062170698787196 -0.8341171715861049\n",
      "-0.5681794687468464 -0.32047164366899583\n",
      "-1.2848875435578206 -0.30118074437584275\n",
      "-0.17683571118208086 0.8104407916641843\n",
      "-1.2625907871220514 2.3335380603549027\n",
      "-1.9929576552271235 3.146395433842689\n",
      "2.4003412608906958 3.382087376417844\n",
      "0.34626572249568777 1.951645369391535\n",
      "1.6586574768679192 1.0063389471442505\n",
      "-2.7042603342640956 0.12673959841111015\n",
      "4.466732516064008 -2.1957033386138494\n",
      "3.5241144175540082 -1.692748861482099\n",
      "3.9279332814952777 -1.8921602728687255\n",
      "4.089918872545544 -0.1876505845254237\n",
      "-0.07207351734353121 -0.5128401740847865\n",
      "-0.7262934505866778 -0.022846217085353615\n",
      "-0.9237070148228905 -0.4158731856650244\n",
      "4.068409869914104 1.1857640108579717\n",
      "0.9447738629304501 0.9066095301034838\n",
      "0.7078586670029055 0.8029784771062967\n",
      "1.5925537974330037 -0.2672065128030625\n",
      "-0.2799834492577916 2.2585466597615698\n",
      "2.620726347443327 -0.8767273525983819\n",
      "0.9685678096140038 -0.971771420402963\n",
      "-2.0623825609329356 0.5866304901307613\n",
      "-2.5933580429073455 1.148735279083748\n",
      "-1.2297849282028954 0.43439937522151784\n",
      "5.140938991966276 -0.8207505303104039\n",
      "0.8142622294599728 1.475521092515785\n",
      "0.7317812735953743 -1.3216710633654323\n",
      "0.6604870021067094 -0.5069426558709778\n",
      "0.9054443157012256 -0.8238991851043411\n",
      "0.16766763463369255 0.7183578293186631\n",
      "3.116928950990786 -1.1955975472676332\n",
      "-2.5668542456936514 2.2486315616913686\n",
      "-2.5058563130699336 -0.386021759987925\n",
      "3.2073169033400992 1.872276826026387\n",
      "-1.7978434531198908 1.2713150264457709\n",
      "-0.9997654101279393 2.5364863576525867\n",
      "0.31162734032951545 1.6016326714121023\n",
      "2.869490283742188 0.3397017318942597\n",
      "0.5709576761821928 0.0006355492626448633\n",
      "1.0427820182431005 0.9718743320501302\n",
      "1.65009986287202 -0.8879349114221471\n",
      "2.3528795104403333 -1.256442935001688\n",
      "2.5802583641910215 -2.3302166261925494\n",
      "-3.3905473495947707 -0.8619672576663835\n",
      "0.17149848966933046 0.6193535636762864\n",
      "1.3989244464753277 -0.025912527639734728\n",
      "-1.3513140243033337 -0.16625308609070966\n",
      "0.5673232646331332 -0.6919712147856854\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 200\n",
    "\n",
    "p1.network.set_param(param)\n",
    "for i in range(ITERATIONS):\n",
    "    # play against tit-for-tat\n",
    "    # training function is called after each game internally\n",
    "    play(p1, axl.TitForTat(), show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c8484f04-6c3d-4c59-b7e8-14701b9fc5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.615 0.385]\n",
      "[0.702 0.298]\n",
      "[0.734 0.266]\n",
      "[0.746 0.254]\n",
      "[0.8 0.2]\n",
      "[0.839 0.161]\n",
      "[0.957 0.043]\n",
      "[0.943 0.057]\n",
      "[0.955 0.045]\n",
      "[0.965 0.035]\n",
      "[0.967 0.033]\n",
      "[0.971 0.029]\n",
      "[0.984 0.016]\n",
      "[0.981 0.019]\n",
      "[0.991 0.009]\n",
      "[0.988 0.012]\n",
      "[0.986 0.014]\n",
      "[0.966 0.034]\n",
      "[0.989 0.011]\n",
      "[0.986 0.014]\n",
      "[0.984 0.016]\n",
      "[(3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3)]\n",
      "Player 1 score = 60\n",
      "Player 2 score = 60\n"
     ]
    }
   ],
   "source": [
    "with p2:\n",
    "    play(p2, axl.TitForTat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "95d93ac8-7a6b-4c37-8da2-5714df6f09a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--0--\n",
      "Printing flatten layer:\n",
      "{'freeze': False, 'shape': (1, 2, 21), 'type': 'flatten'}\n",
      "--1--\n",
      "Printing linear layer:\n",
      "{'bias': 0,\n",
      " 'freeze': False,\n",
      " 'input': array([[0.1, 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ,\n",
      "        1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 0.1, 1. , 1. , 1. , 1. ,\n",
      "        1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ,\n",
      "        1. , 1. , 1. , 1. ]]),\n",
      " 'input_nodes': 42,\n",
      " 'm1': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]),\n",
      " 'm2': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]),\n",
      " 'optimizer': <network.layers.layer.Optimizer object at 0x000001E4BFD1E040>,\n",
      " 'output': array([[ 0.53 ,  0.598, -2.19 , -0.142, -1.605,  0.93 ,  2.034, -1.345,\n",
      "        -0.9  ,  1.676, -0.44 ,  0.046,  1.288, -0.645, -0.551, -0.091,\n",
      "         1.793, -2.422, -3.803, -1.047,  0.492, -1.676, -1.779,  1.233,\n",
      "         0.682,  1.809, -0.664, -0.933,  0.555, -0.892, -4.529, -1.807,\n",
      "         0.355, -0.343,  0.252, -2.873,  0.972, -0.905,  0.746,  0.995,\n",
      "         1.66 ,  0.682,  1.943, -0.353, -2.425,  3.212, -2.514,  2.373,\n",
      "        -0.048,  1.147,  0.658,  0.323,  1.823,  1.007,  1.986,  0.878,\n",
      "        -2.149,  1.427, -0.327,  1.062, -1.059, -2.043,  0.511, -2.505,\n",
      "        -1.029, -0.274,  0.304,  0.955,  1.069, -0.315, -0.591,  2.53 ,\n",
      "        -0.886,  0.721,  0.883, -2.315, -0.764,  0.85 ,  0.905,  0.67 ,\n",
      "        -1.818, -0.857, -0.622,  1.764, -1.25 ,  0.812,  0.566, -0.513,\n",
      "        -0.286,  0.205,  0.958, -0.794,  1.219, -1.57 , -1.157,  0.595,\n",
      "        -2.068,  0.407, -0.921, -0.376]]),\n",
      " 'output_nodes': 100,\n",
      " 'type': 'linear',\n",
      " 'weights': array([[-0.001,  0.049,  0.087, ..., -0.557,  0.433,  0.255],\n",
      "       [ 0.042, -0.099,  0.23 , ...,  0.401,  0.248,  0.041],\n",
      "       [-0.223,  0.091, -0.152, ..., -0.051, -0.463,  0.02 ],\n",
      "       ...,\n",
      "       [-0.125,  0.111, -0.21 , ...,  0.036,  0.212, -0.077],\n",
      "       [-0.033, -0.164,  0.109, ...,  0.376,  0.315, -0.366],\n",
      "       [ 0.   ,  0.   ,  0.   , ...,  0.   ,  0.   ,  0.   ]])}\n",
      "--2--\n",
      "Printing activation layer:\n",
      "{'cache': cache(x=array([[ 0.53 ,  0.598, -2.19 , -0.142, -1.605,  0.93 ,  2.034, -1.345,\n",
      "        -0.9  ,  1.676, -0.44 ,  0.046,  1.288, -0.645, -0.551, -0.091,\n",
      "         1.793, -2.422, -3.803, -1.047,  0.492, -1.676, -1.779,  1.233,\n",
      "         0.682,  1.809, -0.664, -0.933,  0.555, -0.892, -4.529, -1.807,\n",
      "         0.355, -0.343,  0.252, -2.873,  0.972, -0.905,  0.746,  0.995,\n",
      "         1.66 ,  0.682,  1.943, -0.353, -2.425,  3.212, -2.514,  2.373,\n",
      "        -0.048,  1.147,  0.658,  0.323,  1.823,  1.007,  1.986,  0.878,\n",
      "        -2.149,  1.427, -0.327,  1.062, -1.059, -2.043,  0.511, -2.505,\n",
      "        -1.029, -0.274,  0.304,  0.955,  1.069, -0.315, -0.591,  2.53 ,\n",
      "        -0.886,  0.721,  0.883, -2.315, -0.764,  0.85 ,  0.905,  0.67 ,\n",
      "        -1.818, -0.857, -0.622,  1.764, -1.25 ,  0.812,  0.566, -0.513,\n",
      "        -0.286,  0.205,  0.958, -0.794,  1.219, -1.57 , -1.157,  0.595,\n",
      "        -2.068,  0.407, -0.921, -0.376]]), y=array([[ 0.53 ,  0.598, -0.   , -0.   , -0.   ,  0.93 ,  2.034, -0.   ,\n",
      "        -0.   ,  1.676, -0.   ,  0.046,  1.288, -0.   , -0.   , -0.   ,\n",
      "         1.793, -0.   , -0.   , -0.   ,  0.492, -0.   , -0.   ,  1.233,\n",
      "         0.682,  1.809, -0.   , -0.   ,  0.555, -0.   , -0.   , -0.   ,\n",
      "         0.355, -0.   ,  0.252, -0.   ,  0.972, -0.   ,  0.746,  0.995,\n",
      "         1.66 ,  0.682,  1.943, -0.   , -0.   ,  3.212, -0.   ,  2.373,\n",
      "        -0.   ,  1.147,  0.658,  0.323,  1.823,  1.007,  1.986,  0.878,\n",
      "        -0.   ,  1.427, -0.   ,  1.062, -0.   , -0.   ,  0.511, -0.   ,\n",
      "        -0.   , -0.   ,  0.304,  0.955,  1.069, -0.   , -0.   ,  2.53 ,\n",
      "        -0.   ,  0.721,  0.883, -0.   , -0.   ,  0.85 ,  0.905,  0.67 ,\n",
      "        -0.   , -0.   , -0.   ,  1.764, -0.   ,  0.812,  0.566, -0.   ,\n",
      "        -0.   ,  0.205,  0.958, -0.   ,  1.219, -0.   , -0.   ,  0.595,\n",
      "        -0.   ,  0.407, -0.   , -0.   ]])),\n",
      " 'freeze': False,\n",
      " 'func_backward': <function <lambda> at 0x000001E4AA844550>,\n",
      " 'func_forward': <function <lambda> at 0x000001E4AA8444C0>,\n",
      " 'func_name': 'relu',\n",
      " 'type': 'activation'}\n",
      "--3--\n",
      "Printing linear layer:\n",
      "{'bias': 0,\n",
      " 'freeze': False,\n",
      " 'input': array([[ 0.53 ,  0.598, -0.   , -0.   , -0.   ,  0.93 ,  2.034, -0.   ,\n",
      "        -0.   ,  1.676, -0.   ,  0.046,  1.288, -0.   , -0.   , -0.   ,\n",
      "         1.793, -0.   , -0.   , -0.   ,  0.492, -0.   , -0.   ,  1.233,\n",
      "         0.682,  1.809, -0.   , -0.   ,  0.555, -0.   , -0.   , -0.   ,\n",
      "         0.355, -0.   ,  0.252, -0.   ,  0.972, -0.   ,  0.746,  0.995,\n",
      "         1.66 ,  0.682,  1.943, -0.   , -0.   ,  3.212, -0.   ,  2.373,\n",
      "        -0.   ,  1.147,  0.658,  0.323,  1.823,  1.007,  1.986,  0.878,\n",
      "        -0.   ,  1.427, -0.   ,  1.062, -0.   , -0.   ,  0.511, -0.   ,\n",
      "        -0.   , -0.   ,  0.304,  0.955,  1.069, -0.   , -0.   ,  2.53 ,\n",
      "        -0.   ,  0.721,  0.883, -0.   , -0.   ,  0.85 ,  0.905,  0.67 ,\n",
      "        -0.   , -0.   , -0.   ,  1.764, -0.   ,  0.812,  0.566, -0.   ,\n",
      "        -0.   ,  0.205,  0.958, -0.   ,  1.219, -0.   , -0.   ,  0.595,\n",
      "        -0.   ,  0.407, -0.   , -0.   ,  1.   ]]),\n",
      " 'input_nodes': 100,\n",
      " 'm1': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]),\n",
      " 'm2': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]),\n",
      " 'optimizer': <network.layers.layer.Optimizer object at 0x000001E4BDCCE190>,\n",
      " 'output': array([[ 1.916, -0.987,  1.785, -2.227, -0.638, -0.246,  1.165, -2.004,\n",
      "        -0.082,  0.894, -0.226,  0.496, -0.112, -2.505, -3.076, -1.015,\n",
      "        -2.853,  0.494, -0.138,  1.96 ,  0.365, -0.51 ,  3.05 ,  1.017,\n",
      "         1.078, -0.193,  0.43 ,  3.33 , -1.111, -0.766, -0.966,  0.416,\n",
      "        -0.626,  0.846,  0.747,  0.756, -0.985, -0.939,  0.827,  0.694]]),\n",
      " 'output_nodes': 40,\n",
      " 'type': 'linear',\n",
      " 'weights': array([[ 0.024, -0.041, -0.139, ...,  0.014,  0.079,  0.008],\n",
      "       [ 0.104, -0.022,  0.234, ..., -0.132, -0.06 ,  0.129],\n",
      "       [-0.074, -0.104, -0.006, ...,  0.088,  0.005,  0.143],\n",
      "       ...,\n",
      "       [ 0.227,  0.014,  0.03 , ..., -0.138,  0.102, -0.215],\n",
      "       [-0.009, -0.024, -0.004, ...,  0.065, -0.153,  0.01 ],\n",
      "       [ 0.   ,  0.   ,  0.   , ...,  0.   ,  0.   ,  0.   ]])}\n",
      "--4--\n",
      "Printing activation layer:\n",
      "{'cache': cache(x=array([[ 1.916, -0.987,  1.785, -2.227, -0.638, -0.246,  1.165, -2.004,\n",
      "        -0.082,  0.894, -0.226,  0.496, -0.112, -2.505, -3.076, -1.015,\n",
      "        -2.853,  0.494, -0.138,  1.96 ,  0.365, -0.51 ,  3.05 ,  1.017,\n",
      "         1.078, -0.193,  0.43 ,  3.33 , -1.111, -0.766, -0.966,  0.416,\n",
      "        -0.626,  0.846,  0.747,  0.756, -0.985, -0.939,  0.827,  0.694]]), y=array([[ 1.916, -0.   ,  1.785, -0.   , -0.   , -0.   ,  1.165, -0.   ,\n",
      "        -0.   ,  0.894, -0.   ,  0.496, -0.   , -0.   , -0.   , -0.   ,\n",
      "        -0.   ,  0.494, -0.   ,  1.96 ,  0.365, -0.   ,  3.05 ,  1.017,\n",
      "         1.078, -0.   ,  0.43 ,  3.33 , -0.   , -0.   , -0.   ,  0.416,\n",
      "        -0.   ,  0.846,  0.747,  0.756, -0.   , -0.   ,  0.827,  0.694]])),\n",
      " 'freeze': False,\n",
      " 'func_backward': <function <lambda> at 0x000001E4AA844550>,\n",
      " 'func_forward': <function <lambda> at 0x000001E4AA8444C0>,\n",
      " 'func_name': 'relu',\n",
      " 'type': 'activation'}\n",
      "--5--\n",
      "Printing linear layer:\n",
      "{'bias': 0,\n",
      " 'freeze': False,\n",
      " 'input': array([[ 1.916, -0.   ,  1.785, -0.   , -0.   , -0.   ,  1.165, -0.   ,\n",
      "        -0.   ,  0.894, -0.   ,  0.496, -0.   , -0.   , -0.   , -0.   ,\n",
      "        -0.   ,  0.494, -0.   ,  1.96 ,  0.365, -0.   ,  3.05 ,  1.017,\n",
      "         1.078, -0.   ,  0.43 ,  3.33 , -0.   , -0.   , -0.   ,  0.416,\n",
      "        -0.   ,  0.846,  0.747,  0.756, -0.   , -0.   ,  0.827,  0.694,\n",
      "         1.   ]]),\n",
      " 'input_nodes': 40,\n",
      " 'm1': array([[0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.]]),\n",
      " 'm2': array([[0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.]]),\n",
      " 'optimizer': <network.layers.layer.Optimizer object at 0x000001E4BDCCEE50>,\n",
      " 'output': array([[ 1.861, -2.254]]),\n",
      " 'output_nodes': 2,\n",
      " 'type': 'linear',\n",
      " 'weights': array([[-0.079, -0.013],\n",
      "       [ 0.088, -0.384],\n",
      "       [ 0.321,  0.101],\n",
      "       [-0.09 ,  0.102],\n",
      "       [-0.08 ,  0.171],\n",
      "       [-0.158, -0.056],\n",
      "       [-0.081, -0.464],\n",
      "       [-0.278,  0.426],\n",
      "       [-0.046,  0.027],\n",
      "       [-0.223, -0.319],\n",
      "       [-0.073,  0.261],\n",
      "       [-0.005, -0.122],\n",
      "       [ 0.06 , -0.302],\n",
      "       [ 0.031,  0.204],\n",
      "       [ 0.198, -0.32 ],\n",
      "       [ 0.055, -0.237],\n",
      "       [-0.089, -0.162],\n",
      "       [ 0.225, -0.176],\n",
      "       [ 0.033,  0.129],\n",
      "       [-0.162, -0.266],\n",
      "       [ 0.023, -0.095],\n",
      "       [ 0.225,  0.07 ],\n",
      "       [ 0.062,  0.35 ],\n",
      "       [ 0.269, -0.259],\n",
      "       [ 0.315, -0.051],\n",
      "       [ 0.194,  0.066],\n",
      "       [-0.276,  0.071],\n",
      "       [ 0.391, -0.336],\n",
      "       [ 0.243,  0.07 ],\n",
      "       [ 0.16 ,  0.288],\n",
      "       [-0.016, -0.001],\n",
      "       [-0.3  , -0.455],\n",
      "       [ 0.318, -0.049],\n",
      "       [-0.086,  0.071],\n",
      "       [ 0.339,  0.029],\n",
      "       [-0.285, -0.036],\n",
      "       [-0.037, -0.093],\n",
      "       [-0.313, -0.051],\n",
      "       [ 0.267, -0.062],\n",
      "       [-0.16 , -0.518],\n",
      "       [ 0.   ,  0.   ]])}\n",
      "--6--\n",
      "Printing activation layer:\n",
      "{'cache': cache(x=array([[ 1.861, -2.254]]), y=array([[0.984, 0.016]])),\n",
      " 'freeze': False,\n",
      " 'func_backward': <function <lambda> at 0x000001E4AA844430>,\n",
      " 'func_forward': <function <lambda> at 0x000001E4AA8443A0>,\n",
      " 'func_name': 'softmax',\n",
      " 'type': 'activation'}\n"
     ]
    }
   ],
   "source": [
    "p2.network.actor.print_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b6fccf-3566-4e39-b1b7-437d4b107c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

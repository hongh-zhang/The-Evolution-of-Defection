{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e0f03e-6d2e-497f-ac14-ffafc271df85",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# adding parent directory to import path\n",
    "# otherwise simply place the 'network' folder in the same directory\n",
    "import sys\n",
    "import os\n",
    "parent = os.path.dirname(os.path.abspath(''))\n",
    "sys.path.append(parent)\n",
    "\n",
    "\n",
    "import network\n",
    "from network.layers.layer import Layer\n",
    "\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46c02ea-7456-4a5e-9cd6-20936c252c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parameter():\n",
    "    \n",
    "    def __init__(self, p):\n",
    "        \n",
    "        self.p = p\n",
    "        self.shape = p.shape\n",
    "        \n",
    "        self.m1 = np.zeros(self.shape)\n",
    "        self.m2 = np.zeros(self.shape)\n",
    "        \n",
    "    def optimize(self, dp, param):\n",
    "\n",
    "        optimizer = param.get('optimizer', 'sgd')\n",
    "        lr = param.get('lr', 1e-3)\n",
    "        batch = param.get('batch', 16)\n",
    "        decay_rate = param.get('decay', 0)\n",
    "\n",
    "        momentum = param.get('momentum', 0.9)\n",
    "        beta1, beta2 = param.get('beta', (0.9, 0.999))\n",
    "\n",
    "        eps = param.get('eps', 1e-16)\n",
    "        t = param.get('t', 1)\n",
    "\n",
    "        if optimizer.lower() == 'adam':\n",
    "            self.m1 = beta1 * self.m1 + (1 - beta1) * dp\n",
    "            self.m2 = beta2 * self.m2 + (1 - beta2) * np.square(dp)\n",
    "            u1 = self.m1 / (1 - beta1 ** t)\n",
    "            u2 = self.m2 / (1 - beta2 ** t)\n",
    "            dp = (lr * u1 / (np.sqrt(u2) + eps))\n",
    "\n",
    "        elif optimizer.lower() == 'momentum':\n",
    "            self.m1 = momentum * self.m1 + (1-momentum) * dp\n",
    "            dp = (lr * self.m1)\n",
    "\n",
    "        elif optimizer.lower() == 'sgd':\n",
    "            dp = (lr * dp)\n",
    "        \n",
    "        self.p -= dp\n",
    "        self.decay(decay_rate)\n",
    "        \n",
    "    def decay(self, rate):\n",
    "        self.p *= (1-rate)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6876f81-dca0-482d-95c7-da92bf04b3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_layer(Layer):\n",
    "\n",
    "    def __init__(self, input_nodes, output_nodes, bias=0):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        (int) input_nodes = number of input nodes,\n",
    "        (int) output_nodes = number of output nodes,\n",
    "        (bool) bias: enable or disable bias,\n",
    "        \"\"\"\n",
    "        \n",
    "        self.type = 'linear'\n",
    "        \n",
    "        # number of inputs & outputs\n",
    "        self.input_nodes = input_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        self.bias = bias\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"The actual init function, seperate from __init__ to allow NeuralNetworks to be re-initialized\"\"\"\n",
    "\n",
    "        weights = np.random.randn(self.input_nodes, self.output_nodes) / np.sqrt(self.input_nodes/2)\n",
    "        if self.bias:\n",
    "            weights = np.concatenate((weights, np.ones((1,self.output_nodes))*bias), axis=0)\n",
    "\n",
    "        self.weights = Parameter(weights)\n",
    "        \n",
    "    \n",
    "    def forward(self, X, param):\n",
    "        \"\"\"\n",
    "        Forward inputs\n",
    "        Arguments:\n",
    "        (2d array) X: input, in the form of 2d numpy array #of instances * #of attributes\n",
    "        \"\"\"\n",
    "        if self.bias:\n",
    "            # concat ones to x as additional column\n",
    "            X = np.concatenate((X, np.ones((X.shape[0], 1))), axis=1)\n",
    "        \n",
    "        # calculate output\n",
    "        output = np.dot(X, self.weights)\n",
    "        \n",
    "        # record inputs & outputs for weight update later\n",
    "        self.input = X\n",
    "        self.output = output\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, dout, param):\n",
    "        \"\"\"\n",
    "        Error backpropogation function,\n",
    "        Calls self.update to update weights,\n",
    "        Returns this layer's error for the preceding error to propogate\n",
    "        \n",
    "        Arguments:\n",
    "        (2d array) dout: error from the superior layer,\n",
    "        (dict) param\n",
    "        \"\"\"\n",
    "        \n",
    "        lr = param.get(\"lr\", 1e-3)\n",
    "        decay = param.get(\"decay\", 0.01)\n",
    "        \n",
    "        # calculate error to pass\n",
    "        if self.bias:\n",
    "            dx = np.dot(dout, self.weights.T[:,:-1])  # bias is not passed\n",
    "        else:\n",
    "            dx = np.dot(dout, self.weights.T)\n",
    "        \n",
    "        # update self\n",
    "        dw = np.dot(self.input.T, dout)\n",
    "        dw, self.m1, self.m2 = self.optimize(self.delta(dw, self.m1, self.m2), param)\n",
    "        self.weights = (1 - lr*decay) * self.weights - dw\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0366ea-d95e-4b80-9b47-49ea20b5adad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "train = pd.read_csv(\"tests/mnist_train.csv\", header=None)\n",
    "test = pd.read_csv(\"tests/mnist_test.csv\", header=None)\n",
    "\n",
    "# preprocess\n",
    "X   = train.iloc[:, 1:].to_numpy(np.float32) / 255.0 * 0.99 + 0.01\n",
    "X_t = test.iloc[:, 1:].to_numpy(np.float32) / 255.0 * 0.99 + 0.01\n",
    "\n",
    "# one hot encode\n",
    "# np.eye() creates an identity matrix\n",
    "# we then create the one hot matrix by referencing every element\n",
    "y   = np.eye(10)[train.iloc[:,0].to_numpy((int))]\n",
    "y_t = np.eye(10)[test.iloc[:,0].to_numpy((int))]\n",
    "y_true = np.argmax(y_t, axis=1)\n",
    "\n",
    "del test, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b8b988-e0a2-4f0f-a0c3-2516ec402adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network\n",
    "nn = network.NeuralNetwork([\n",
    "                    network.Linear_layer(784, 200, bias=None),\n",
    "                    network.Activation_layer('ReLU'),\n",
    "    \n",
    "                    network.Linear_layer(200, 10, bias=None),\n",
    "                    network.Activation_layer('fast_softmax')\n",
    "                    ])\n",
    "param = {\"lr\": 1e-3, 'batch': 16, \"mode\": \"train\", \"eps\": 1e-9, \"beta\":(0.9, 0.999), \n",
    "         \"epoch\": 0, 'optimizer': 'Adam', 't': 1, 'clip': 1.0, 'decay': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d154a7-b581-4d7d-93e5-7380141eb6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.train(X, y, param, loss_func='fast_cross_entropy')\n",
    "yhat = nn(X_t, mode='classification')\n",
    "accuracy = np.sum(yhat==y_true)/y_true.shape[0]\n",
    "print(f\"Accuracy = {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
